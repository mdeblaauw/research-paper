{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import numpy.random as rnd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "data_subsets = ['train', 'val', 'test']\n",
    "data = {}\n",
    "\n",
    "for name in data_subsets:\n",
    "    with BytesIO() as files:\n",
    "        path = \"omniglot_images/\" +name+ \".pickle\"\n",
    "        s3.Bucket(\"research-paper-omniglot-data\").download_fileobj(path, files)\n",
    "        files.seek(0)    # move back to the beginning after writing\n",
    "        (X,c) = pickle.load(files)\n",
    "        data[name] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with BytesIO() as files:\n",
    "    path = \"omniglot_images/test_affine_transformation/test_transf_27-30k.pickle\"\n",
    "    s3.Bucket(\"research-paper-omniglot-data\").download_fileobj(path, files)\n",
    "    files.seek(0)    # move back to the beginning after writing\n",
    "    (X,c) = pickle.load(files)\n",
    "    affine_test = X\n",
    "    affine_test_labels = np.expand_dims(c, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = '../../omniglot_images/'\n",
    "data_subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "data = {}\n",
    "categories = {}\n",
    "info = {}\n",
    "        \n",
    "for name in data_subsets:\n",
    "    file_path = os.path.join(path, name + \".pickle\")\n",
    "    print(\"loading data from {}\".format(file_path))\n",
    "    with open(file_path,\"rb\") as f:\n",
    "        (X,c) = pickle.load(f)\n",
    "        data[name] = X\n",
    "        categories[name] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(size, s='train'):\n",
    "    #get train data and shape\n",
    "    X=data[s]\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    #initialize 2 empty arrays for the input size in a list\n",
    "    pairs=[np.zeros((size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    #initialize vector for the targets\n",
    "    targets=np.zeros((size,1))\n",
    "    \n",
    "    for x in range(size):\n",
    "        #randomly sample one class (character)\n",
    "        category = rnd.choice(n_classes,1,replace=False)\n",
    "        #randomly sample one example from class (1-20 characters)\n",
    "        idx_1 = rnd.randint(0, n_examples)\n",
    "        pairs[0][x,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        #randomly sample again one example from class and add last class with modulo\n",
    "        # ..to ensure not same class pairs are created\n",
    "        idx_2 = (idx_1 + rnd.randint(0, n_examples)) % n_examples\n",
    "        #pick images of different class for 1st half and same class for 2nd half\n",
    "        if x >= size // 2:\n",
    "            category_2 = category\n",
    "            targets[x] = 1\n",
    "        else: \n",
    "        #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "        # ..different category\n",
    "            idx_2 = rnd.randint(0, n_examples) \n",
    "            category_2 = (category + rnd.randint(1,n_classes)) % n_classes\n",
    "            targets[x] = 0\n",
    "        pairs[1][x,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "        \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_labels = create_train_data(10000)\n",
    "val_set, val_labels = create_train_data(3000, s='val')\n",
    "test_set, test_labels = create_train_data(3000, s='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = affine_test\n",
    "test_labels = affine_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotating of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = test_set[0].shape[0]\n",
    "for i in range(2):\n",
    "    for j in range(images):\n",
    "        img = test_set[i][j,:,:,0]\n",
    "        shift = np.random.randint(low=-10, high=10,size=2)\n",
    "        degrees = np.random.uniform(low=-30, high=30)\n",
    "        img2 = ndimage.rotate(img, degrees, reshape=False, cval = 255, order=0)\n",
    "        new_img = ndimage.shift(img2, shift,  cval = 255, order=0)\n",
    "        test_set[i][j,:,:,0] = new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = test_set[0].reshape(3000,105,105)\n",
    "second = test_set[1].reshape(3000,105,105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3c9c55b0b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair = 2104\n",
    "fig,(ax1,ax2) = plt.subplots(2)\n",
    "ax1.matshow(first[pair],cmap='gray')\n",
    "ax2.matshow(second[pair],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(input_vector, axis):\n",
    "    normalised_input = tf.reduce_sum(tf.square(input_vector), axis = axis, keepdims = True)\n",
    "    scale = tf.divide(normalised_input, tf.add(normalised_input, 1.))\n",
    "    vector = tf.divide(input_vector, tf.sqrt(tf.add(normalised_input, epsilon)))\n",
    "    output = tf.multiply(scale, vector)\n",
    "    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional(input_data, conv_shape, stride_shape, name, relu=True):\n",
    "    weights = tf.get_variable('W'+name, initializer=tf.truncated_normal(conv_shape, stddev=0.01))\n",
    "    bias = tf.get_variable('B'+name, initializer=tf.truncated_normal([conv_shape[-1]], mean = 0.5, stddev=0.01))\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, stride_shape, padding = 'VALID')\n",
    "    out_layer_bias = tf.add(out_layer, bias)\n",
    "    \n",
    "    if relu == True:\n",
    "        out_layer_final = tf.nn.relu(out_layer_bias)\n",
    "        return(out_layer_final, weights)\n",
    "\n",
    "    return(out_layer_bias, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primarycaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primarycaps(input_data, conv_shape, stride_shape, primaryCaps_channels, caps1_size, caps2_size, pose_size, batch):\n",
    "    output, weights_primary = convolutional(input_data, conv_shape, stride_shape, relu=False, name='primaryCaps')\n",
    "    filter_size = output.get_shape().as_list()[1]\n",
    "    caps1_raw = tf.reshape(output, [-1,filter_size*filter_size*primaryCaps_channels,caps1_size], name='caps1_raw')\n",
    "    caps1_output = squash(caps1_raw, axis=-1)\n",
    "    caps1_output_expand = tf.expand_dims(caps1_output, axis=-1)\n",
    "    caps1_output_expand2 = tf.expand_dims(caps1_output_expand, axis=2)\n",
    "    caps1_output_expand2_tiled = tf.tile(caps1_output_expand2, [1,1,caps2_size,1,1], name = 'caps1_out_tiled')\n",
    "    \n",
    "    weight_matrix = tf.get_variable('Weight_matrix', initializer=tf.truncated_normal([filter_size*filter_size*primaryCaps_channels, caps2_size, pose_size, caps1_size], stddev=0.1))\n",
    "    #weight_matrix_tiled = tf.tile(weight_matrix, [batch, 1, 1, 1, 1], name = 'W_matrix_tiled')\n",
    "    #caps2_predicted = tf.matmul(weight_matrix_tiled, caps1_output_expand2_tiled, name='caps2_predicted')\n",
    "    caps2_predicted = tf.einsum('abdc,iabcf->iabdf', weight_matrix, caps1_output_expand2_tiled)\n",
    "    \n",
    "    return(caps2_predicted, weights_primary, weight_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing by agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def routing_by_agreement(input_data, caps2_size, rounds, batch):\n",
    "    raw_weights = tf.zeros([batch, input_data.get_shape().as_list()[1], caps2_size, 1, 1], name = 'raw_weights')\n",
    "    \n",
    "    for i in range(rounds):\n",
    "        routing_weights = tf.nn.softmax(raw_weights, axis=2, name = 'routing_weights' + str(i))\n",
    "        weighted_predictions = tf.multiply(routing_weights, input_data, name = 'weighted_predictions' + str(i))\n",
    "        weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, name = 'weighted_sum' + str(i), keepdims = True)\n",
    "        caps2_output = squash(weighted_sum, axis=-2)\n",
    "    \n",
    "        #caps2_output_tiled = tf.tile(caps2_output, [1, input_data.get_shape().as_list()[1], 1, 1, 1], name = 'caps2_output_tiled'+ str(i))\n",
    "        #agreement = tf.matmul(input_data, caps2_output_tiled, transpose_a = True, name = 'agreement'+ str(i))\n",
    "        agreement = tf.einsum('iabcd,ifbcd->iabcd', input_data, caps2_output)\n",
    "        raw_weights = tf.add(raw_weights, agreement, name = 'raw_weights' + str(i))\n",
    "        \n",
    "    return(caps2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_layer(input_data, input_shape, neurons, name):\n",
    "    weights = tf.get_variable(name+'_W', initializer=tf.truncated_normal([input_shape, neurons], stddev=0.2))\n",
    "    bias = tf.get_variable(name+'b', initializer=tf.truncated_normal([neurons], mean=0.5, stddev=0.01))\n",
    "    fully_connected = tf.add(tf.matmul(input_data, weights), bias)\n",
    "    out_dense_activation = tf.nn.sigmoid(fully_connected)\n",
    "    \n",
    "    return(out_dense_activation, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(caps1_size, caps2_size,pred_matrix_size,conv1_channels,conv1_filter,primaryCaps_channels,primaryCaps_filter,routing_rounds,X,fully_layer_size,stride_conv1,stride_conv2):\n",
    "    conv1, weights_conv1 = convolutional(X, [conv1_filter,conv1_filter,X.get_shape().as_list()[-1],conv1_channels],[1,stride_conv1,stride_conv1,1], name='conv')\n",
    "    primary, weights_primary, weight_matrix = primarycaps(conv1, [primaryCaps_filter,primaryCaps_filter,conv1.get_shape().as_list()[-1],\n",
    "                              primaryCaps_channels*caps1_size], [1,stride_conv2,stride_conv2,1],primaryCaps_channels,\n",
    "                      caps1_size,caps2_size, pred_matrix_size, batch=tf.shape(X)[0])\n",
    "    output = routing_by_agreement(primary, caps2_size, routing_rounds, batch=tf.shape(X)[0])\n",
    "    flat = tf.reshape(output, [-1, caps2_size*pred_matrix_size])\n",
    "    fully_connected, weights_fc = create_dense_layer(flat, caps2_size*pred_matrix_size , fully_layer_size, 'fully')\n",
    "    \n",
    "    return(fully_connected, weights_conv1, weights_primary, weight_matrix, weights_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(learning_rate,beta1,beta2,caps1_size,caps2_size,pred_matrix_size,conv1_channels,conv1_filter,primaryCaps_channels,primaryCaps_filter,routing_rounds,fully_layer_size,stride_conv1,stride_conv2, lambda1, lambda2, lambda3, lambda4):\n",
    "    X1 = tf.placeholder(tf.float32, [None, 105, 105, 1])\n",
    "    X2 = tf.placeholder(tf.float32, [None, 105, 105, 1])\n",
    "    y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    with tf.variable_scope('siamese') as scope:\n",
    "        output1, weights_conv1_1, weights_primary_1, weight_matrix_1, weights_fc_1 = create_network(caps1_size, caps2_size,pred_matrix_size,conv1_channels,conv1_filter,primaryCaps_channels,primaryCaps_filter,routing_rounds,X1,fully_layer_size,stride_conv1,stride_conv2)\n",
    "        scope.reuse_variables()\n",
    "        output2, _, _, _, _ = create_network(caps1_size, caps2_size,pred_matrix_size,conv1_channels,conv1_filter,primaryCaps_channels,primaryCaps_filter,routing_rounds,X2,fully_layer_size,stride_conv1,stride_conv2)\n",
    "\n",
    "    l1_dis = tf.abs(tf.subtract(output1, output2))\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([fully_layer_size, 1], stddev=0.03), name='w_final')\n",
    "    bias = tf.Variable(tf.truncated_normal([1], stddev=0.01), name='b_final')\n",
    "    fully_final = tf.add(tf.matmul(l1_dis, weights), bias)\n",
    "    y_estimate = tf.nn.sigmoid(fully_final)\n",
    "\n",
    "    cross_entropy = tf.add(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = fully_final)), lambda1 * tf.nn.l2_loss(weights_conv1_1) +\n",
    "                           lambda2 * tf.nn.l2_loss(weights_primary_1) +\n",
    "                           lambda3 * tf.nn.l2_loss(weight_matrix_1) +\n",
    "                           lambda4 * tf.nn.l2_loss(weights_fc_1))\n",
    "    \n",
    "    optimiser = tf.train.AdamOptimizer(learning_rate = learning_rate, beta1=beta1, beta2=beta2).minimize(cross_entropy)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.round(y_estimate), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype = tf.float32))\n",
    "    \n",
    "    return(optimiser, cross_entropy, accuracy, X1, X2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0161\n",
    "beta1 = 0.9999\n",
    "beta2 = 0.9647\n",
    "caps1_size = 4\n",
    "caps2_size = 32\n",
    "pred_matrix_size = 8\n",
    "conv1_channels = 160\n",
    "conv1_filter = 10\n",
    "primaryCaps_channels = 48\n",
    "primaryCaps_filter = 14\n",
    "routing_rounds = 2\n",
    "fully_layer_size = 64\n",
    "stride_conv1 = 3\n",
    "stride_conv2 = 3\n",
    "batch_size = 48\n",
    "lambda1 = 8.097e-06\n",
    "lambda2 = 1.412e-08\n",
    "lambda3 = 4.705e-06\n",
    "lambda4 = 0.0004176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 20\n",
      "batch: 40\n",
      "batch: 60\n",
      "batch: 80\n",
      "batch: 100\n",
      "batch: 120\n",
      "batch: 140\n",
      "batch: 160\n",
      "batch: 180\n",
      "batch: 200\n",
      "epoch: 0\n",
      "batch: 220\n",
      "batch: 240\n",
      "batch: 260\n",
      "batch: 280\n",
      "batch: 300\n",
      "batch: 320\n",
      "batch: 340\n",
      "batch: 360\n",
      "batch: 380\n",
      "batch: 400\n",
      "epoch: 1\n",
      "batch: 420\n",
      "batch: 440\n",
      "batch: 460\n",
      "batch: 480\n",
      "batch: 500\n",
      "batch: 520\n",
      "batch: 540\n",
      "batch: 560\n",
      "batch: 580\n",
      "batch: 600\n",
      "batch: 620\n",
      "epoch: 2\n",
      "batch: 640\n",
      "batch: 660\n",
      "batch: 680\n",
      "batch: 700\n",
      "batch: 720\n",
      "batch: 740\n",
      "batch: 760\n",
      "batch: 780\n",
      "batch: 800\n",
      "batch: 820\n",
      "epoch: 3\n",
      "batch: 840\n",
      "batch: 860\n",
      "batch: 880\n",
      "batch: 900\n",
      "batch: 920\n",
      "batch: 940\n",
      "batch: 960\n",
      "batch: 980\n",
      "batch: 1000\n",
      "batch: 1020\n",
      "epoch: 4\n",
      "batch: 1060\n",
      "batch: 1080\n",
      "batch: 1100\n",
      "batch: 1120\n",
      "batch: 1140\n",
      "batch: 1160\n",
      "batch: 1180\n",
      "batch: 1200\n",
      "batch: 1220\n",
      "batch: 1240\n",
      "epoch: 5\n",
      "batch: 1260\n",
      "batch: 1280\n",
      "batch: 1300\n",
      "batch: 1320\n",
      "batch: 1340\n",
      "batch: 1360\n",
      "batch: 1380\n",
      "batch: 1400\n",
      "batch: 1420\n",
      "batch: 1440\n",
      "epoch: 6\n",
      "batch: 1460\n",
      "batch: 1480\n",
      "batch: 1500\n",
      "batch: 1520\n",
      "batch: 1540\n",
      "batch: 1560\n",
      "batch: 1580\n",
      "batch: 1600\n",
      "batch: 1620\n",
      "batch: 1640\n",
      "batch: 1660\n",
      "epoch: 7\n",
      "batch: 1680\n",
      "batch: 1700\n",
      "batch: 1720\n",
      "batch: 1740\n",
      "batch: 1760\n",
      "batch: 1780\n",
      "batch: 1800\n",
      "batch: 1820\n",
      "batch: 1840\n",
      "batch: 1860\n",
      "epoch: 8\n",
      "batch: 1880\n",
      "batch: 1900\n",
      "batch: 1920\n",
      "batch: 1940\n",
      "batch: 1960\n",
      "batch: 1980\n",
      "batch: 2000\n",
      "batch: 2020\n",
      "batch: 2040\n",
      "batch: 2060\n",
      "test_acc: 0.6720430110731432\n",
      "epoch: 9\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "epsilon = 1e-7\n",
    "\n",
    "#If run on AWS\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "\n",
    "optimiser, cross_entropy, accuracy, X1, X2, y= create_graph(learning_rate,beta1,beta2,caps1_size,caps2_size,pred_matrix_size,conv1_channels,conv1_filter,primaryCaps_channels,primaryCaps_filter,routing_rounds,fully_layer_size,stride_conv1,stride_conv2, lambda1, lambda2, lambda3, lambda4)\n",
    "accuracy_summary = tf.summary.scalar(\"Training_Accuracy_rotation_test1\", accuracy)\n",
    "summaries_dir = '../../logs-tensorboard'\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "    \n",
    "with tf.Session(config=config) as sess:\n",
    "    train_writer = tf.summary.FileWriter(summaries_dir + '/train', sess.graph)\n",
    "    validation_writer = tf.summary.FileWriter(summaries_dir + '/val')\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    init_op.run()\n",
    "        \n",
    "    total_batch = int(10000/batch_size)\n",
    "    total_batch_val = int(3000/batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        k=0\n",
    "        batch_train1, batch_train2, batch_trainy = shuffle(train_set[0],train_set[1], train_labels)\n",
    "        for i in range(1+(total_batch*epoch), total_batch+(total_batch*epoch)):\n",
    "            #batch_x1, batch_x2, batch_y = shuffle(train_set[0],train_set[1], train_labels, n_samples = batch_size)\n",
    "            summary, a = sess.run([summary_op, optimiser], feed_dict={X1: batch_train1[k*batch_size:(k+1)*batch_size], X2: batch_train2[k*batch_size:(k+1)*batch_size], y: batch_trainy[k*batch_size:(k+1)*batch_size]})\n",
    "            train_writer.add_summary(summary, i)\n",
    "            train_writer.flush()\n",
    "            if i % 20 == 0:\n",
    "                print('batch:', i)\n",
    "            \n",
    "            #batch_x1, batch_x2, batch_y = shuffle(val_set[0],val_set[1], val_labels, n_samples = batch_size)\n",
    "            #summary = sess.run(summary_op, feed_dict={X1: batch_x1, X2: batch_x2, y: batch_y})\n",
    "            #validation_writer.add_summary(summary, i)\n",
    "            #validation_writer.flush()\n",
    "            k += 1\n",
    "        if epoch == epochs-1:\n",
    "            test_val = 0\n",
    "            batch_x1, batch_x2, batch_y = shuffle(test_set[0],test_set[1], test_labels)\n",
    "            for iterations in range(total_batch_val):\n",
    "                test_acc = sess.run(accuracy, feed_dict={X1: batch_x1[iterations*batch_size:(iterations+1)*batch_size], X2: batch_x2[iterations*batch_size:(iterations+1)*batch_size], y: batch_y[iterations*batch_size:(iterations+1)*batch_size]})\n",
    "                test_val += test_acc\n",
    "            test_accuracy = test_val/total_batch_val\n",
    "            print('test_acc:', test_accuracy)\n",
    "        print('epoch:', epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Test accuracies of iterations:\n",
    "1.  0.8649193567614402\n",
    "2.\t0.8615591420281318\n",
    "3.\t0.8736559123762192\n",
    "4.  0.8481182811721679\n",
    "5.  0.8571908627786944\n",
    "6.  0.8595430139572390\n",
    "7.  0.8555107539699923\n",
    "8.  0.8484542985116282\n",
    "9.  0.8450940847396851\n",
    "10. 0.8242607520472619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = np.array([0.8649193567614402,\n",
    "    0.8615591420281318,\n",
    "    0.8736559123762192,\n",
    "    0.8481182811721679,\n",
    "    0.8571908627786944,\n",
    "    0.8595430139572390,\n",
    "    0.8555107539699923,\n",
    "    0.8484542985116282,\n",
    "    0.8450940847396851,\n",
    "    0.8242607520472619])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853830645834246\n",
      "0.012785607794073396\n"
     ]
    }
   ],
   "source": [
    "meanscore = test_scores.mean()\n",
    "print(test_scores.mean())\n",
    "print(test_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test accuracies of rotated iterations:\n",
    "\n",
    "1.  0.7745295714947485\n",
    "2.  0.8121639788150787\n",
    "3.  0.7678091401054014\n",
    "4.  0.7217741906642914\n",
    "5.  0.8007392498754686"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = np.array([0.7745295714947485,\n",
    "0.8121639788150787,\n",
    "0.7678091401054014,\n",
    "0.7217741906642914,\n",
    "0.8007392498754686])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7754032261909977"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031393546780054675"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "affine rotation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 0.6901881718827833\n",
    "2. 0.7006048396710427\n",
    "3. 0.7043010717438113\n",
    "4. 0.6649865618636531\n",
    "5. 0.6747311843979743\n",
    "6. 0.661290324022693\n",
    "7. 0.6790994631667291\n",
    "8. 0.6599462368795949\n",
    "9. 0.6844758064516129\n",
    "10. 0.6720430110731432"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
