{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdeblaauw/anaconda3/envs/research-paper/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import numpy.random as rnd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4ac1349c3b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "data_subsets = ['train', 'val']\n",
    "data = {}\n",
    "\n",
    "for name in data_subsets:\n",
    "    with BytesIO() as files:\n",
    "        path = \"omniglot_images/\" +name+ \".pickle\"\n",
    "        s3.Bucket(\"research-paper-omniglot-data\").download_fileobj(path, files)\n",
    "        files.seek(0)    # move back to the beginning after writing\n",
    "        (X,c) = pickle.load(files)\n",
    "        data[name] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ../omniglot_images/train.pickle\n",
      "loading data from ../omniglot_images/val.pickle\n",
      "loading data from ../omniglot_images/test.pickle\n"
     ]
    }
   ],
   "source": [
    "path = '../omniglot_images/'\n",
    "data_subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "data = {}\n",
    "categories = {}\n",
    "info = {}\n",
    "        \n",
    "for name in data_subsets:\n",
    "    file_path = os.path.join(path, name + \".pickle\")\n",
    "    print(\"loading data from {}\".format(file_path))\n",
    "    with open(file_path,\"rb\") as f:\n",
    "        (X,c) = pickle.load(f)\n",
    "        data[name] = X\n",
    "        categories[name] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_data(size, s='train'):\n",
    "    #get train data and shape\n",
    "    X=data[s]\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    #initialize 2 empty arrays for the input size in a list\n",
    "    pairs=[np.zeros((size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    #initialize vector for the targets\n",
    "    targets=np.zeros((size,1))\n",
    "    \n",
    "    for x in range(size):\n",
    "        #randomly sample one class (character)\n",
    "        category = rnd.choice(n_classes,1,replace=False)\n",
    "        #randomly sample one example from class (1-20 characters)\n",
    "        idx_1 = rnd.randint(0, n_examples)\n",
    "        pairs[0][x,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        #randomly sample again one example from class and add last class with modulo\n",
    "        # ..to ensure not same class pairs are created\n",
    "        idx_2 = (idx_1 + rnd.randint(0, n_examples)) % n_examples\n",
    "        #pick images of different class for 1st half and same class for 2nd half\n",
    "        if x >= size // 2:\n",
    "            category_2 = category\n",
    "            targets[x] = 1\n",
    "        else: \n",
    "        #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "        # ..different category\n",
    "            idx_2 = rnd.randint(0, n_examples) \n",
    "            category_2 = (category + rnd.randint(1,n_classes)) % n_classes\n",
    "            targets[x] = 0\n",
    "        pairs[1][x,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "        \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, train_labels = create_train_data(10000)\n",
    "val_set, val_labels = create_train_data(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loguniform(low=0, high=1, size=None, base=np.exp(1)):\n",
    "    return(np.power(base, np.random.uniform(np.log(low), np.log(high), size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniform_sample(low=0, high=1, size=None):\n",
    "    return(np.random.uniform(low, high, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns random integer from discrete uniform distribution with closed interval [low,high]\n",
    "def random_integer(low=0, high=10, size=None):\n",
    "    return(np.random.randint(low, high + 1, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorical_random(input_vector, size=None):\n",
    "    return(np.random.choice(input_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(input_vector, axis):\n",
    "    normalised_input = tf.reduce_sum(tf.square(input_vector), axis = axis, keepdims = True)\n",
    "    scale = tf.divide(normalised_input, tf.add(normalised_input, 1.))\n",
    "    vector = tf.divide(input_vector, tf.sqrt(tf.add(normalised_input, epsilon)))\n",
    "    output = tf.multiply(scale, vector)\n",
    "    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutional(input_data, conv_shape, stride_shape, name, relu=True):\n",
    "    weights = tf.get_variable('W'+name, initializer=tf.truncated_normal(conv_shape, stddev=0.3))\n",
    "    bias = tf.get_variable('B'+name, initializer=tf.truncated_normal([conv_shape[-1]], stddev=0.3))\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, stride_shape, padding = 'VALID')\n",
    "    out_layer_bias = tf.add(out_layer, bias)\n",
    "    \n",
    "    if relu == True:\n",
    "        out_layer_final = tf.nn.relu(out_layer_bias)\n",
    "        return(out_layer_final, weights)\n",
    "\n",
    "    return(out_layer_bias, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primarycaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def primarycaps(input_data, conv_shape, stride_shape, primaryCaps_channels, caps1_size, caps2_size, pose_size, batch):\n",
    "    output, weights_primary = convolutional(input_data, conv_shape, stride_shape, relu=False, name='primaryCaps')\n",
    "    filter_size = output.get_shape().as_list()[1]\n",
    "    caps1_raw = tf.reshape(output, [-1,filter_size*filter_size*primaryCaps_channels,caps1_size], name='caps1_raw')\n",
    "    caps1_output = squash(caps1_raw, axis=-1)\n",
    "    caps1_output_expand = tf.expand_dims(caps1_output, axis=-1)\n",
    "    caps1_output_expand2 = tf.expand_dims(caps1_output_expand, axis=2)\n",
    "    caps1_output_expand2_tiled = tf.tile(caps1_output_expand2, [1,1,caps2_size,1,1], name = 'caps1_out_tiled')\n",
    "    \n",
    "    weight_matrix = tf.get_variable('Weight_matrix', initializer=tf.truncated_normal([filter_size*filter_size*primaryCaps_channels, caps2_size, pose_size, caps1_size], stddev=0.1))\n",
    "    #weight_matrix_tiled = tf.tile(weight_matrix, [batch, 1, 1, 1, 1], name = 'W_matrix_tiled')\n",
    "    #caps2_predicted = tf.matmul(weight_matrix_tiled, caps1_output_expand2_tiled, name='caps2_predicted')\n",
    "    caps2_predicted = tf.einsum('abdc,iabcf->iabdf', weight_matrix, caps1_output_expand2_tiled)\n",
    "    \n",
    "    return(caps2_predicted, weights_primary, weight_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing by agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def routing_by_agreement(input_data, caps2_size, rounds, batch):\n",
    "    raw_weights = tf.zeros([batch, input_data.get_shape().as_list()[1], caps2_size, 1, 1], name = 'raw_weights')\n",
    "    \n",
    "    for i in range(rounds):\n",
    "        routing_weights = tf.nn.softmax(raw_weights, axis=2, name = 'routing_weights' + str(i))\n",
    "        weighted_predictions = tf.multiply(routing_weights, input_data, name = 'weighted_predictions' + str(i))\n",
    "        weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, name = 'weighted_sum' + str(i), keepdims = True)\n",
    "        caps2_output = squash(weighted_sum, axis=-2)\n",
    "    \n",
    "        #caps2_output_tiled = tf.tile(caps2_output, [1, input_data.get_shape().as_list()[1], 1, 1, 1], name = 'caps2_output_tiled'+ str(i))\n",
    "        #agreement = tf.matmul(input_data, caps2_output_tiled, transpose_a = True, name = 'agreement'+ str(i))\n",
    "        agreement = tf.einsum('iabcd,ifbcd->iabcd', input_data, caps2_output)\n",
    "        raw_weights = tf.add(raw_weights, agreement, name = 'raw_weights' + str(i))\n",
    "        \n",
    "    return(caps2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dense_layer(input_data, input_shape, neurons, name):\n",
    "    weights = tf.get_variable(name+'_W', initializer=tf.truncated_normal([input_shape, neurons], stddev=0.2))\n",
    "    bias = tf.get_variable(name+'b', initializer=tf.truncated_normal([neurons], mean=0.5, stddev=0.02))\n",
    "    fully_connected = tf.add(tf.matmul(input_data, weights), bias)\n",
    "    out_dense_activation = tf.nn.sigmoid(fully_connected)\n",
    "    \n",
    "    return(out_dense_activation, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(caps1_size, caps2_size,pred_matrix_size,conv1_channels,conv1_filter,primaryCaps_channels,primaryCaps_filter,routing_rounds,X,fully_layer_size,stride_conv1,stride_conv2):\n",
    "    conv1, weights_conv1 = convolutional(X, [conv1_filter,conv1_filter,X.get_shape().as_list()[-1],conv1_channels],[1,stride_conv1,stride_conv1,1], name='conv')\n",
    "    primary, weights_primary, weight_matrix = primarycaps(conv1, [primaryCaps_filter,primaryCaps_filter,conv1.get_shape().as_list()[-1],\n",
    "                              primaryCaps_channels*caps1_size], [1,stride_conv2,stride_conv2,1],primaryCaps_channels,\n",
    "                      caps1_size,caps2_size, pred_matrix_size, batch=tf.shape(X)[0])\n",
    "    output = routing_by_agreement(primary, caps2_size, routing_rounds, batch=tf.shape(X)[0])\n",
    "    flat = tf.reshape(output, [-1, caps2_size*pred_matrix_size])\n",
    "    fully_connected, weights_fc = create_dense_layer(flat, caps2_size*pred_matrix_size , fully_layer_size, 'fully')\n",
    "    \n",
    "    return(fully_connected, weights_conv1, weights_primary, weight_matrix, weights_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph(learning_rate,beta1,beta2,caps1_size,caps2_size,pred_matrix_size,conv1_channels,conv1_filter,primaryCaps_channels,primaryCaps_filter,routing_rounds,fully_layer_size,stride_conv1,stride_conv2, lambda1, lambda2, lambda3, lambda4):\n",
    "    X1 = tf.placeholder(tf.float32, [None, 105, 105, 1])\n",
    "    X2 = tf.placeholder(tf.float32, [None, 105, 105, 1])\n",
    "    y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    with tf.variable_scope('siamese') as scope:\n",
    "        output1, weights_conv1_1, weights_primary_1, weight_matrix_1, weights_fc_1 = create_network(caps1_size, caps2_size,pred_matrix_size,conv1_channels,conv1_filter,primaryCaps_channels,primaryCaps_filter,routing_rounds,X1,fully_layer_size,stride_conv1,stride_conv2)\n",
    "        scope.reuse_variables()\n",
    "        output2, _, _, _, _ = create_network(caps1_size, caps2_size,pred_matrix_size,conv1_channels,conv1_filter,primaryCaps_channels,primaryCaps_filter,routing_rounds,X2,fully_layer_size,stride_conv1,stride_conv2)\n",
    "\n",
    "    l1_dis = tf.abs(tf.subtract(output1, output2))\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([fully_layer_size, 1], stddev=0.03), name='w_final')\n",
    "    bias = tf.Variable(tf.truncated_normal([1], stddev=0.01), name='b_final')\n",
    "    fully_final = tf.add(tf.matmul(l1_dis, weights), bias)\n",
    "    y_estimate = tf.nn.sigmoid(fully_final)\n",
    "\n",
    "    cross_entropy = tf.add(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = fully_final)), lambda1 * tf.nn.l2_loss(weights_conv1_1) +\n",
    "                           lambda2 * tf.nn.l2_loss(weights_primary_1) +\n",
    "                           lambda3 * tf.nn.l2_loss(weight_matrix_1) +\n",
    "                           lambda4 * tf.nn.l2_loss(weights_fc_1))\n",
    "    \n",
    "    optimiser = tf.train.AdamOptimizer(learning_rate = learning_rate, beta1=beta1, beta2=beta2).minimize(cross_entropy)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.round(y_estimate), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype = tf.float32))\n",
    "    \n",
    "    return(optimiser, cross_entropy, accuracy, X1, X2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_size(batch_size,conv1_filter,stride_conv1,primaryCaps_filter,stride_conv2,primaryCaps_channels,caps1_size,caps2_size,pred_matrix_size):\n",
    "    conv1_output_filter = np.floor((105-conv1_filter)/stride_conv1 + 1)\n",
    "    conv2_output_filter = np.floor((conv1_output_filter - primaryCaps_filter)/stride_conv2 + 1)\n",
    "    size_tensor = batch_size*conv2_output_filter*conv2_output_filter*primaryCaps_channels*caps2_size*pred_matrix_size*caps1_size\n",
    "    \n",
    "    if size_tensor < 850000000:\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "beta1 = 0.999\n",
    "beta2 = 0.999\n",
    "caps1_size = 4\n",
    "caps2_size = 16\n",
    "pred_matrix_size = 16\n",
    "conv1_channels = 16\n",
    "conv1_filter = 3\n",
    "primaryCaps_channels = 16\n",
    "primaryCaps_filter = 3\n",
    "routing_rounds = 1\n",
    "fully_layer_size = 16\n",
    "stride_conv1 = 1\n",
    "stride_conv2 = 1\n",
    "batch_size = 32\n",
    "lambda1 = 0.000001\n",
    "lambda2 = 0.000001\n",
    "lambda3 = 0.000001\n",
    "lambda4 = 0.000001\n",
    "\n",
    "settings = {'validation_accuracy' : 0,\n",
    "            'learning_rate' : 0.1,\n",
    "           'beta1' : 0.999,\n",
    "           'beta2' : 0.999,\n",
    "           'caps1_size' : 4,\n",
    "           'caps2_size' : 16,\n",
    "           'pred_matrix_size' : 16,\n",
    "           'conv1_channels' : 16,\n",
    "           'conv1_filter' : 3,\n",
    "           'primaryCaps_channels' : 16,\n",
    "           'primaryCaps_filter' : 3,\n",
    "           'routing_rounds' : 1,\n",
    "           'fully_layer_size' : 16,\n",
    "           'stride_conv1' : 1,\n",
    "           'stride_conv2' : 1,\n",
    "           'batch_size' : 32,\n",
    "           'lambda1' : 0.000001,\n",
    "           'lambda2' : 0.000001,\n",
    "           'lambda3' : 0.000001,\n",
    "           'lambda4' : 0.000001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n",
      "batch: 8\n",
      "batch: 9\n",
      "batch: 10\n",
      "batch: 11\n",
      "batch: 12\n",
      "batch: 13\n",
      "batch: 14\n",
      "batch: 15\n",
      "batch: 16\n",
      "batch: 17\n",
      "batch: 18\n",
      "batch: 19\n",
      "batch: 20\n",
      "batch: 21\n",
      "batch: 22\n",
      "batch: 23\n",
      "batch: 24\n",
      "batch: 25\n",
      "batch: 26\n",
      "batch: 27\n",
      "batch: 28\n",
      "batch: 29\n",
      "batch: 30\n",
      "batch: 31\n",
      "batch: 32\n",
      "batch: 33\n",
      "batch: 34\n",
      "batch: 35\n",
      "batch: 36\n",
      "batch: 37\n",
      "batch: 38\n",
      "batch: 39\n",
      "batch: 40\n",
      "batch: 41\n",
      "batch: 42\n",
      "batch: 43\n",
      "batch: 44\n",
      "batch: 45\n",
      "batch: 46\n",
      "batch: 47\n",
      "batch: 48\n",
      "batch: 49\n",
      "batch: 50\n",
      "batch: 51\n",
      "batch: 52\n",
      "batch: 53\n",
      "batch: 54\n",
      "batch: 55\n",
      "batch: 56\n",
      "batch: 57\n",
      "batch: 58\n",
      "batch: 59\n",
      "batch: 60\n",
      "batch: 61\n",
      "batch: 62\n",
      "batch: 63\n",
      "batch: 64\n",
      "batch: 65\n",
      "batch: 66\n",
      "batch: 67\n",
      "batch: 68\n",
      "batch: 69\n",
      "batch: 70\n",
      "batch: 71\n",
      "batch: 72\n",
      "batch: 73\n",
      "batch: 74\n",
      "batch: 75\n",
      "batch: 76\n",
      "batch: 77\n",
      "batch: 78\n",
      "batch: 79\n",
      "batch: 80\n",
      "batch: 81\n",
      "batch: 82\n",
      "batch: 83\n",
      "batch: 84\n",
      "batch: 85\n",
      "batch: 86\n",
      "batch: 87\n",
      "batch: 88\n",
      "batch: 89\n",
      "batch: 90\n",
      "batch: 91\n",
      "batch: 92\n",
      "batch: 93\n",
      "batch: 94\n",
      "batch: 95\n",
      "batch: 96\n",
      "batch: 97\n",
      "batch: 98\n",
      "batch: 99\n",
      "batch: 100\n",
      "batch: 101\n",
      "batch: 102\n",
      "batch: 103\n",
      "batch: 104\n",
      "batch: 105\n",
      "batch: 106\n",
      "batch: 107\n",
      "batch: 108\n",
      "batch: 109\n",
      "batch: 110\n",
      "batch: 111\n",
      "batch: 112\n",
      "batch: 113\n",
      "batch: 114\n",
      "batch: 115\n",
      "batch: 116\n",
      "batch: 117\n",
      "batch: 118\n",
      "batch: 119\n",
      "batch: 120\n",
      "batch: 121\n",
      "batch: 122\n",
      "batch: 123\n",
      "batch: 124\n",
      "batch: 125\n",
      "batch: 126\n",
      "batch: 127\n",
      "batch: 128\n",
      "batch: 129\n",
      "batch: 130\n",
      "batch: 131\n",
      "batch: 132\n",
      "batch: 133\n",
      "batch: 134\n",
      "batch: 135\n",
      "batch: 136\n",
      "batch: 137\n",
      "batch: 138\n",
      "batch: 139\n",
      "batch: 140\n",
      "batch: 141\n",
      "batch: 142\n",
      "batch: 143\n",
      "batch: 144\n",
      "batch: 145\n",
      "batch: 146\n",
      "batch: 147\n",
      "batch: 148\n",
      "batch: 149\n",
      "batch: 150\n",
      "batch: 151\n",
      "batch: 152\n",
      "batch: 153\n",
      "batch: 154\n",
      "batch: 155\n",
      "batch: 156\n",
      "batch: 157\n",
      "batch: 158\n",
      "batch: 159\n",
      "batch: 160\n",
      "batch: 161\n",
      "batch: 162\n",
      "batch: 163\n",
      "batch: 164\n",
      "batch: 165\n",
      "batch: 166\n",
      "batch: 167\n",
      "batch: 168\n",
      "batch: 169\n",
      "batch: 170\n",
      "batch: 171\n",
      "batch: 172\n",
      "batch: 173\n",
      "batch: 174\n",
      "batch: 175\n",
      "batch: 176\n",
      "batch: 177\n",
      "batch: 178\n",
      "batch: 179\n",
      "batch: 180\n",
      "batch: 181\n",
      "batch: 182\n",
      "batch: 183\n",
      "batch: 184\n",
      "batch: 185\n",
      "batch: 186\n",
      "batch: 187\n",
      "batch: 188\n",
      "batch: 189\n",
      "batch: 190\n",
      "batch: 191\n",
      "batch: 192\n",
      "batch: 193\n",
      "batch: 194\n",
      "batch: 195\n",
      "batch: 196\n",
      "batch: 197\n",
      "batch: 198\n",
      "batch: 199\n",
      "batch: 200\n",
      "batch: 201\n",
      "batch: 202\n",
      "batch: 203\n",
      "batch: 204\n",
      "batch: 205\n",
      "batch: 206\n",
      "batch: 207\n",
      "avg_cost: 1.0494010606064244\n",
      "train_acc: 0.506109774685823\n",
      "val_acc: 0.526545696200863\n",
      "batch: 0\n",
      "batch: 1\n",
      "batch: 2\n",
      "batch: 3\n",
      "batch: 4\n",
      "batch: 5\n",
      "batch: 6\n",
      "batch: 7\n",
      "batch: 8\n",
      "batch: 9\n",
      "batch: 10\n",
      "batch: 11\n",
      "batch: 12\n",
      "batch: 13\n",
      "batch: 14\n",
      "batch: 15\n",
      "batch: 16\n",
      "batch: 17\n",
      "batch: 18\n",
      "batch: 19\n",
      "batch: 20\n",
      "batch: 21\n",
      "batch: 22\n",
      "batch: 23\n",
      "batch: 24\n",
      "batch: 25\n",
      "batch: 26\n",
      "batch: 27\n",
      "batch: 28\n",
      "batch: 29\n",
      "batch: 30\n",
      "batch: 31\n",
      "batch: 32\n",
      "batch: 33\n",
      "batch: 34\n",
      "batch: 35\n",
      "batch: 36\n",
      "batch: 37\n",
      "batch: 38\n",
      "batch: 39\n",
      "batch: 40\n",
      "batch: 41\n",
      "batch: 42\n",
      "batch: 43\n",
      "batch: 44\n",
      "batch: 45\n",
      "batch: 46\n",
      "batch: 47\n",
      "batch: 48\n",
      "batch: 49\n",
      "batch: 50\n",
      "batch: 51\n",
      "batch: 52\n",
      "batch: 53\n",
      "batch: 54\n",
      "batch: 55\n",
      "batch: 56\n",
      "batch: 57\n",
      "batch: 58\n",
      "batch: 59\n",
      "batch: 60\n",
      "batch: 61\n",
      "batch: 62\n",
      "batch: 63\n",
      "batch: 64\n",
      "batch: 65\n",
      "batch: 66\n",
      "batch: 67\n",
      "batch: 68\n",
      "batch: 69\n",
      "batch: 70\n",
      "batch: 71\n",
      "batch: 72\n",
      "batch: 73\n",
      "batch: 74\n",
      "batch: 75\n",
      "batch: 76\n",
      "batch: 77\n",
      "batch: 78\n",
      "batch: 79\n",
      "batch: 80\n",
      "batch: 81\n",
      "batch: 82\n",
      "batch: 83\n",
      "batch: 84\n",
      "batch: 85\n",
      "batch: 86\n",
      "batch: 87\n",
      "batch: 88\n",
      "batch: 89\n",
      "batch: 90\n",
      "batch: 91\n",
      "batch: 92\n",
      "batch: 93\n",
      "batch: 94\n",
      "batch: 95\n",
      "batch: 96\n",
      "batch: 97\n",
      "batch: 98\n",
      "batch: 99\n",
      "batch: 100\n",
      "batch: 101\n",
      "batch: 102\n",
      "batch: 103\n",
      "batch: 104\n",
      "batch: 105\n",
      "batch: 106\n",
      "batch: 107\n",
      "batch: 108\n",
      "batch: 109\n",
      "batch: 110\n",
      "batch: 111\n",
      "batch: 112\n",
      "batch: 113\n",
      "batch: 114\n",
      "batch: 115\n",
      "batch: 116\n",
      "batch: 117\n",
      "batch: 118\n",
      "batch: 119\n",
      "batch: 120\n",
      "batch: 121\n",
      "batch: 122\n",
      "batch: 123\n",
      "batch: 124\n",
      "batch: 125\n",
      "batch: 126\n",
      "batch: 127\n",
      "batch: 128\n",
      "batch: 129\n",
      "batch: 130\n",
      "batch: 131\n",
      "batch: 132\n",
      "batch: 133\n",
      "batch: 134\n",
      "batch: 135\n",
      "batch: 136\n",
      "batch: 137\n",
      "batch: 138\n",
      "batch: 139\n",
      "batch: 140\n",
      "batch: 141\n",
      "batch: 142\n",
      "batch: 143\n",
      "batch: 144\n",
      "batch: 145\n",
      "batch: 146\n",
      "batch: 147\n",
      "batch: 148\n",
      "batch: 149\n",
      "batch: 150\n",
      "batch: 151\n",
      "batch: 152\n",
      "batch: 153\n",
      "batch: 154\n",
      "batch: 155\n",
      "batch: 156\n",
      "batch: 157\n",
      "batch: 158\n",
      "batch: 159\n",
      "batch: 160\n",
      "batch: 161\n",
      "batch: 162\n",
      "batch: 163\n",
      "batch: 164\n",
      "batch: 165\n",
      "batch: 166\n",
      "batch: 167\n",
      "batch: 168\n",
      "batch: 169\n",
      "batch: 170\n",
      "batch: 171\n",
      "batch: 172\n",
      "batch: 173\n",
      "batch: 174\n",
      "batch: 175\n",
      "batch: 176\n",
      "batch: 177\n",
      "batch: 178\n",
      "batch: 179\n",
      "batch: 180\n",
      "batch: 181\n",
      "batch: 182\n",
      "batch: 183\n",
      "batch: 184\n",
      "batch: 185\n",
      "batch: 186\n",
      "batch: 187\n",
      "batch: 188\n",
      "batch: 189\n",
      "batch: 190\n",
      "batch: 191\n",
      "batch: 192\n",
      "batch: 193\n",
      "batch: 194\n",
      "batch: 195\n",
      "batch: 196\n",
      "batch: 197\n",
      "batch: 198\n",
      "batch: 199\n",
      "batch: 200\n",
      "batch: 201\n",
      "batch: 202\n",
      "batch: 203\n",
      "batch: 204\n",
      "batch: 205\n",
      "batch: 206\n",
      "batch: 207\n",
      "avg_cost: 0.8776910709073913\n",
      "train_acc: 0.603866186995919\n",
      "val_acc: 0.6239919374066014\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Do not use tf.reset_default_graph() to clear nested graphs. If you need a cleared graph, exit the nesting and create a new graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3b12fb533c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda4'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/research-paper/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mreset_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5514\u001b[0m   \"\"\"\n\u001b[1;32m   5515\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_default_graph_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cleared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5516\u001b[0;31m     raise AssertionError(\"Do not use tf.reset_default_graph() to clear \"\n\u001b[0m\u001b[1;32m   5517\u001b[0m                          \u001b[0;34m\"nested graphs. If you need a cleared graph, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5518\u001b[0m                          \"exit the nesting and create a new graph.\")\n",
      "\u001b[0;31mAssertionError\u001b[0m: Do not use tf.reset_default_graph() to clear nested graphs. If you need a cleared graph, exit the nesting and create a new graph."
     ]
    }
   ],
   "source": [
    "random_search_iterations = 2\n",
    "epochs = 2\n",
    "epsilon = 1e-7\n",
    "best_acc_val = 0\n",
    "\n",
    "#If run on AWS\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "\n",
    "for i in range(random_search_iterations):\n",
    "    \n",
    "    size_tensor = False\n",
    "    \n",
    "    while not size_tensor:\n",
    "        learning_rate = loguniform(1e-5, 0.1)\n",
    "        beta1 = uniform_sample(0.,1.)\n",
    "        beta2 = uniform_sample(0.,1.)\n",
    "        caps1_size = categorical_random([4,8,16])\n",
    "        caps2_size = categorical_random([16,32,48,64])\n",
    "        pred_matrix_size = categorical_random([4,8,16])\n",
    "        conv1_channels = categorical_random([16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256])\n",
    "        conv1_filter = random_integer(3,35)\n",
    "        primaryCaps_channels = categorical_random([16,32,48,64])\n",
    "        primaryCaps_filter = random_integer(3,20)\n",
    "        routing_rounds = random_integer(1,5)\n",
    "        fully_layer_sizec = categorical_random([16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256])\n",
    "        stride_conv1 = random_integer(1,3)\n",
    "        stride_conv2 = random_integer(1,3)\n",
    "        batch_size = categorical_random([48])\n",
    "        lambda1 = loguniform(0.00000001, 0.1)\n",
    "        lambda2 = loguniform(0.00000001, 0.1)\n",
    "        lambda3 = loguniform(0.00000001, 0.1)\n",
    "        lambda4 = loguniform(0.00000001, 0.1)\n",
    "        size_tensor = check_size(batch_size,conv1_filter,stride_conv1,primaryCaps_filter,stride_conv2,primaryCaps_channels,caps1_size,caps2_size,pred_matrix_size)\n",
    "        \n",
    "    optimiser, cross_entropy, accuracy, X1, X2, y = create_graph(learning_rate,beta1,beta2,caps1_size,caps2_size,pred_matrix_size,conv1_channels,conv1_filter,primaryCaps_channels,primaryCaps_filter,routing_rounds,fully_layer_size,stride_conv1,stride_conv2, lambda1, lambda2, lambda3, lambda4)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        init_op.run()\n",
    "\n",
    "        total_batch = int(10000/batch_size)\n",
    "        total_batch_val = int(3000/batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            acc = 0\n",
    "            for i in range(total_batch):\n",
    "                batch_x1, batch_x2, batch_y = shuffle(train_set[0],train_set[1], train_labels, n_samples = batch_size)\n",
    "                a, c, accur= sess.run([optimiser, cross_entropy, accuracy], feed_dict={X1: batch_x1, X2: batch_x2, y: batch_y})\n",
    "                avg_cost += c/total_batch\n",
    "                acc += accur\n",
    "                print('batch:', i)\n",
    "            print('avg_cost:', avg_cost)\n",
    "            print('train_acc:', acc/total_batch)\n",
    "            acc_val = 0\n",
    "            for iterations in range(total_batch_val):\n",
    "                batch_x1, batch_x2, batch_y = shuffle(val_set[0],val_set[1], val_labels, n_samples = batch_size)\n",
    "                val_acc = sess.run(accuracy, feed_dict={X1: batch_x1, X2: batch_x2, y: batch_y})\n",
    "                acc_val += val_acc\n",
    "            validation_accuracy = acc_val/total_batch_val\n",
    "            print('val_acc:', validation_accuracy)\n",
    "            \n",
    "        if validation_accuracy > best_acc_val:\n",
    "            best_acc_val = validation_accuracy\n",
    "            settings['validation_accuracy'] = best_acc_val\n",
    "            settings['learning_rate'] = learning_rate\n",
    "            settings['beta1'] = beta1\n",
    "            settings['beta2'] = beta2\n",
    "            settings['caps1_size'] = caps1_size\n",
    "            settings['caps2_size'] = caps2_size\n",
    "            settings['pred_matrix_size'] = pred_matrix_size\n",
    "            settings['conv1_channels'] = conv1_channels\n",
    "            settings['conv1_filter'] = conv1_filter\n",
    "            settings['primaryCaps_channels'] = primaryCaps_channels\n",
    "            settings['primaryCaps_filter'] = primaryCaps_filter\n",
    "            settings['routing_rounds'] = routing_rounds\n",
    "            settings['fully_layer_size'] = fully_layer_size\n",
    "            settings['stride_conv1'] = stride_conv1\n",
    "            settings['stride_conv2'] = stride_conv2\n",
    "            settings['batch_size'] = batch_size\n",
    "            settings['lambda1'] = lambda1\n",
    "            settings['lambda2'] = lambda2\n",
    "            settings['lambda3'] = lambda3\n",
    "            settings['lambda4'] = lambda4\n",
    "            f = open(\"../settings_l2\" + str(i) +\".txt\",\"w\")\n",
    "            f.write( str(settings) )\n",
    "            f.close()\n",
    "    print('Random iteration:', i)       \n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_accuracy': 0.6239919374066014,\n",
       " 'learning_rate': 0.053859986946186884,\n",
       " 'beta1': 0.6210319553385454,\n",
       " 'beta2': 0.41528680073299495,\n",
       " 'caps1_size': 4,\n",
       " 'caps2_size': 64,\n",
       " 'pred_matrix_size': 4,\n",
       " 'conv1_channels': 224,\n",
       " 'conv1_filter': 14,\n",
       " 'primaryCaps_channels': 16,\n",
       " 'primaryCaps_filter': 20,\n",
       " 'routing_rounds': 3,\n",
       " 'fully_layer_size': 16,\n",
       " 'stride_conv1': 3,\n",
       " 'stride_conv2': 3,\n",
       " 'batch_size': 48,\n",
       " 'lambda1': 0.013892346924459198,\n",
       " 'lambda2': 1.1338186150592745e-08,\n",
       " 'lambda3': 3.0832006365500998e-06,\n",
       " 'lambda4': 0.0169846597072021}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research-paper]",
   "language": "python",
   "name": "conda-env-research-paper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
