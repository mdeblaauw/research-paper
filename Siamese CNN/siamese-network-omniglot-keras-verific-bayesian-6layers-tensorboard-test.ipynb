{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "#from time import time\n",
    "#from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "data_subsets = ['train', 'val', 'test']\n",
    "data = {}\n",
    "\n",
    "for name in data_subsets:\n",
    "    with BytesIO() as files:\n",
    "        path = \"omniglot_images/\" +name+ \".pickle\"\n",
    "        s3.Bucket(\"research-paper-omniglot-data\").download_fileobj(path, files)\n",
    "        files.seek(0)    # move back to the beginning after writing\n",
    "        (X,c) = pickle.load(files)\n",
    "        data[name] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = '../../omniglot_images/'\n",
    "data_subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "data = {}\n",
    "categories = {}\n",
    "info = {}\n",
    "        \n",
    "for name in data_subsets:\n",
    "    file_path = os.path.join(path, name + \".pickle\")\n",
    "    print(\"loading data from {}\".format(file_path))\n",
    "    with open(file_path,\"rb\") as f:\n",
    "        (X,c) = pickle.load(f)\n",
    "        data[name] = X\n",
    "        categories[name] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(size, s='train'):\n",
    "    #get train data and shape\n",
    "    X=data[s]\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    #initialize 2 empty arrays for the input size in a list\n",
    "    pairs=[np.zeros((size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    #initialize vector for the targets\n",
    "    targets=np.zeros((size,1))\n",
    "    \n",
    "    for x in range(size):\n",
    "        #randomly sample one class (character)\n",
    "        category = rnd.choice(n_classes,1,replace=False)\n",
    "        #randomly sample one example from class (1-20 characters)\n",
    "        idx_1 = rnd.randint(0, n_examples)\n",
    "        pairs[0][x,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        #randomly sample again one example from class and add last class with modulo\n",
    "        # ..to ensure not same class pairs are created\n",
    "        idx_2 = (idx_1 + rnd.randint(0, n_examples)) % n_examples\n",
    "        #pick images of different class for 1st half and same class for 2nd half\n",
    "        if x >= size // 2:\n",
    "            category_2 = category\n",
    "            targets[x] = 1\n",
    "        else: \n",
    "        #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "        # ..different category\n",
    "            idx_2 = rnd.randint(0, n_examples) \n",
    "            category_2 = (category + rnd.randint(1,n_classes)) % n_classes\n",
    "            targets[x] = 0\n",
    "        pairs[1][x,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "        \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_labels = create_train_data(10000)\n",
    "val_set, val_labels = create_train_data(3000, 'val')\n",
    "test_set, test_labels = create_train_data(3000, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotate test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = test_set[0].shape[0]\n",
    "for i in range(2):\n",
    "    for j in range(images):\n",
    "        img = test_set[i][j,:,:,0]\n",
    "        shift = np.random.randint(low=-10, high=10,size=2)\n",
    "        degrees = np.random.uniform(low=-30, high=30)\n",
    "        img2 = ndimage.rotate(img, degrees, reshape=False, cval = 255, order=0)\n",
    "        new_img = ndimage.shift(img2, shift,  cval = 255, order=0)\n",
    "        test_set[i][j,:,:,0] = new_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5.491e-05\n",
    "reg_layer1 = 1.271e-06\n",
    "reg_layer2 = 1e-08\n",
    "reg_layer3 = 2.734e-05\n",
    "reg_layer4 = 0.005639\n",
    "reg_layer5 = 0.0004877\n",
    "filt_layer1 = 5\n",
    "filt_layer2 = 12\n",
    "filt_layer3 = 7\n",
    "filt_layer4 = 2\n",
    "chan_layer1 = 64\n",
    "chan_layer2 = 112\n",
    "chan_layer3 = 96\n",
    "chan_layer4 = 144\n",
    "fc_layer5 = 1536\n",
    "beta1 = 0.5745\n",
    "beta2 = 0.9999\n",
    "batch = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['learning_rate'] = learning_rate\n",
    "params['reg_layer1'] = reg_layer1\n",
    "params['reg_layer2'] = reg_layer2\n",
    "params['reg_layer2'] = reg_layer2\n",
    "params['reg_layer3'] = reg_layer3\n",
    "params['reg_layer4'] = reg_layer4\n",
    "params['reg_layer5'] = reg_layer5\n",
    "params['filter_layer1'] = filt_layer1\n",
    "params['filter_layer2'] = filt_layer2\n",
    "params['filter_layer3'] = filt_layer3\n",
    "params['filter_layer4'] = filt_layer4\n",
    "params['channel_layer1'] = chan_layer1\n",
    "params['channel_layer2'] = chan_layer2\n",
    "params['channel_layer3'] = chan_layer3\n",
    "params['channel_layer4'] = chan_layer4\n",
    "params['channel_layer5'] = fc_layer5\n",
    "params['beta1'] = beta1\n",
    "params['beta2'] = beta2\n",
    "params['batch'] = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rnd.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rnd.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(**params):\n",
    "    input_shape = (105, 105, 1)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    #build convnet to use in each siamese 'leg'\n",
    "    convnet = Sequential()\n",
    "    convnet.add(Conv2D(params['channel_layer1'],(params['filter_layer1'],params['filter_layer1']),activation='relu',input_shape=input_shape,kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer1']),bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer2'],(params['filter_layer2'],params['filter_layer2']),activation='relu',kernel_regularizer=l2(params['reg_layer2']),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer3'],(params['filter_layer3'],params['filter_layer3']),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer3']),bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer4'],(params['filter_layer4'],params['filter_layer4']),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer4']),bias_initializer=b_init))\n",
    "    convnet.add(Flatten())\n",
    "    convnet.add(Dense(params['channel_layer5'],activation=\"sigmoid\",kernel_regularizer=l2(params['reg_layer5']),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    #call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "    encoded_l = convnet(left_input)\n",
    "    encoded_r = convnet(right_input)\n",
    "    #layer to merge two encoded inputs with the l1 distance between them\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    #call this layer on list of two input tensors.\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    optimizer = Adam(lr=params['learning_rate'], beta_1=params['beta1'], beta_2=params['beta2'])\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "    #siamese_net.count_params()\n",
    "    return(siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, true_val):\n",
    "    acc_bool = np.equal(np.round_(pred), true_val)\n",
    "    acc = np.mean(acc_bool.astype(int))\n",
    "    \n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just as fast as the Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "test start\n",
      "test: 0.7805779569892474\n",
      "Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "###################################\n",
    "\n",
    "\n",
    "siamese_net = create_network(**params)\n",
    "        \n",
    "print(\"!\")\n",
    "batch_size = params['batch']\n",
    "total_batch = int(10000/batch_size)\n",
    "total_batch_val = int(3000/batch_size)\n",
    "epoch = 10\n",
    "summaries_dir = '../../logs-tensorboard/SCNN/'\n",
    "train_writer = tf.summary.FileWriter(summaries_dir + '/train')\n",
    "validation_writer = tf.summary.FileWriter(summaries_dir + '/val')\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "print(\"training\")\n",
    "for i in range(epoch):\n",
    "    train_batch_acc = 0\n",
    "    val_batch_acc = 0\n",
    "    for j in range(1+(total_batch*i), total_batch+(total_batch*i)):\n",
    "        batch_x1, batch_x2, batch_y = shuffle(train_set[0],train_set[1], train_labels, n_samples = batch_size)\n",
    "        loss=siamese_net.train_on_batch([batch_x1, batch_x2],batch_y)\n",
    "        probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "        train_batch_acc1 = accuracy(probs, batch_y)\n",
    "        #train_batch_acc += train_batch_acc1\n",
    "        \n",
    "        summary_train = tf.Summary(value=[tf.Summary.Value(tag=\"summary_rotate_test6\", simple_value=train_batch_acc1),])\n",
    "        \n",
    "        train_writer.add_summary(summary_train, j)\n",
    "        \n",
    "        batch_x1, batch_x2, batch_y = shuffle(val_set[0],val_set[1], val_labels, n_samples = batch_size)\n",
    "        probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "        val_batch_acc1 = accuracy(probs, batch_y)\n",
    "        #val_batch_acc += val_batch_acc1\n",
    "        \n",
    "        summary_val = tf.Summary(value=[tf.Summary.Value(tag=\"summary_rotate_test6\", simple_value= val_batch_acc1),])\n",
    "        \n",
    "        validation_writer.add_summary(summary_val, j)\n",
    "        #print('Loss:', loss)\n",
    "        #print('Batch:', j)\n",
    "    #train_acc = train_batch_acc/total_batch\n",
    "    #val_acc = val_batch_acc/total_batch      \n",
    "    \n",
    "    #print('Train accuracy:', train_acc)\n",
    "    #print('Validation accuracy:', val_acc)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    if i == epoch-1:\n",
    "        test_batch_acc = 0\n",
    "        print('test start')\n",
    "        for k in range(total_batch_val):\n",
    "            batch_x1, batch_x2, batch_y = shuffle(test_set[0],test_set[1], test_labels, n_samples = batch_size)\n",
    "            probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "            test_batch_acc += accuracy(probs, batch_y)\n",
    "        test_acc = test_batch_acc/total_batch_val\n",
    "        print('test:', test_acc)\n",
    "    print('Epoch:', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Siamese CNN test results:\n",
    "1.  0.8434139784946237\n",
    "2.  0.8548387096774196\n",
    "3.  0.8608870967741936\n",
    "4.  0.8541666666666666\n",
    "5.  0.8467741935483872\n",
    "6.  0.8575268817204306\n",
    "7.  0.8484543010752689\n",
    "8.  0.8390456989247307\n",
    "9.  0.8595430107526885\n",
    "10. 0.8548387096774194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.array([0.8434139784946237,\n",
    "0.8548387096774196,\n",
    "0.8608870967741936,\n",
    "0.8541666666666666,\n",
    "0.8467741935483872,\n",
    "0.8575268817204306,\n",
    "0.8484543010752689,\n",
    "0.8390456989247307,\n",
    "0.8595430107526885,\n",
    "0.8548387096774194])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.851948924731183"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006845608375401687"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test accuracies of rotated iterations:\n",
    "\n",
    "1. 0.7906586021505376\n",
    "2. 0.7691532258064514\n",
    "3. 0.7587365591397851\n",
    "4. 0.7768817204301076\n",
    "5. 0.7805779569892474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = np.array([0.7906586021505376,\n",
    "0.7691532258064514,\n",
    "0.7587365591397851,\n",
    "0.7768817204301076,\n",
    "0.7805779569892474])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7752016129032258"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010754788101400391"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
