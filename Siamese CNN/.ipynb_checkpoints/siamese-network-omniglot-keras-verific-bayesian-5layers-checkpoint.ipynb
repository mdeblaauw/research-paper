{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "data_subsets = ['train', 'val']\n",
    "data = {}\n",
    "\n",
    "for name in data_subsets:\n",
    "    with BytesIO() as files:\n",
    "        path = \"omniglot_images/\" +name+ \".pickle\"\n",
    "        s3.Bucket(\"research-paper-omniglot-data\").download_fileobj(path, files)\n",
    "        files.seek(0)    # move back to the beginning after writing\n",
    "        (X,c) = pickle.load(files)\n",
    "        data[name] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = '../../omniglot_images/'\n",
    "data_subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "data = {}\n",
    "categories = {}\n",
    "info = {}\n",
    "        \n",
    "for name in data_subsets:\n",
    "    file_path = os.path.join(path, name + \".pickle\")\n",
    "    print(\"loading data from {}\".format(file_path))\n",
    "    with open(file_path,\"rb\") as f:\n",
    "        (X,c) = pickle.load(f)\n",
    "        data[name] = X\n",
    "        categories[name] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(size, s='train'):\n",
    "    #get train data and shape\n",
    "    X=data[s]\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    #initialize 2 empty arrays for the input size in a list\n",
    "    pairs=[np.zeros((size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    #initialize vector for the targets\n",
    "    targets=np.zeros((size,1))\n",
    "    \n",
    "    for x in range(size):\n",
    "        #randomly sample one class (character)\n",
    "        category = rnd.choice(n_classes,1,replace=False)\n",
    "        #randomly sample one example from class (1-20 characters)\n",
    "        idx_1 = rnd.randint(0, n_examples)\n",
    "        pairs[0][x,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        #randomly sample again one example from class and add last class with modulo\n",
    "        # ..to ensure not same class pairs are created\n",
    "        idx_2 = (idx_1 + rnd.randint(0, n_examples)) % n_examples\n",
    "        #pick images of different class for 1st half and same class for 2nd half\n",
    "        if x >= size // 2:\n",
    "            category_2 = category\n",
    "            targets[x] = 1\n",
    "        else: \n",
    "        #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "        # ..different category\n",
    "            idx_2 = rnd.randint(0, n_examples) \n",
    "            category_2 = (category + rnd.randint(1,n_classes)) % n_classes\n",
    "            targets[x] = 0\n",
    "        pairs[1][x,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "        \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_labels = create_train_data(10000)\n",
    "val_set, val_labels = create_train_data(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-7, high=0.1, prior='log-uniform', name='learning_rate')\n",
    "dim_reg_layer1 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer1')\n",
    "dim_reg_layer2 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer2')\n",
    "dim_reg_layer3 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer3')\n",
    "dim_reg_layer5 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer5')\n",
    "dim_filt_layer1 = Integer(3,20, name='filter_layer1')\n",
    "dim_filt_layer2 = Integer(3,15, name='filter_layer2')\n",
    "dim_filt_layer3 = Integer(3,15, name='filter_layer3')\n",
    "dim_chan_layer1 = Categorical([16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256], name='channel_layer1')\n",
    "dim_chan_layer2 = Categorical([16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256], name='channel_layer2')\n",
    "dim_chan_layer3 = Categorical([16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256], name='channel_layer3')\n",
    "dim_fc_layer5 = Categorical([256,512,768,1024,1280,1536,1792,2048,2304,2560,2816,3072,3328,3584,3840,4096], name='channel_layer5')\n",
    "beta1 = Real(low=0.00001, high=0.9999, prior = 'uniform', name='beta1')\n",
    "beta2 = Real(low=0.00001, high=0.9999, prior = 'uniform', name='beta2')\n",
    "batch = Categorical([32,64], name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [dim_learning_rate,\n",
    "             dim_reg_layer1,\n",
    "             dim_reg_layer2,\n",
    "             dim_reg_layer3,\n",
    "             dim_reg_layer5,\n",
    "             dim_filt_layer1,\n",
    "             dim_filt_layer2,\n",
    "             dim_filt_layer3,\n",
    "             dim_chan_layer1,\n",
    "             dim_chan_layer2,\n",
    "             dim_chan_layer3,\n",
    "             dim_fc_layer5,\n",
    "             beta1,\n",
    "             beta2,\n",
    "             batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_parameters = [0.00006,2e-4,2e-4,2e-4,1e-3,10,7,4,64,128,128,2048,0.9,0.999,64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rnd.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rnd.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(**params):\n",
    "    input_shape = (105, 105, 1)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    #build convnet to use in each siamese 'leg'\n",
    "    convnet = Sequential()\n",
    "    convnet.add(Conv2D(params['channel_layer1'],(params['filter_layer1'],params['filter_layer1']),activation='relu',input_shape=input_shape,kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer1']),bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer2'],(params['filter_layer2'],params['filter_layer2']),activation='relu',kernel_regularizer=l2(params['reg_layer2']),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer3'],(params['filter_layer3'],params['filter_layer3']),strides=(2, 2),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer3']),bias_initializer=b_init))\n",
    "    convnet.add(Flatten())\n",
    "    convnet.add(Dense(params['channel_layer5'],activation=\"sigmoid\",kernel_regularizer=l2(params['reg_layer5']),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    #call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "    encoded_l = convnet(left_input)\n",
    "    encoded_r = convnet(right_input)\n",
    "    #layer to merge two encoded inputs with the l1 distance between them\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    #call this layer on list of two input tensors.\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    optimizer = Adam(lr=params['learning_rate'], beta_1=params['beta1'], beta_2=params['beta2'])\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "    #siamese_net.count_params()\n",
    "    return(siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, true_val):\n",
    "    acc_bool = np.equal(np.round_(pred), true_val)\n",
    "    acc = np.mean(acc_bool.astype(int))\n",
    "    \n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just as fast as the Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "###################################\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(**params):\n",
    "    #Training loop\n",
    "    siamese_net = create_network(**params)\n",
    "        \n",
    "    print(\"!\")\n",
    "    batch_size = params['batch']\n",
    "    total_batch = int(10000/batch_size)\n",
    "    total_batch_val = int(3000/batch_size)\n",
    "    epoch = 10\n",
    "\n",
    "    print(\"training\")\n",
    "    for i in range(epoch):\n",
    "        batch_x1, batch_x2, batch_y = shuffle(train_set[0],train_set[1], train_labels, n_samples = batch_size)\n",
    "        train_batch_acc = 0\n",
    "        for j in range(total_batch):\n",
    "            loss=siamese_net.train_on_batch([batch_x1, batch_x2],batch_y)\n",
    "            probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "            train_batch_acc += accuracy(probs, batch_y)\n",
    "            #print('Loss:', loss)\n",
    "            #print('Batch:', j)\n",
    "        train_acc = train_batch_acc/total_batch\n",
    "        val_batch_acc = 0\n",
    "        for validation in range(total_batch_val):\n",
    "            batch_x1, batch_x2, batch_y = shuffle(val_set[0],val_set[1], val_labels, n_samples = batch_size)\n",
    "            probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "            val_batch_acc += accuracy(probs, batch_y)\n",
    "        val_acc = val_batch_acc/total_batch_val\n",
    "        print('Epoch:', i)\n",
    "        print('Train accuracy:', train_acc)\n",
    "        print('Validation accuracy:', val_acc)\n",
    "            \n",
    "    return(-val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "Loss: 4.4984665\n",
      "Batch: 0\n",
      "Loss: 4.403412\n",
      "Batch: 1\n",
      "Loss: 4.330426\n",
      "Batch: 2\n",
      "Loss: 4.24168\n",
      "Batch: 3\n",
      "Loss: 4.1520605\n",
      "Batch: 4\n",
      "Loss: 4.066664\n",
      "Batch: 5\n",
      "Loss: 3.988257\n",
      "Batch: 6\n",
      "Loss: 3.9156978\n",
      "Batch: 7\n",
      "Loss: 3.8498886\n",
      "Batch: 8\n",
      "Loss: 3.791628\n",
      "Batch: 9\n",
      "Loss: 3.7408202\n",
      "Batch: 10\n",
      "Loss: 3.6968129\n",
      "Batch: 11\n",
      "Loss: 3.658761\n",
      "Batch: 12\n",
      "Loss: 3.6255662\n",
      "Batch: 13\n",
      "Loss: 3.5960796\n",
      "Batch: 14\n",
      "Loss: 3.5691466\n",
      "Batch: 15\n",
      "Loss: 3.5438907\n",
      "Batch: 16\n",
      "Loss: 3.5197856\n",
      "Batch: 17\n",
      "Loss: 3.496443\n",
      "Batch: 18\n",
      "Loss: 3.4735928\n",
      "Batch: 19\n",
      "Loss: 3.4510489\n",
      "Batch: 20\n",
      "Loss: 3.4286983\n",
      "Batch: 21\n",
      "Loss: 3.4064739\n",
      "Batch: 22\n",
      "Loss: 3.384332\n",
      "Batch: 23\n",
      "Loss: 3.3622463\n",
      "Batch: 24\n",
      "Loss: 3.3402033\n",
      "Batch: 25\n",
      "Loss: 3.318196\n",
      "Batch: 26\n",
      "Loss: 3.2962224\n",
      "Batch: 27\n",
      "Loss: 3.2742836\n",
      "Batch: 28\n",
      "Loss: 3.252379\n",
      "Batch: 29\n",
      "Loss: 3.2305143\n",
      "Batch: 30\n",
      "Loss: 3.2086942\n",
      "Batch: 31\n",
      "Loss: 3.186924\n",
      "Batch: 32\n",
      "Loss: 3.165208\n",
      "Batch: 33\n",
      "Loss: 3.1435518\n",
      "Batch: 34\n",
      "Loss: 3.121961\n",
      "Batch: 35\n",
      "Loss: 3.10044\n",
      "Batch: 36\n",
      "Loss: 3.078994\n",
      "Batch: 37\n",
      "Loss: 3.0576262\n",
      "Batch: 38\n",
      "Loss: 3.0363412\n",
      "Batch: 39\n",
      "Loss: 3.0151424\n",
      "Batch: 40\n",
      "Loss: 2.9940333\n",
      "Batch: 41\n",
      "Loss: 2.9730182\n",
      "Batch: 42\n",
      "Loss: 2.9520986\n",
      "Batch: 43\n",
      "Loss: 2.931277\n",
      "Batch: 44\n",
      "Loss: 2.9105566\n",
      "Batch: 45\n",
      "Loss: 2.8899386\n",
      "Batch: 46\n",
      "Loss: 2.8694258\n",
      "Batch: 47\n",
      "Loss: 2.8490198\n",
      "Batch: 48\n",
      "Loss: 2.828722\n",
      "Batch: 49\n",
      "Loss: 2.808533\n",
      "Batch: 50\n",
      "Loss: 2.7884555\n",
      "Batch: 51\n",
      "Loss: 2.7684894\n",
      "Batch: 52\n",
      "Loss: 2.7486362\n",
      "Batch: 53\n",
      "Loss: 2.7288964\n",
      "Batch: 54\n",
      "Loss: 2.709271\n",
      "Batch: 55\n",
      "Loss: 2.6897604\n",
      "Batch: 56\n",
      "Loss: 2.6703656\n",
      "Batch: 57\n",
      "Loss: 2.651086\n",
      "Batch: 58\n",
      "Loss: 2.631923\n",
      "Batch: 59\n",
      "Loss: 2.6128764\n",
      "Batch: 60\n",
      "Loss: 2.5939467\n",
      "Batch: 61\n",
      "Loss: 2.5751336\n",
      "Batch: 62\n",
      "Loss: 2.556437\n",
      "Batch: 63\n",
      "Loss: 2.537858\n",
      "Batch: 64\n",
      "Loss: 2.519395\n",
      "Batch: 65\n",
      "Loss: 2.5010495\n",
      "Batch: 66\n",
      "Loss: 2.4828203\n",
      "Batch: 67\n",
      "Loss: 2.464708\n",
      "Batch: 68\n",
      "Loss: 2.4467118\n",
      "Batch: 69\n",
      "Loss: 2.428831\n",
      "Batch: 70\n",
      "Loss: 2.4110668\n",
      "Batch: 71\n",
      "Loss: 2.3934178\n",
      "Batch: 72\n",
      "Loss: 2.375884\n",
      "Batch: 73\n",
      "Loss: 2.3584652\n",
      "Batch: 74\n",
      "Loss: 2.341161\n",
      "Batch: 75\n",
      "Loss: 2.3239706\n",
      "Batch: 76\n",
      "Loss: 2.3068943\n",
      "Batch: 77\n",
      "Train accuracy: 0.9938902243589743\n",
      "Validation accuracy: 0.6769701086956522\n",
      "Loss: 3.0459366\n",
      "Batch: 0\n",
      "Loss: 2.739714\n",
      "Batch: 1\n",
      "Loss: 2.5221207\n",
      "Batch: 2\n",
      "Loss: 2.4402697\n",
      "Batch: 3\n",
      "Loss: 2.9450927\n",
      "Batch: 4\n",
      "Loss: 2.3504941\n",
      "Batch: 5\n",
      "Loss: 2.3620136\n",
      "Batch: 6\n",
      "Loss: 2.3611927\n",
      "Batch: 7\n",
      "Loss: 2.3569481\n",
      "Batch: 8\n",
      "Loss: 2.321332\n",
      "Batch: 9\n",
      "Loss: 2.2981665\n",
      "Batch: 10\n",
      "Loss: 2.2569647\n",
      "Batch: 11\n",
      "Loss: 2.2380733\n",
      "Batch: 12\n",
      "Loss: 2.2247086\n",
      "Batch: 13\n",
      "Loss: 2.2093043\n",
      "Batch: 14\n",
      "Loss: 2.1959233\n",
      "Batch: 15\n",
      "Loss: 2.1854534\n",
      "Batch: 16\n",
      "Loss: 2.1763175\n",
      "Batch: 17\n",
      "Loss: 2.1672475\n",
      "Batch: 18\n",
      "Loss: 2.1579008\n",
      "Batch: 19\n",
      "Loss: 2.1485438\n",
      "Batch: 20\n",
      "Loss: 2.1394808\n",
      "Batch: 21\n",
      "Loss: 2.1308556\n",
      "Batch: 22\n",
      "Loss: 2.1226432\n",
      "Batch: 23\n",
      "Loss: 2.114736\n",
      "Batch: 24\n",
      "Loss: 2.107013\n",
      "Batch: 25\n",
      "Loss: 2.0993977\n",
      "Batch: 26\n",
      "Loss: 2.0918615\n",
      "Batch: 27\n",
      "Loss: 2.084403\n",
      "Batch: 28\n",
      "Loss: 2.0770338\n",
      "Batch: 29\n",
      "Loss: 2.0697644\n",
      "Batch: 30\n",
      "Loss: 2.0625944\n",
      "Batch: 31\n",
      "Loss: 2.0555203\n",
      "Batch: 32\n",
      "Loss: 2.048532\n",
      "Batch: 33\n",
      "Loss: 2.0416203\n",
      "Batch: 34\n",
      "Loss: 2.0347724\n",
      "Batch: 35\n",
      "Loss: 2.027978\n",
      "Batch: 36\n",
      "Loss: 2.0212264\n",
      "Batch: 37\n",
      "Loss: 2.0145106\n",
      "Batch: 38\n",
      "Loss: 2.0078266\n",
      "Batch: 39\n",
      "Loss: 2.0011725\n",
      "Batch: 40\n",
      "Loss: 1.9945476\n",
      "Batch: 41\n",
      "Loss: 1.9879524\n",
      "Batch: 42\n",
      "Loss: 1.9813871\n",
      "Batch: 43\n",
      "Loss: 1.974852\n",
      "Batch: 44\n",
      "Loss: 1.9683465\n",
      "Batch: 45\n",
      "Loss: 1.9618701\n",
      "Batch: 46\n",
      "Loss: 1.955422\n",
      "Batch: 47\n",
      "Loss: 1.9490012\n",
      "Batch: 48\n",
      "Loss: 1.9426072\n",
      "Batch: 49\n",
      "Loss: 1.9362389\n",
      "Batch: 50\n",
      "Loss: 1.9298962\n",
      "Batch: 51\n",
      "Loss: 1.9235791\n",
      "Batch: 52\n",
      "Loss: 1.9172871\n",
      "Batch: 53\n",
      "Loss: 1.9110205\n",
      "Batch: 54\n",
      "Loss: 1.9047788\n",
      "Batch: 55\n",
      "Loss: 1.8985623\n",
      "Batch: 56\n",
      "Loss: 1.8923708\n",
      "Batch: 57\n",
      "Loss: 1.886204\n",
      "Batch: 58\n",
      "Loss: 1.8800616\n",
      "Batch: 59\n",
      "Loss: 1.8739433\n",
      "Batch: 60\n",
      "Loss: 1.8678491\n",
      "Batch: 61\n",
      "Loss: 1.8617784\n",
      "Batch: 62\n",
      "Loss: 1.8557315\n",
      "Batch: 63\n",
      "Loss: 1.8497082\n",
      "Batch: 64\n",
      "Loss: 1.8437086\n",
      "Batch: 65\n",
      "Loss: 1.8377331\n",
      "Batch: 66\n",
      "Loss: 1.8317809\n",
      "Batch: 67\n",
      "Loss: 1.8258522\n",
      "Batch: 68\n",
      "Loss: 1.8199469\n",
      "Batch: 69\n",
      "Loss: 1.8140651\n",
      "Batch: 70\n",
      "Loss: 1.8082062\n",
      "Batch: 71\n",
      "Loss: 1.8023704\n",
      "Batch: 72\n",
      "Loss: 1.7965577\n",
      "Batch: 73\n",
      "Loss: 1.7907677\n",
      "Batch: 74\n",
      "Loss: 1.7850004\n",
      "Batch: 75\n",
      "Loss: 1.779256\n",
      "Batch: 76\n",
      "Loss: 1.7735342\n",
      "Batch: 77\n",
      "Train accuracy: 0.9886818910256411\n",
      "Validation accuracy: 0.6885190217391305\n",
      "Loss: 2.4116068\n",
      "Batch: 0\n",
      "Loss: 2.0978696\n",
      "Batch: 1\n",
      "Loss: 1.9236507\n",
      "Batch: 2\n",
      "Loss: 1.8640379\n",
      "Batch: 3\n",
      "Loss: 1.8124716\n",
      "Batch: 4\n",
      "Loss: 1.8012815\n",
      "Batch: 5\n",
      "Loss: 1.782497\n",
      "Batch: 6\n",
      "Loss: 1.7683357\n",
      "Batch: 7\n",
      "Loss: 1.7590804\n",
      "Batch: 8\n",
      "Loss: 1.7491661\n",
      "Batch: 9\n",
      "Loss: 1.73991\n",
      "Batch: 10\n",
      "Loss: 1.7325654\n",
      "Batch: 11\n",
      "Loss: 1.7264568\n",
      "Batch: 12\n",
      "Loss: 1.7206893\n",
      "Batch: 13\n",
      "Loss: 1.7149061\n",
      "Batch: 14\n",
      "Loss: 1.7092019\n",
      "Batch: 15\n",
      "Loss: 1.7037178\n",
      "Batch: 16\n",
      "Loss: 1.6984788\n",
      "Batch: 17\n",
      "Loss: 1.693422\n",
      "Batch: 18\n",
      "Loss: 1.68846\n",
      "Batch: 19\n",
      "Loss: 1.6835389\n",
      "Batch: 20\n",
      "Loss: 1.6786492\n",
      "Batch: 21\n",
      "Loss: 1.6738027\n",
      "Batch: 22\n",
      "Loss: 1.6690087\n",
      "Batch: 23\n",
      "Loss: 1.6642683\n",
      "Batch: 24\n",
      "Loss: 1.6595743\n",
      "Batch: 25\n",
      "Loss: 1.6549157\n",
      "Batch: 26\n",
      "Loss: 1.6502811\n",
      "Batch: 27\n",
      "Loss: 1.6456624\n",
      "Batch: 28\n",
      "Loss: 1.641055\n",
      "Batch: 29\n",
      "Loss: 1.6364573\n",
      "Batch: 30\n",
      "Loss: 1.6318709\n",
      "Batch: 31\n",
      "Loss: 1.6272967\n",
      "Batch: 32\n",
      "Loss: 1.6227374\n",
      "Batch: 33\n",
      "Loss: 1.618194\n",
      "Batch: 34\n",
      "Loss: 1.6136677\n",
      "Batch: 35\n",
      "Loss: 1.6091578\n",
      "Batch: 36\n",
      "Loss: 1.6046641\n",
      "Batch: 37\n",
      "Loss: 1.6001855\n",
      "Batch: 38\n",
      "Loss: 1.5957208\n",
      "Batch: 39\n",
      "Loss: 1.5912693\n",
      "Batch: 40\n",
      "Loss: 1.5868299\n",
      "Batch: 41\n",
      "Loss: 1.5824023\n",
      "Batch: 42\n",
      "Loss: 1.5779872\n",
      "Batch: 43\n",
      "Loss: 1.5735847\n",
      "Batch: 44\n",
      "Loss: 1.5691955\n",
      "Batch: 45\n",
      "Loss: 1.5648202\n",
      "Batch: 46\n",
      "Loss: 1.5604581\n",
      "Batch: 47\n",
      "Loss: 1.5561103\n",
      "Batch: 48\n",
      "Loss: 1.551776\n",
      "Batch: 49\n",
      "Loss: 1.5474557\n",
      "Batch: 50\n",
      "Loss: 1.5431489\n",
      "Batch: 51\n",
      "Loss: 1.5388556\n",
      "Batch: 52\n",
      "Loss: 1.5345757\n",
      "Batch: 53\n",
      "Loss: 1.5303088\n",
      "Batch: 54\n",
      "Loss: 1.5260559\n",
      "Batch: 55\n",
      "Loss: 1.521816\n",
      "Batch: 56\n",
      "Loss: 1.5175892\n",
      "Batch: 57\n",
      "Loss: 1.5133762\n",
      "Batch: 58\n",
      "Loss: 1.5091763\n",
      "Batch: 59\n",
      "Loss: 1.50499\n",
      "Batch: 60\n",
      "Loss: 1.500817\n",
      "Batch: 61\n",
      "Loss: 1.4966577\n",
      "Batch: 62\n",
      "Loss: 1.4925116\n",
      "Batch: 63\n",
      "Loss: 1.4883791\n",
      "Batch: 64\n",
      "Loss: 1.4842596\n",
      "Batch: 65\n",
      "Loss: 1.4801533\n",
      "Batch: 66\n",
      "Loss: 1.4760605\n",
      "Batch: 67\n",
      "Loss: 1.4719807\n",
      "Batch: 68\n",
      "Loss: 1.4679137\n",
      "Batch: 69\n",
      "Loss: 1.46386\n",
      "Batch: 70\n",
      "Loss: 1.4598191\n",
      "Batch: 71\n",
      "Loss: 1.4557912\n",
      "Batch: 72\n",
      "Loss: 1.4517763\n",
      "Batch: 73\n",
      "Loss: 1.4477742\n",
      "Batch: 74\n",
      "Loss: 1.4437846\n",
      "Batch: 75\n",
      "Loss: 1.4398079\n",
      "Batch: 76\n",
      "Loss: 1.4358443\n",
      "Batch: 77\n",
      "Train accuracy: 0.9982972756410257\n",
      "Validation accuracy: 0.6698369565217391\n",
      "Loss: 2.4583118\n",
      "Batch: 0\n",
      "Loss: 1.9351004\n",
      "Batch: 1\n",
      "Loss: 1.6917096\n",
      "Batch: 2\n",
      "Loss: 1.5674249\n",
      "Batch: 3\n",
      "Loss: 1.5247016\n",
      "Batch: 4\n",
      "Loss: 1.4929019\n",
      "Batch: 5\n",
      "Loss: 1.4727852\n",
      "Batch: 6\n",
      "Loss: 1.4599686\n",
      "Batch: 7\n",
      "Loss: 1.4519944\n",
      "Batch: 8\n",
      "Loss: 1.444861\n",
      "Batch: 9\n",
      "Loss: 1.4370844\n",
      "Batch: 10\n",
      "Loss: 1.4295865\n",
      "Batch: 11\n",
      "Loss: 1.4233198\n",
      "Batch: 12\n",
      "Loss: 1.41837\n",
      "Batch: 13\n",
      "Loss: 1.414305\n",
      "Batch: 14\n",
      "Loss: 1.4105344\n",
      "Batch: 15\n",
      "Loss: 1.4066947\n",
      "Batch: 16\n",
      "Loss: 1.4028361\n",
      "Batch: 17\n",
      "Loss: 1.3991036\n",
      "Batch: 18\n",
      "Loss: 1.3955147\n",
      "Batch: 19\n",
      "Loss: 1.3920304\n",
      "Batch: 20\n",
      "Loss: 1.3886316\n",
      "Batch: 21\n",
      "Loss: 1.3853152\n",
      "Batch: 22\n",
      "Loss: 1.382062\n",
      "Batch: 23\n",
      "Loss: 1.3788428\n",
      "Batch: 24\n",
      "Loss: 1.375637\n",
      "Batch: 25\n",
      "Loss: 1.3724406\n",
      "Batch: 26\n",
      "Loss: 1.3692571\n",
      "Batch: 27\n",
      "Loss: 1.36609\n",
      "Batch: 28\n",
      "Loss: 1.3629397\n",
      "Batch: 29\n",
      "Loss: 1.3598043\n",
      "Batch: 30\n",
      "Loss: 1.3566797\n",
      "Batch: 31\n",
      "Loss: 1.3535639\n",
      "Batch: 32\n",
      "Loss: 1.3504541\n",
      "Batch: 33\n",
      "Loss: 1.3473488\n",
      "Batch: 34\n",
      "Loss: 1.3442473\n",
      "Batch: 35\n",
      "Loss: 1.341149\n",
      "Batch: 36\n",
      "Loss: 1.3380538\n",
      "Batch: 37\n",
      "Loss: 1.3349622\n",
      "Batch: 38\n",
      "Loss: 1.3318743\n",
      "Batch: 39\n",
      "Loss: 1.3287907\n",
      "Batch: 40\n",
      "Loss: 1.3257115\n",
      "Batch: 41\n",
      "Loss: 1.3226379\n",
      "Batch: 42\n",
      "Loss: 1.3195701\n",
      "Batch: 43\n",
      "Loss: 1.316508\n",
      "Batch: 44\n",
      "Loss: 1.3134528\n",
      "Batch: 45\n",
      "Loss: 1.3104043\n",
      "Batch: 46\n",
      "Loss: 1.3073628\n",
      "Batch: 47\n",
      "Loss: 1.3043282\n",
      "Batch: 48\n",
      "Loss: 1.3013008\n",
      "Batch: 49\n",
      "Loss: 1.2982804\n",
      "Batch: 50\n",
      "Loss: 1.2952669\n",
      "Batch: 51\n",
      "Loss: 1.2922606\n",
      "Batch: 52\n",
      "Loss: 1.2892611\n",
      "Batch: 53\n",
      "Loss: 1.2862688\n",
      "Batch: 54\n",
      "Loss: 1.2832835\n",
      "Batch: 55\n",
      "Loss: 1.2803055\n",
      "Batch: 56\n",
      "Loss: 1.2773348\n",
      "Batch: 57\n",
      "Loss: 1.2743714\n",
      "Batch: 58\n",
      "Loss: 1.2714154\n",
      "Batch: 59\n",
      "Loss: 1.2684668\n",
      "Batch: 60\n",
      "Loss: 1.2655255\n",
      "Batch: 61\n",
      "Loss: 1.262592\n",
      "Batch: 62\n",
      "Loss: 1.2596656\n",
      "Batch: 63\n",
      "Loss: 1.2567468\n",
      "Batch: 64\n",
      "Loss: 1.2538352\n",
      "Batch: 65\n",
      "Loss: 1.2509313\n",
      "Batch: 66\n",
      "Loss: 1.2480346\n",
      "Batch: 67\n",
      "Loss: 1.2451456\n",
      "Batch: 68\n",
      "Loss: 1.2422636\n",
      "Batch: 69\n",
      "Loss: 1.2393893\n",
      "Batch: 70\n",
      "Loss: 1.2365218\n",
      "Batch: 71\n",
      "Loss: 1.2336622\n",
      "Batch: 72\n",
      "Loss: 1.2308095\n",
      "Batch: 73\n",
      "Loss: 1.2279643\n",
      "Batch: 74\n",
      "Loss: 1.2251261\n",
      "Batch: 75\n",
      "Loss: 1.2222955\n",
      "Batch: 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2194717\n",
      "Batch: 77\n",
      "Train accuracy: 0.9955929487179487\n",
      "Validation accuracy: 0.681046195652174\n",
      "Loss: 2.047232\n",
      "Batch: 0\n",
      "Loss: 1.6528767\n",
      "Batch: 1\n",
      "Loss: 1.3625965\n",
      "Batch: 2\n",
      "Loss: 1.2733976\n",
      "Batch: 3\n",
      "Loss: 1.2483989\n",
      "Batch: 4\n",
      "Loss: 1.2347552\n",
      "Batch: 5\n",
      "Loss: 1.218029\n",
      "Batch: 6\n",
      "Loss: 1.2065902\n",
      "Batch: 7\n",
      "Loss: 1.198956\n",
      "Batch: 8\n",
      "Loss: 1.192144\n",
      "Batch: 9\n",
      "Loss: 1.1861367\n",
      "Batch: 10\n",
      "Loss: 1.1815007\n",
      "Batch: 11\n",
      "Loss: 1.1777263\n",
      "Batch: 12\n",
      "Loss: 1.1742325\n",
      "Batch: 13\n",
      "Loss: 1.1709801\n",
      "Batch: 14\n",
      "Loss: 1.1680155\n",
      "Batch: 15\n",
      "Loss: 1.1651844\n",
      "Batch: 16\n",
      "Loss: 1.1623319\n",
      "Batch: 17\n",
      "Loss: 1.1594843\n",
      "Batch: 18\n",
      "Loss: 1.1567261\n",
      "Batch: 19\n",
      "Loss: 1.154084\n",
      "Batch: 20\n",
      "Loss: 1.1515338\n",
      "Batch: 21\n",
      "Loss: 1.1490407\n",
      "Batch: 22\n",
      "Loss: 1.146579\n",
      "Batch: 23\n",
      "Loss: 1.144134\n",
      "Batch: 24\n",
      "Loss: 1.1416973\n",
      "Batch: 25\n",
      "Loss: 1.1392659\n",
      "Batch: 26\n",
      "Loss: 1.136837\n",
      "Batch: 27\n",
      "Loss: 1.1344097\n",
      "Batch: 28\n",
      "Loss: 1.1319836\n",
      "Batch: 29\n",
      "Loss: 1.1295587\n",
      "Batch: 30\n",
      "Loss: 1.1271353\n",
      "Batch: 31\n",
      "Loss: 1.1247138\n",
      "Batch: 32\n",
      "Loss: 1.1222942\n",
      "Batch: 33\n",
      "Loss: 1.1198772\n",
      "Batch: 34\n",
      "Loss: 1.1174628\n",
      "Batch: 35\n",
      "Loss: 1.1150512\n",
      "Batch: 36\n",
      "Loss: 1.1126425\n",
      "Batch: 37\n",
      "Loss: 1.1102363\n",
      "Batch: 38\n",
      "Loss: 1.1078328\n",
      "Batch: 39\n",
      "Loss: 1.1054322\n",
      "Batch: 40\n",
      "Loss: 1.1030343\n",
      "Batch: 41\n",
      "Loss: 1.1006393\n",
      "Batch: 42\n",
      "Loss: 1.098248\n",
      "Batch: 43\n",
      "Loss: 1.0958604\n",
      "Batch: 44\n",
      "Loss: 1.093477\n",
      "Batch: 45\n",
      "Loss: 1.0910981\n",
      "Batch: 46\n",
      "Loss: 1.0887237\n",
      "Batch: 47\n",
      "Loss: 1.086354\n",
      "Batch: 48\n",
      "Loss: 1.083989\n",
      "Batch: 49\n",
      "Loss: 1.0816288\n",
      "Batch: 50\n",
      "Loss: 1.079273\n",
      "Batch: 51\n",
      "Loss: 1.076922\n",
      "Batch: 52\n",
      "Loss: 1.0745755\n",
      "Batch: 53\n",
      "Loss: 1.0722342\n",
      "Batch: 54\n",
      "Loss: 1.069897\n",
      "Batch: 55\n",
      "Loss: 1.067565\n",
      "Batch: 56\n",
      "Loss: 1.0652382\n",
      "Batch: 57\n",
      "Loss: 1.0629164\n",
      "Batch: 58\n",
      "Loss: 1.0605997\n",
      "Batch: 59\n",
      "Loss: 1.0582882\n",
      "Batch: 60\n",
      "Loss: 1.0559818\n",
      "Batch: 61\n",
      "Loss: 1.0536807\n",
      "Batch: 62\n",
      "Loss: 1.0513848\n",
      "Batch: 63\n",
      "Loss: 1.0490936\n",
      "Batch: 64\n",
      "Loss: 1.0468079\n",
      "Batch: 65\n",
      "Loss: 1.0445275\n",
      "Batch: 66\n",
      "Loss: 1.0422521\n",
      "Batch: 67\n",
      "Loss: 1.0399818\n",
      "Batch: 68\n",
      "Loss: 1.0377167\n",
      "Batch: 69\n",
      "Loss: 1.0354568\n",
      "Batch: 70\n",
      "Loss: 1.0332019\n",
      "Batch: 71\n",
      "Loss: 1.0309523\n",
      "Batch: 72\n",
      "Loss: 1.0287076\n",
      "Batch: 73\n",
      "Loss: 1.0264683\n",
      "Batch: 74\n",
      "Loss: 1.0242338\n",
      "Batch: 75\n",
      "Loss: 1.0220046\n",
      "Batch: 76\n",
      "Loss: 1.0197803\n",
      "Batch: 77\n",
      "Train accuracy: 0.996895032051282\n",
      "Validation accuracy: 0.7027853260869565\n",
      "Loss: 1.6389763\n",
      "Batch: 0\n",
      "Loss: 1.3091729\n",
      "Batch: 1\n",
      "Loss: 1.1467144\n",
      "Batch: 2\n",
      "Loss: 1.0931153\n",
      "Batch: 3\n",
      "Loss: 1.0689673\n",
      "Batch: 4\n",
      "Loss: 1.0627166\n",
      "Batch: 5\n",
      "Loss: 1.04995\n",
      "Batch: 6\n",
      "Loss: 1.0389872\n",
      "Batch: 7\n",
      "Loss: 1.0329069\n",
      "Batch: 8\n",
      "Loss: 1.0289795\n",
      "Batch: 9\n",
      "Loss: 1.0255647\n",
      "Batch: 10\n",
      "Loss: 1.0222299\n",
      "Batch: 11\n",
      "Loss: 1.0190207\n",
      "Batch: 12\n",
      "Loss: 1.0160377\n",
      "Batch: 13\n",
      "Loss: 1.0133082\n",
      "Batch: 14\n",
      "Loss: 1.010803\n",
      "Batch: 15\n",
      "Loss: 1.0084656\n",
      "Batch: 16\n",
      "Loss: 1.0062324\n",
      "Batch: 17\n",
      "Loss: 1.004049\n",
      "Batch: 18\n",
      "Loss: 1.0018888\n",
      "Batch: 19\n",
      "Loss: 0.99975806\n",
      "Batch: 20\n",
      "Loss: 0.99768215\n",
      "Batch: 21\n",
      "Loss: 0.9956751\n",
      "Batch: 22\n",
      "Loss: 0.9937209\n",
      "Batch: 23\n",
      "Loss: 0.9917778\n",
      "Batch: 24\n",
      "Loss: 0.989817\n",
      "Batch: 25\n",
      "Loss: 0.98784417\n",
      "Batch: 26\n",
      "Loss: 0.9858779\n",
      "Batch: 27\n",
      "Loss: 0.9839246\n",
      "Batch: 28\n",
      "Loss: 0.9819781\n",
      "Batch: 29\n",
      "Loss: 0.98003113\n",
      "Batch: 30\n",
      "Loss: 0.9780824\n",
      "Batch: 31\n",
      "Loss: 0.9761351\n",
      "Batch: 32\n",
      "Loss: 0.974192\n",
      "Batch: 33\n",
      "Loss: 0.97225404\n",
      "Batch: 34\n",
      "Loss: 0.97032\n",
      "Batch: 35\n",
      "Loss: 0.96838796\n",
      "Batch: 36\n",
      "Loss: 0.96645665\n",
      "Batch: 37\n",
      "Loss: 0.96452475\n",
      "Batch: 38\n",
      "Loss: 0.9625922\n",
      "Batch: 39\n",
      "Loss: 0.960659\n",
      "Batch: 40\n",
      "Loss: 0.9587263\n",
      "Batch: 41\n",
      "Loss: 0.9567944\n",
      "Batch: 42\n",
      "Loss: 0.9548641\n",
      "Batch: 43\n",
      "Loss: 0.9529358\n",
      "Batch: 44\n",
      "Loss: 0.9510101\n",
      "Batch: 45\n",
      "Loss: 0.9490872\n",
      "Batch: 46\n",
      "Loss: 0.9471673\n",
      "Batch: 47\n",
      "Loss: 0.94525045\n",
      "Batch: 48\n",
      "Loss: 0.94333667\n",
      "Batch: 49\n",
      "Loss: 0.94142616\n",
      "Batch: 50\n",
      "Loss: 0.939519\n",
      "Batch: 51\n",
      "Loss: 0.93761516\n",
      "Batch: 52\n",
      "Loss: 0.935715\n",
      "Batch: 53\n",
      "Loss: 0.93381834\n",
      "Batch: 54\n",
      "Loss: 0.93192554\n",
      "Batch: 55\n",
      "Loss: 0.9300364\n",
      "Batch: 56\n",
      "Loss: 0.928151\n",
      "Batch: 57\n",
      "Loss: 0.92626953\n",
      "Batch: 58\n",
      "Loss: 0.9243915\n",
      "Batch: 59\n",
      "Loss: 0.9225175\n",
      "Batch: 60\n",
      "Loss: 0.9206473\n",
      "Batch: 61\n",
      "Loss: 0.918781\n",
      "Batch: 62\n",
      "Loss: 0.9169186\n",
      "Batch: 63\n",
      "Loss: 0.91506016\n",
      "Batch: 64\n",
      "Loss: 0.9132057\n",
      "Batch: 65\n",
      "Loss: 0.911355\n",
      "Batch: 66\n",
      "Loss: 0.90950793\n",
      "Batch: 67\n",
      "Loss: 0.9076646\n",
      "Batch: 68\n",
      "Loss: 0.90582526\n",
      "Batch: 69\n",
      "Loss: 0.90398973\n",
      "Batch: 70\n",
      "Loss: 0.90215814\n",
      "Batch: 71\n",
      "Loss: 0.90033036\n",
      "Batch: 72\n",
      "Loss: 0.89850634\n",
      "Batch: 73\n",
      "Loss: 0.89668614\n",
      "Batch: 74\n",
      "Loss: 0.89486974\n",
      "Batch: 75\n",
      "Loss: 0.893057\n",
      "Batch: 76\n",
      "Loss: 0.89124817\n",
      "Batch: 77\n",
      "Train accuracy: 0.9984975961538461\n",
      "Validation accuracy: 0.6793478260869565\n",
      "Loss: 1.4110727\n",
      "Batch: 0\n",
      "Loss: 1.0909864\n",
      "Batch: 1\n",
      "Loss: 0.9459318\n",
      "Batch: 2\n",
      "Loss: 0.9242387\n",
      "Batch: 3\n",
      "Loss: 0.9006247\n",
      "Batch: 4\n",
      "Loss: 0.890997\n",
      "Batch: 5\n",
      "Loss: 0.88526475\n",
      "Batch: 6\n",
      "Loss: 0.8785959\n",
      "Batch: 7\n",
      "Loss: 0.8727845\n",
      "Batch: 8\n",
      "Loss: 0.86855227\n",
      "Batch: 9\n",
      "Loss: 0.865387\n",
      "Batch: 10\n",
      "Loss: 0.8627663\n",
      "Batch: 11\n",
      "Loss: 0.86040413\n",
      "Batch: 12\n",
      "Loss: 0.8581778\n",
      "Batch: 13\n",
      "Loss: 0.85605776\n",
      "Batch: 14\n",
      "Loss: 0.85403776\n",
      "Batch: 15\n",
      "Loss: 0.8521053\n",
      "Batch: 16\n",
      "Loss: 0.85024947\n",
      "Batch: 17\n",
      "Loss: 0.8484558\n",
      "Batch: 18\n",
      "Loss: 0.84670806\n",
      "Batch: 19\n",
      "Loss: 0.8449929\n",
      "Batch: 20\n",
      "Loss: 0.8433\n",
      "Batch: 21\n",
      "Loss: 0.84162116\n",
      "Batch: 22\n",
      "Loss: 0.8399519\n",
      "Batch: 23\n",
      "Loss: 0.83828837\n",
      "Batch: 24\n",
      "Loss: 0.8366289\n",
      "Batch: 25\n",
      "Loss: 0.8349723\n",
      "Batch: 26\n",
      "Loss: 0.83331776\n",
      "Batch: 27\n",
      "Loss: 0.8316651\n",
      "Batch: 28\n",
      "Loss: 0.8300141\n",
      "Batch: 29\n",
      "Loss: 0.8283647\n",
      "Batch: 30\n",
      "Loss: 0.826717\n",
      "Batch: 31\n",
      "Loss: 0.82507116\n",
      "Batch: 32\n",
      "Loss: 0.8234272\n",
      "Batch: 33\n",
      "Loss: 0.82178503\n",
      "Batch: 34\n",
      "Loss: 0.8201449\n",
      "Batch: 35\n",
      "Loss: 0.8185068\n",
      "Batch: 36\n",
      "Loss: 0.816871\n",
      "Batch: 37\n",
      "Loss: 0.8152374\n",
      "Batch: 38\n",
      "Loss: 0.81360614\n",
      "Batch: 39\n",
      "Loss: 0.81197727\n",
      "Batch: 40\n",
      "Loss: 0.81035066\n",
      "Batch: 41\n",
      "Loss: 0.8087264\n",
      "Batch: 42\n",
      "Loss: 0.80710477\n",
      "Batch: 43\n",
      "Loss: 0.8054857\n",
      "Batch: 44\n",
      "Loss: 0.8038692\n",
      "Batch: 45\n",
      "Loss: 0.8022554\n",
      "Batch: 46\n",
      "Loss: 0.80064434\n",
      "Batch: 47\n",
      "Loss: 0.79903597\n",
      "Batch: 48\n",
      "Loss: 0.79743046\n",
      "Batch: 49\n",
      "Loss: 0.7958277\n",
      "Batch: 50\n",
      "Loss: 0.79422784\n",
      "Batch: 51\n",
      "Loss: 0.7926311\n",
      "Batch: 52\n",
      "Loss: 0.79103696\n",
      "Batch: 53\n",
      "Loss: 0.789446\n",
      "Batch: 54\n",
      "Loss: 0.7878579\n",
      "Batch: 55\n",
      "Loss: 0.786273\n",
      "Batch: 56\n",
      "Loss: 0.784691\n",
      "Batch: 57\n",
      "Loss: 0.783112\n",
      "Batch: 58\n",
      "Loss: 0.7815361\n",
      "Batch: 59\n",
      "Loss: 0.7799631\n",
      "Batch: 60\n",
      "Loss: 0.7783933\n",
      "Batch: 61\n",
      "Loss: 0.7768266\n",
      "Batch: 62\n",
      "Loss: 0.77526295\n",
      "Batch: 63\n",
      "Loss: 0.7737023\n",
      "Batch: 64\n",
      "Loss: 0.77214473\n",
      "Batch: 65\n",
      "Loss: 0.7705903\n",
      "Batch: 66\n",
      "Loss: 0.7690388\n",
      "Batch: 67\n",
      "Loss: 0.7674905\n",
      "Batch: 68\n",
      "Loss: 0.7659451\n",
      "Batch: 69\n",
      "Loss: 0.7644027\n",
      "Batch: 70\n",
      "Loss: 0.7628635\n",
      "Batch: 71\n",
      "Loss: 0.76132727\n",
      "Batch: 72\n",
      "Loss: 0.7597941\n",
      "Batch: 73\n",
      "Loss: 0.7582639\n",
      "Batch: 74\n",
      "Loss: 0.7567368\n",
      "Batch: 75\n",
      "Loss: 0.75521266\n",
      "Batch: 76\n",
      "Loss: 0.7536915\n",
      "Batch: 77\n",
      "Train accuracy: 0.999198717948718\n",
      "Validation accuracy: 0.6820652173913043\n",
      "Loss: 1.3868016\n",
      "Batch: 0\n",
      "Loss: 1.0358479\n",
      "Batch: 1\n",
      "Loss: 0.8219839\n",
      "Batch: 2\n",
      "Loss: 0.8144639\n",
      "Batch: 3\n",
      "Loss: 0.7964127\n",
      "Batch: 4\n",
      "Loss: 0.78090435\n",
      "Batch: 5\n",
      "Loss: 0.7715162\n",
      "Batch: 6\n",
      "Loss: 0.76583576\n",
      "Batch: 7\n",
      "Loss: 0.76113707\n",
      "Batch: 8\n",
      "Loss: 0.7564961\n",
      "Batch: 9\n",
      "Loss: 0.7524457\n",
      "Batch: 10\n",
      "Loss: 0.749241\n",
      "Batch: 11\n",
      "Loss: 0.746696\n",
      "Batch: 12\n",
      "Loss: 0.74451256\n",
      "Batch: 13\n",
      "Loss: 0.74247783\n",
      "Batch: 14\n",
      "Loss: 0.7405241\n",
      "Batch: 15\n",
      "Loss: 0.73869\n",
      "Batch: 16\n",
      "Loss: 0.73703057\n",
      "Batch: 17\n",
      "Loss: 0.7355357\n",
      "Batch: 18\n",
      "Loss: 0.7341207\n",
      "Batch: 19\n",
      "Loss: 0.73270786\n",
      "Batch: 20\n",
      "Loss: 0.73128664\n",
      "Batch: 21\n",
      "Loss: 0.7298837\n",
      "Batch: 22\n",
      "Loss: 0.72850144\n",
      "Batch: 23\n",
      "Loss: 0.72712314\n",
      "Batch: 24\n",
      "Loss: 0.72574264\n",
      "Batch: 25\n",
      "Loss: 0.72436726\n",
      "Batch: 26\n",
      "Loss: 0.7230025\n",
      "Batch: 27\n",
      "Loss: 0.7216453\n",
      "Batch: 28\n",
      "Loss: 0.72029066\n",
      "Batch: 29\n",
      "Loss: 0.71893543\n",
      "Batch: 30\n",
      "Loss: 0.71757877\n",
      "Batch: 31\n",
      "Loss: 0.71622086\n",
      "Batch: 32\n",
      "Loss: 0.714862\n",
      "Batch: 33\n",
      "Loss: 0.7135021\n",
      "Batch: 34\n",
      "Loss: 0.7121406\n",
      "Batch: 35\n",
      "Loss: 0.71077716\n",
      "Batch: 36\n",
      "Loss: 0.7094109\n",
      "Batch: 37\n",
      "Loss: 0.70804167\n",
      "Batch: 38\n",
      "Loss: 0.70666975\n",
      "Batch: 39\n",
      "Loss: 0.70529586\n",
      "Batch: 40\n",
      "Loss: 0.7039206\n",
      "Batch: 41\n",
      "Loss: 0.70254457\n",
      "Batch: 42\n",
      "Loss: 0.7011685\n",
      "Batch: 43\n",
      "Loss: 0.6997928\n",
      "Batch: 44\n",
      "Loss: 0.698418\n",
      "Batch: 45\n",
      "Loss: 0.6970443\n",
      "Batch: 46\n",
      "Loss: 0.6956722\n",
      "Batch: 47\n",
      "Loss: 0.6943015\n",
      "Batch: 48\n",
      "Loss: 0.69293237\n",
      "Batch: 49\n",
      "Loss: 0.69156504\n",
      "Batch: 50\n",
      "Loss: 0.69019955\n",
      "Batch: 51\n",
      "Loss: 0.6888358\n",
      "Batch: 52\n",
      "Loss: 0.68747413\n",
      "Batch: 53\n",
      "Loss: 0.6861147\n",
      "Batch: 54\n",
      "Loss: 0.68475765\n",
      "Batch: 55\n",
      "Loss: 0.68340284\n",
      "Batch: 56\n",
      "Loss: 0.6820505\n",
      "Batch: 57\n",
      "Loss: 0.6807009\n",
      "Batch: 58\n",
      "Loss: 0.67935383\n",
      "Batch: 59\n",
      "Loss: 0.67800945\n",
      "Batch: 60\n",
      "Loss: 0.6766679\n",
      "Batch: 61\n",
      "Loss: 0.67532897\n",
      "Batch: 62\n",
      "Loss: 0.6739928\n",
      "Batch: 63\n",
      "Loss: 0.6726593\n",
      "Batch: 64\n",
      "Loss: 0.67132837\n",
      "Batch: 65\n",
      "Loss: 0.67000026\n",
      "Batch: 66\n",
      "Loss: 0.66867477\n",
      "Batch: 67\n",
      "Loss: 0.667352\n",
      "Batch: 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6660321\n",
      "Batch: 69\n",
      "Loss: 0.66471463\n",
      "Batch: 70\n",
      "Loss: 0.66340005\n",
      "Batch: 71\n",
      "Loss: 0.66208804\n",
      "Batch: 72\n",
      "Loss: 0.6607786\n",
      "Batch: 73\n",
      "Loss: 0.6594721\n",
      "Batch: 74\n",
      "Loss: 0.65816814\n",
      "Batch: 75\n",
      "Loss: 0.6568668\n",
      "Batch: 76\n",
      "Loss: 0.6555682\n",
      "Batch: 77\n",
      "Train accuracy: 0.9983974358974359\n",
      "Validation accuracy: 0.7088994565217391\n",
      "Loss: 1.185165\n",
      "Batch: 0\n",
      "Loss: 0.92216676\n",
      "Batch: 1\n",
      "Loss: 0.74742746\n",
      "Batch: 2\n",
      "Loss: 0.7317742\n",
      "Batch: 3\n",
      "Loss: 0.7031685\n",
      "Batch: 4\n",
      "Loss: 0.68872476\n",
      "Batch: 5\n",
      "Loss: 0.68291694\n",
      "Batch: 6\n",
      "Loss: 0.6806782\n",
      "Batch: 7\n",
      "Loss: 0.6785071\n",
      "Batch: 8\n",
      "Loss: 0.674973\n",
      "Batch: 9\n",
      "Loss: 0.67100304\n",
      "Batch: 10\n",
      "Loss: 0.6676639\n",
      "Batch: 11\n",
      "Loss: 0.665144\n",
      "Batch: 12\n",
      "Loss: 0.66321176\n",
      "Batch: 13\n",
      "Loss: 0.6616208\n",
      "Batch: 14\n",
      "Loss: 0.6601999\n",
      "Batch: 15\n",
      "Loss: 0.6588511\n",
      "Batch: 16\n",
      "Loss: 0.657523\n",
      "Batch: 17\n",
      "Loss: 0.6561891\n",
      "Batch: 18\n",
      "Loss: 0.6548443\n",
      "Batch: 19\n",
      "Loss: 0.6534978\n",
      "Batch: 20\n",
      "Loss: 0.65216553\n",
      "Batch: 21\n",
      "Loss: 0.6508656\n",
      "Batch: 22\n",
      "Loss: 0.6496026\n",
      "Batch: 23\n",
      "Loss: 0.6483559\n",
      "Batch: 24\n",
      "Loss: 0.6471102\n",
      "Batch: 25\n",
      "Loss: 0.6458692\n",
      "Batch: 26\n",
      "Loss: 0.64464\n",
      "Batch: 27\n",
      "Loss: 0.6434247\n",
      "Batch: 28\n",
      "Loss: 0.64222014\n",
      "Batch: 29\n",
      "Loss: 0.64102155\n",
      "Batch: 30\n",
      "Loss: 0.6398255\n",
      "Batch: 31\n",
      "Loss: 0.63862973\n",
      "Batch: 32\n",
      "Loss: 0.6374339\n",
      "Batch: 33\n",
      "Loss: 0.63623816\n",
      "Batch: 34\n",
      "Loss: 0.6350436\n",
      "Batch: 35\n",
      "Loss: 0.6338509\n",
      "Batch: 36\n",
      "Loss: 0.63266057\n",
      "Batch: 37\n",
      "Loss: 0.6314725\n",
      "Batch: 38\n",
      "Loss: 0.63028574\n",
      "Batch: 39\n",
      "Loss: 0.62909985\n",
      "Batch: 40\n",
      "Loss: 0.62791455\n",
      "Batch: 41\n",
      "Loss: 0.62672985\n",
      "Batch: 42\n",
      "Loss: 0.62554675\n",
      "Batch: 43\n",
      "Loss: 0.62436545\n",
      "Batch: 44\n",
      "Loss: 0.62318647\n",
      "Batch: 45\n",
      "Loss: 0.62200963\n",
      "Batch: 46\n",
      "Loss: 0.62083495\n",
      "Batch: 47\n",
      "Loss: 0.6196621\n",
      "Batch: 48\n",
      "Loss: 0.61849123\n",
      "Batch: 49\n",
      "Loss: 0.6173219\n",
      "Batch: 50\n",
      "Loss: 0.61615443\n",
      "Batch: 51\n",
      "Loss: 0.61498904\n",
      "Batch: 52\n",
      "Loss: 0.61382556\n",
      "Batch: 53\n",
      "Loss: 0.6126644\n",
      "Batch: 54\n",
      "Loss: 0.61150557\n",
      "Batch: 55\n",
      "Loss: 0.61034894\n",
      "Batch: 56\n",
      "Loss: 0.60919446\n",
      "Batch: 57\n",
      "Loss: 0.6080422\n",
      "Batch: 58\n",
      "Loss: 0.60689217\n",
      "Batch: 59\n",
      "Loss: 0.6057441\n",
      "Batch: 60\n",
      "Loss: 0.60459834\n",
      "Batch: 61\n",
      "Loss: 0.6034547\n",
      "Batch: 62\n",
      "Loss: 0.60231334\n",
      "Batch: 63\n",
      "Loss: 0.60117424\n",
      "Batch: 64\n",
      "Loss: 0.60003734\n",
      "Batch: 65\n",
      "Loss: 0.5989028\n",
      "Batch: 66\n",
      "Loss: 0.5977704\n",
      "Batch: 67\n",
      "Loss: 0.59664017\n",
      "Batch: 68\n",
      "Loss: 0.5955123\n",
      "Batch: 69\n",
      "Loss: 0.5943864\n",
      "Batch: 70\n",
      "Loss: 0.59326273\n",
      "Batch: 71\n",
      "Loss: 0.59214133\n",
      "Batch: 72\n",
      "Loss: 0.591022\n",
      "Batch: 73\n",
      "Loss: 0.589905\n",
      "Batch: 74\n",
      "Loss: 0.5887903\n",
      "Batch: 75\n",
      "Loss: 0.5876776\n",
      "Batch: 76\n",
      "Loss: 0.5865672\n",
      "Batch: 77\n",
      "Train accuracy: 0.9988982371794872\n",
      "Validation accuracy: 0.7068614130434783\n",
      "Loss: 1.3137329\n",
      "Batch: 0\n",
      "Loss: 0.8412391\n",
      "Batch: 1\n",
      "Loss: 0.68440145\n",
      "Batch: 2\n",
      "Loss: 0.6211965\n",
      "Batch: 3\n",
      "Loss: 0.6081442\n",
      "Batch: 4\n",
      "Loss: 0.60226697\n",
      "Batch: 5\n",
      "Loss: 0.59443396\n",
      "Batch: 6\n",
      "Loss: 0.58770174\n",
      "Batch: 7\n",
      "Loss: 0.5832158\n",
      "Batch: 8\n",
      "Loss: 0.58011526\n",
      "Batch: 9\n",
      "Loss: 0.5774893\n",
      "Batch: 10\n",
      "Loss: 0.5752469\n",
      "Batch: 11\n",
      "Loss: 0.5734676\n",
      "Batch: 12\n",
      "Loss: 0.5719652\n",
      "Batch: 13\n",
      "Loss: 0.5705218\n",
      "Batch: 14\n",
      "Loss: 0.56911635\n",
      "Batch: 15\n",
      "Loss: 0.56782603\n",
      "Batch: 16\n",
      "Loss: 0.56666505\n",
      "Batch: 17\n",
      "Loss: 0.56557924\n",
      "Batch: 18\n",
      "Loss: 0.5645136\n",
      "Batch: 19\n",
      "Loss: 0.56345206\n",
      "Batch: 20\n",
      "Loss: 0.56240505\n",
      "Batch: 21\n",
      "Loss: 0.56138355\n",
      "Batch: 22\n",
      "Loss: 0.5603892\n",
      "Batch: 23\n",
      "Loss: 0.55941635\n",
      "Batch: 24\n",
      "Loss: 0.5584558\n",
      "Batch: 25\n",
      "Loss: 0.55749923\n",
      "Batch: 26\n",
      "Loss: 0.55653954\n",
      "Batch: 27\n",
      "Loss: 0.555572\n",
      "Batch: 28\n",
      "Loss: 0.554595\n",
      "Batch: 29\n",
      "Loss: 0.5536081\n",
      "Batch: 30\n",
      "Loss: 0.552613\n",
      "Batch: 31\n",
      "Loss: 0.5516114\n",
      "Batch: 32\n",
      "Loss: 0.55060554\n",
      "Batch: 33\n",
      "Loss: 0.54959667\n",
      "Batch: 34\n",
      "Loss: 0.548586\n",
      "Batch: 35\n",
      "Loss: 0.54757386\n",
      "Batch: 36\n",
      "Loss: 0.5465605\n",
      "Batch: 37\n",
      "Loss: 0.5455459\n",
      "Batch: 38\n",
      "Loss: 0.5445298\n",
      "Batch: 39\n",
      "Loss: 0.5435124\n",
      "Batch: 40\n",
      "Loss: 0.54249394\n",
      "Batch: 41\n",
      "Loss: 0.5414747\n",
      "Batch: 42\n",
      "Loss: 0.54045504\n",
      "Batch: 43\n",
      "Loss: 0.5394353\n",
      "Batch: 44\n",
      "Loss: 0.5384159\n",
      "Batch: 45\n",
      "Loss: 0.5373973\n",
      "Batch: 46\n",
      "Loss: 0.53637964\n",
      "Batch: 47\n",
      "Loss: 0.53536314\n",
      "Batch: 48\n",
      "Loss: 0.53434795\n",
      "Batch: 49\n",
      "Loss: 0.53333426\n",
      "Batch: 50\n",
      "Loss: 0.53232193\n",
      "Batch: 51\n",
      "Loss: 0.531311\n",
      "Batch: 52\n",
      "Loss: 0.53030163\n",
      "Batch: 53\n",
      "Loss: 0.52929384\n",
      "Batch: 54\n",
      "Loss: 0.5282874\n",
      "Batch: 55\n",
      "Loss: 0.5272827\n",
      "Batch: 56\n",
      "Loss: 0.5262796\n",
      "Batch: 57\n",
      "Loss: 0.5252782\n",
      "Batch: 58\n",
      "Loss: 0.5242784\n",
      "Batch: 59\n",
      "Loss: 0.5232806\n",
      "Batch: 60\n",
      "Loss: 0.5222845\n",
      "Batch: 61\n",
      "Loss: 0.5212903\n",
      "Batch: 62\n",
      "Loss: 0.52029806\n",
      "Batch: 63\n",
      "Loss: 0.5193077\n",
      "Batch: 64\n",
      "Loss: 0.51831925\n",
      "Batch: 65\n",
      "Loss: 0.5173329\n",
      "Batch: 66\n",
      "Loss: 0.5163484\n",
      "Batch: 67\n",
      "Loss: 0.515366\n",
      "Batch: 68\n",
      "Loss: 0.5143854\n",
      "Batch: 69\n",
      "Loss: 0.5134068\n",
      "Batch: 70\n",
      "Loss: 0.51243013\n",
      "Batch: 71\n",
      "Loss: 0.5114554\n",
      "Batch: 72\n",
      "Loss: 0.5104826\n",
      "Batch: 73\n",
      "Loss: 0.50951177\n",
      "Batch: 74\n",
      "Loss: 0.50854284\n",
      "Batch: 75\n",
      "Loss: 0.50757575\n",
      "Batch: 76\n",
      "Loss: 0.5066107\n",
      "Batch: 77\n",
      "Train accuracy: 0.9987980769230769\n",
      "Validation accuracy: 0.7238451086956522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.7238451086956522"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.996895032051282\n",
      "Validation accuracy: 0.5893342391304348\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9997996794871795\n",
      "Validation accuracy: 0.6036005434782609\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9972956730769231\n",
      "Validation accuracy: 0.578125\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9978966346153846\n",
      "Validation accuracy: 0.6331521739130435\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9996995192307693\n",
      "Validation accuracy: 0.6052989130434783\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9998998397435898\n",
      "Validation accuracy: 0.6219429347826086\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9997996794871795\n",
      "Validation accuracy: 0.5981657608695652\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9998998397435898\n",
      "Validation accuracy: 0.623641304347826\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9993990384615384\n",
      "Validation accuracy: 0.6141304347826086\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9997996794871795\n",
      "Validation accuracy: 0.6022418478260869\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9918870192307693\n",
      "Validation accuracy: 0.5436827956989247\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9983974358974359\n",
      "Validation accuracy: 0.5446908602150538\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9990985576923077\n",
      "Validation accuracy: 0.5134408602150538\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9993990384615384\n",
      "Validation accuracy: 0.5567876344086021\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9984975961538461\n",
      "Validation accuracy: 0.5349462365591398\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9988982371794872\n",
      "Validation accuracy: 0.5591397849462365\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9996995192307693\n",
      "Validation accuracy: 0.5181451612903226\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9997996794871795\n",
      "Validation accuracy: 0.5332661290322581\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9994991987179487\n",
      "Validation accuracy: 0.5467069892473119\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9995993589743589\n",
      "Validation accuracy: 0.5816532258064516\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.6063701923076923\n",
      "Validation accuracy: 0.5641801075268817\n",
      "Epoch: 1\n",
      "Train accuracy: 0.5257411858974359\n",
      "Validation accuracy: 0.44993279569892475\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5790264423076923\n",
      "Validation accuracy: 0.5191532258064516\n",
      "Epoch: 3\n",
      "Train accuracy: 0.6110777243589743\n",
      "Validation accuracy: 0.508736559139785\n",
      "Epoch: 4\n",
      "Train accuracy: 0.6459334935897436\n",
      "Validation accuracy: 0.5520833333333334\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5921474358974359\n",
      "Validation accuracy: 0.5325940860215054\n",
      "Epoch: 6\n",
      "Train accuracy: 0.6207932692307693\n",
      "Validation accuracy: 0.5618279569892473\n",
      "Epoch: 7\n",
      "Train accuracy: 0.6371193910256411\n",
      "Validation accuracy: 0.53125\n",
      "Epoch: 8\n",
      "Train accuracy: 0.6133814102564102\n",
      "Validation accuracy: 0.5154569892473119\n",
      "Epoch: 9\n",
      "Train accuracy: 0.6663661858974359\n",
      "Validation accuracy: 0.5692204301075269\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9235777243589743\n",
      "Validation accuracy: 0.5003360215053764\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9554286858974359\n",
      "Validation accuracy: 0.5198252688172043\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9669471153846154\n",
      "Validation accuracy: 0.6155913978494624\n",
      "Epoch: 3\n",
      "Train accuracy: 0.979667467948718\n",
      "Validation accuracy: 0.5403225806451613\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9611378205128205\n",
      "Validation accuracy: 0.6676747311827957\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9819711538461539\n",
      "Validation accuracy: 0.5470430107526881\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9633413461538461\n",
      "Validation accuracy: 0.5675403225806451\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9852764423076923\n",
      "Validation accuracy: 0.6414650537634409\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9886818910256411\n",
      "Validation accuracy: 0.6979166666666666\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9790665064102564\n",
      "Validation accuracy: 0.6189516129032258\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.6682692307692307\n",
      "Validation accuracy: 0.5994623655913979\n",
      "Epoch: 1\n",
      "Train accuracy: 0.6543469551282052\n",
      "Validation accuracy: 0.5272177419354839\n",
      "Epoch: 2\n",
      "Train accuracy: 0.6555488782051282\n",
      "Validation accuracy: 0.6159274193548387\n",
      "Epoch: 3\n",
      "Train accuracy: 0.5986578525641025\n",
      "Validation accuracy: 0.5238575268817204\n",
      "Epoch: 4\n",
      "Train accuracy: 0.5797275641025641\n",
      "Validation accuracy: 0.5793010752688172\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5719150641025641\n",
      "Validation accuracy: 0.511760752688172\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5444711538461539\n",
      "Validation accuracy: 0.5426747311827957\n",
      "Epoch: 7\n",
      "Train accuracy: 0.6205929487179487\n",
      "Validation accuracy: 0.5766129032258065\n",
      "Epoch: 8\n",
      "Train accuracy: 0.6809895833333334\n",
      "Validation accuracy: 0.6122311827956989\n",
      "Epoch: 9\n",
      "Train accuracy: 0.6655649038461539\n",
      "Validation accuracy: 0.5460349462365591\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.8869190705128205\n",
      "Validation accuracy: 0.5536684782608695\n",
      "Epoch: 1\n",
      "Train accuracy: 0.8936298076923077\n",
      "Validation accuracy: 0.5461956521739131\n",
      "Epoch: 2\n",
      "Train accuracy: 0.8960336538461539\n",
      "Validation accuracy: 0.5424592391304348\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9026442307692307\n",
      "Validation accuracy: 0.5414402173913043\n",
      "Epoch: 4\n",
      "Train accuracy: 0.8948317307692307\n",
      "Validation accuracy: 0.5652173913043478\n",
      "Epoch: 5\n",
      "Train accuracy: 0.8978365384615384\n",
      "Validation accuracy: 0.5910326086956522\n",
      "Epoch: 6\n",
      "Train accuracy: 0.895332532051282\n",
      "Validation accuracy: 0.5713315217391305\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9493189102564102\n",
      "Validation accuracy: 0.5815217391304348\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9067508012820513\n",
      "Validation accuracy: 0.6182065217391305\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9286858974358975\n",
      "Validation accuracy: 0.5968070652173914\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9710536858974359\n",
      "Validation accuracy: 0.5090725806451613\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9792668269230769\n",
      "Validation accuracy: 0.6112231182795699\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9933894230769231\n",
      "Validation accuracy: 0.5756048387096774\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9967948717948718\n",
      "Validation accuracy: 0.6004704301075269\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9974959935897436\n",
      "Validation accuracy: 0.5823252688172043\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9984975961538461\n",
      "Validation accuracy: 0.6508736559139785\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9996995192307693\n",
      "Validation accuracy: 0.6303763440860215\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9974959935897436\n",
      "Validation accuracy: 0.6394489247311828\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9997996794871795\n",
      "Validation accuracy: 0.6491935483870968\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9976963141025641\n",
      "Validation accuracy: 0.6848118279569892\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5352564102564102\n",
      "Validation accuracy: 0.5217391304347826\n",
      "Epoch: 1\n",
      "Train accuracy: 0.5584935897435898\n",
      "Validation accuracy: 0.5900135869565217\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5900440705128205\n",
      "Validation accuracy: 0.5180027173913043\n",
      "Epoch: 3\n",
      "Train accuracy: 0.5230368589743589\n",
      "Validation accuracy: 0.47316576086956524\n",
      "Epoch: 4\n",
      "Train accuracy: 0.5220352564102564\n",
      "Validation accuracy: 0.5037364130434783\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5409655448717948\n",
      "Validation accuracy: 0.5536684782608695\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5330528846153846\n",
      "Validation accuracy: 0.5356657608695652\n",
      "Epoch: 7\n",
      "Train accuracy: 0.5342548076923077\n",
      "Validation accuracy: 0.5261548913043478\n",
      "Epoch: 8\n",
      "Train accuracy: 0.604667467948718\n",
      "Validation accuracy: 0.5363451086956522\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5757211538461539\n",
      "Validation accuracy: 0.5067934782608695\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.530448717948718\n",
      "Validation accuracy: 0.5078125\n",
      "Epoch: 1\n",
      "Train accuracy: 0.5363581730769231\n",
      "Validation accuracy: 0.5125679347826086\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5140224358974359\n",
      "Validation accuracy: 0.4945652173913043\n",
      "Epoch: 3\n",
      "Train accuracy: 0.5237379807692307\n",
      "Validation accuracy: 0.49660326086956524\n",
      "Epoch: 4\n",
      "Train accuracy: 0.5207331730769231\n",
      "Validation accuracy: 0.49014945652173914\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5269431089743589\n",
      "Validation accuracy: 0.49558423913043476\n",
      "Epoch: 6\n",
      "Train accuracy: 0.6056690705128205\n",
      "Validation accuracy: 0.5118885869565217\n",
      "Epoch: 7\n",
      "Train accuracy: 0.5353565705128205\n",
      "Validation accuracy: 0.5003396739130435\n",
      "Epoch: 8\n",
      "Train accuracy: 0.561698717948718\n",
      "Validation accuracy: 0.5146059782608695\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5539863782051282\n",
      "Validation accuracy: 0.492866847826087\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9899839743589743\n",
      "Validation accuracy: 0.5584677419354839\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9954927884615384\n",
      "Validation accuracy: 0.5887096774193549\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9984975961538461\n",
      "Validation accuracy: 0.5504032258064516\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9971955128205128\n",
      "Validation accuracy: 0.5248655913978495\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9979967948717948\n",
      "Validation accuracy: 0.5772849462365591\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9989983974358975\n",
      "Validation accuracy: 0.5453629032258065\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9995993589743589\n",
      "Validation accuracy: 0.5577956989247311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Train accuracy: 0.9993990384615384\n",
      "Validation accuracy: 0.5749327956989247\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9996995192307693\n",
      "Validation accuracy: 0.5685483870967742\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9996995192307693\n",
      "Validation accuracy: 0.5584677419354839\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9994991987179487\n",
      "Validation accuracy: 0.5557795698924731\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9988982371794872\n",
      "Validation accuracy: 0.5305779569892473\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9964943910256411\n",
      "Validation accuracy: 0.5621639784946236\n",
      "Epoch: 3\n",
      "Train accuracy: 0.996895032051282\n",
      "Validation accuracy: 0.5396505376344086\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9889823717948718\n",
      "Validation accuracy: 0.5547715053763441\n",
      "Epoch: 5\n",
      "Train accuracy: 0.996895032051282\n",
      "Validation accuracy: 0.5682123655913979\n",
      "Epoch: 6\n",
      "Train accuracy: 0.99609375\n",
      "Validation accuracy: 0.5547715053763441\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9903846153846154\n",
      "Validation accuracy: 0.5292338709677419\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9945913461538461\n",
      "Validation accuracy: 0.5658602150537635\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9942908653846154\n",
      "Validation accuracy: 0.5903897849462365\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9866786858974359\n",
      "Validation accuracy: 0.542002688172043\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9845753205128205\n",
      "Validation accuracy: 0.4969758064516129\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9918870192307693\n",
      "Validation accuracy: 0.5685483870967742\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9940905448717948\n",
      "Validation accuracy: 0.6216397849462365\n",
      "Epoch: 4\n",
      "Train accuracy: 0.99609375\n",
      "Validation accuracy: 0.5621639784946236\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9953926282051282\n",
      "Validation accuracy: 0.6038306451612904\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9955929487179487\n",
      "Validation accuracy: 0.5416666666666666\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9934895833333334\n",
      "Validation accuracy: 0.6196236559139785\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9967948717948718\n",
      "Validation accuracy: 0.6283602150537635\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9978966346153846\n",
      "Validation accuracy: 0.6071908602150538\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.6264022435897436\n",
      "Validation accuracy: 0.510752688172043\n",
      "Epoch: 1\n",
      "Train accuracy: 0.8266225961538461\n",
      "Validation accuracy: 0.4895833333333333\n",
      "Epoch: 2\n",
      "Train accuracy: 0.8891225961538461\n",
      "Validation accuracy: 0.5641801075268817\n",
      "Epoch: 3\n",
      "Train accuracy: 0.8920272435897436\n",
      "Validation accuracy: 0.5198252688172043\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9382011217948718\n",
      "Validation accuracy: 0.5540994623655914\n",
      "Epoch: 5\n",
      "Train accuracy: 0.8847155448717948\n",
      "Validation accuracy: 0.5315860215053764\n",
      "Epoch: 6\n",
      "Train accuracy: 0.8284254807692307\n",
      "Validation accuracy: 0.4939516129032258\n",
      "Epoch: 7\n",
      "Train accuracy: 0.8036858974358975\n",
      "Validation accuracy: 0.5080645161290323\n",
      "Epoch: 8\n",
      "Train accuracy: 0.8954326923076923\n",
      "Validation accuracy: 0.5816532258064516\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9260817307692307\n",
      "Validation accuracy: 0.5628360215053764\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.8057892628205128\n",
      "Validation accuracy: 0.6135752688172043\n",
      "Epoch: 1\n",
      "Train accuracy: 0.6789863782051282\n",
      "Validation accuracy: 0.5887096774193549\n",
      "Epoch: 2\n",
      "Train accuracy: 0.6696714743589743\n",
      "Validation accuracy: 0.572244623655914\n",
      "Epoch: 3\n",
      "Train accuracy: 0.643729967948718\n",
      "Validation accuracy: 0.5376344086021505\n",
      "Epoch: 4\n",
      "Train accuracy: 0.7172475961538461\n",
      "Validation accuracy: 0.5779569892473119\n",
      "Epoch: 5\n",
      "Train accuracy: 0.6546474358974359\n",
      "Validation accuracy: 0.5248655913978495\n",
      "Epoch: 6\n",
      "Train accuracy: 0.6524439102564102\n",
      "Validation accuracy: 0.5987903225806451\n",
      "Epoch: 7\n",
      "Train accuracy: 0.643729967948718\n",
      "Validation accuracy: 0.5709005376344086\n",
      "Epoch: 8\n",
      "Train accuracy: 0.6291065705128205\n",
      "Validation accuracy: 0.5984543010752689\n",
      "Epoch: 9\n",
      "Train accuracy: 0.7359775641025641\n",
      "Validation accuracy: 0.6283602150537635\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5835336538461539\n",
      "Validation accuracy: 0.5193614130434783\n",
      "Epoch: 1\n",
      "Train accuracy: 0.6424278846153846\n",
      "Validation accuracy: 0.5940896739130435\n",
      "Epoch: 2\n",
      "Train accuracy: 0.6122796474358975\n",
      "Validation accuracy: 0.6226222826086957\n",
      "Epoch: 3\n",
      "Train accuracy: 0.6121794871794872\n",
      "Validation accuracy: 0.49592391304347827\n",
      "Epoch: 4\n",
      "Train accuracy: 0.6412259615384616\n",
      "Validation accuracy: 0.5451766304347826\n",
      "Epoch: 5\n",
      "Train accuracy: 0.6030649038461539\n",
      "Validation accuracy: 0.5944293478260869\n",
      "Epoch: 6\n",
      "Train accuracy: 0.6299078525641025\n",
      "Validation accuracy: 0.6056385869565217\n",
      "Epoch: 7\n",
      "Train accuracy: 0.64453125\n",
      "Validation accuracy: 0.5995244565217391\n",
      "Epoch: 8\n",
      "Train accuracy: 0.6618589743589743\n",
      "Validation accuracy: 0.6375679347826086\n",
      "Epoch: 9\n",
      "Train accuracy: 0.6078725961538461\n",
      "Validation accuracy: 0.5614809782608695\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.7237580128205128\n",
      "Validation accuracy: 0.5181451612903226\n",
      "Epoch: 1\n",
      "Train accuracy: 0.7912660256410257\n",
      "Validation accuracy: 0.5131048387096774\n",
      "Epoch: 2\n",
      "Train accuracy: 0.8751001602564102\n",
      "Validation accuracy: 0.5265456989247311\n",
      "Epoch: 3\n",
      "Train accuracy: 0.8827123397435898\n",
      "Validation accuracy: 0.5423387096774194\n",
      "Epoch: 4\n",
      "Train accuracy: 0.922676282051282\n",
      "Validation accuracy: 0.530241935483871\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9496193910256411\n",
      "Validation accuracy: 0.5584677419354839\n",
      "Epoch: 6\n",
      "Train accuracy: 0.8721955128205128\n",
      "Validation accuracy: 0.5487231182795699\n",
      "Epoch: 7\n",
      "Train accuracy: 0.913261217948718\n",
      "Validation accuracy: 0.5688844086021505\n",
      "Epoch: 8\n",
      "Train accuracy: 0.8811097756410257\n",
      "Validation accuracy: 0.5275537634408602\n",
      "Epoch: 9\n",
      "Train accuracy: 0.8594751602564102\n",
      "Validation accuracy: 0.5577956989247311\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9958934294871795\n",
      "Validation accuracy: 0.5386424731182796\n",
      "Epoch: 1\n",
      "Train accuracy: 0.995292467948718\n",
      "Validation accuracy: 0.5762768817204301\n",
      "Epoch: 2\n",
      "Train accuracy: 0.6889022435897436\n",
      "Validation accuracy: 0.5325940860215054\n",
      "Epoch: 3\n",
      "Train accuracy: 0.6441306089743589\n",
      "Validation accuracy: 0.5272177419354839\n",
      "Epoch: 4\n",
      "Train accuracy: 0.4673477564102564\n",
      "Validation accuracy: 0.4855510752688172\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5458733974358975\n",
      "Validation accuracy: 0.4831989247311828\n",
      "Epoch: 6\n",
      "Train accuracy: 0.6022636217948718\n",
      "Validation accuracy: 0.49260752688172044\n",
      "Epoch: 7\n",
      "Train accuracy: 0.628104967948718\n",
      "Validation accuracy: 0.5356182795698925\n",
      "Epoch: 8\n",
      "Train accuracy: 0.7904647435897436\n",
      "Validation accuracy: 0.5003360215053764\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5071113782051282\n",
      "Validation accuracy: 0.5067204301075269\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9945913461538461\n",
      "Validation accuracy: 0.5228494623655914\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9940905448717948\n",
      "Validation accuracy: 0.5228494623655914\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9969951923076923\n",
      "Validation accuracy: 0.5567876344086021\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9969951923076923\n",
      "Validation accuracy: 0.5887096774193549\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9961939102564102\n",
      "Validation accuracy: 0.5944220430107527\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9979967948717948\n",
      "Validation accuracy: 0.5702284946236559\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9972956730769231\n",
      "Validation accuracy: 0.5836693548387096\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9973958333333334\n",
      "Validation accuracy: 0.614247311827957\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9977964743589743\n",
      "Validation accuracy: 0.6401209677419355\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9976963141025641\n",
      "Validation accuracy: 0.633736559139785\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.48127003205128205\n",
      "Validation accuracy: 0.5016801075268817\n",
      "Epoch: 1\n",
      "Train accuracy: 0.410556891025641\n",
      "Validation accuracy: 0.5359543010752689\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5586939102564102\n",
      "Validation accuracy: 0.5184811827956989\n",
      "Epoch: 3\n",
      "Train accuracy: 0.5388621794871795\n",
      "Validation accuracy: 0.5036962365591398\n",
      "Epoch: 4\n",
      "Train accuracy: 0.4742588141025641\n",
      "Validation accuracy: 0.5497311827956989\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5406650641025641\n",
      "Validation accuracy: 0.5530913978494624\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5126201923076923\n",
      "Validation accuracy: 0.5147849462365591\n",
      "Epoch: 7\n",
      "Train accuracy: 0.5795272435897436\n",
      "Validation accuracy: 0.5067204301075269\n",
      "Epoch: 8\n",
      "Train accuracy: 0.6783854166666666\n",
      "Validation accuracy: 0.5104166666666666\n",
      "Epoch: 9\n",
      "Train accuracy: 0.3816105769230769\n",
      "Validation accuracy: 0.53125\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5277443910256411\n",
      "Validation accuracy: 0.5090725806451613\n",
      "Epoch: 1\n",
      "Train accuracy: 0.5017027243589743\n",
      "Validation accuracy: 0.48588709677419356\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5\n",
      "Validation accuracy: 0.5151209677419355\n",
      "Epoch: 3\n",
      "Train accuracy: 0.53125\n",
      "Validation accuracy: 0.5060483870967742\n",
      "Epoch: 4\n",
      "Train accuracy: 0.59375\n",
      "Validation accuracy: 0.5080645161290323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Train accuracy: 0.59375\n",
      "Validation accuracy: 0.49764784946236557\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5625\n",
      "Validation accuracy: 0.5030241935483871\n",
      "Epoch: 7\n",
      "Train accuracy: 0.71875\n",
      "Validation accuracy: 0.4986559139784946\n",
      "Epoch: 8\n",
      "Train accuracy: 0.46875\n",
      "Validation accuracy: 0.5026881720430108\n",
      "Epoch: 9\n",
      "Train accuracy: 0.4778645833333333\n",
      "Validation accuracy: 0.521505376344086\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9591346153846154\n",
      "Validation accuracy: 0.614247311827957\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9964943910256411\n",
      "Validation accuracy: 0.6414650537634409\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9898838141025641\n",
      "Validation accuracy: 0.6525537634408602\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9965945512820513\n",
      "Validation accuracy: 0.6622983870967742\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9959935897435898\n",
      "Validation accuracy: 0.6428091397849462\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9972956730769231\n",
      "Validation accuracy: 0.6387768817204301\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9964943910256411\n",
      "Validation accuracy: 0.6512096774193549\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9990985576923077\n",
      "Validation accuracy: 0.6555779569892473\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9957932692307693\n",
      "Validation accuracy: 0.6801075268817204\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9971955128205128\n",
      "Validation accuracy: 0.6663306451612904\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9798677884615384\n",
      "Validation accuracy: 0.5685483870967742\n",
      "Epoch: 1\n",
      "Train accuracy: 0.8181089743589743\n",
      "Validation accuracy: 0.550739247311828\n",
      "Epoch: 2\n",
      "Train accuracy: 0.42237580128205127\n",
      "Validation accuracy: 0.5272177419354839\n",
      "Epoch: 3\n",
      "Train accuracy: 0.47115384615384615\n",
      "Validation accuracy: 0.5325940860215054\n",
      "Epoch: 4\n",
      "Train accuracy: 0.6118790064102564\n",
      "Validation accuracy: 0.5641801075268817\n",
      "Epoch: 5\n",
      "Train accuracy: 0.4137620192307692\n",
      "Validation accuracy: 0.5584677419354839\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5985576923076923\n",
      "Validation accuracy: 0.5752688172043011\n",
      "Epoch: 7\n",
      "Train accuracy: 0.47806490384615385\n",
      "Validation accuracy: 0.5759408602150538\n",
      "Epoch: 8\n",
      "Train accuracy: 0.5625\n",
      "Validation accuracy: 0.581989247311828\n",
      "Epoch: 9\n",
      "Train accuracy: 0.44200721153846156\n",
      "Validation accuracy: 0.5887096774193549\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9472155448717948\n",
      "Validation accuracy: 0.5221774193548387\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9786658653846154\n",
      "Validation accuracy: 0.6001344086021505\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9924879807692307\n",
      "Validation accuracy: 0.5356182795698925\n",
      "Epoch: 3\n",
      "Train accuracy: 0.987479967948718\n",
      "Validation accuracy: 0.6186155913978495\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9984975961538461\n",
      "Validation accuracy: 0.5544354838709677\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9974959935897436\n",
      "Validation accuracy: 0.6243279569892473\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9975961538461539\n",
      "Validation accuracy: 0.6421370967741935\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9975961538461539\n",
      "Validation accuracy: 0.6139112903225806\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9984975961538461\n",
      "Validation accuracy: 0.605510752688172\n",
      "Epoch: 9\n",
      "Train accuracy: 0.999198717948718\n",
      "Validation accuracy: 0.5950940860215054\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.4375\n",
      "Validation accuracy: 0.4956317204301075\n",
      "Epoch: 1\n",
      "Train accuracy: 0.6875\n",
      "Validation accuracy: 0.49361559139784944\n",
      "Epoch: 2\n",
      "Train accuracy: 0.40625\n",
      "Validation accuracy: 0.49764784946236557\n",
      "Epoch: 3\n",
      "Train accuracy: 0.5\n",
      "Validation accuracy: 0.4788306451612903\n",
      "Epoch: 4\n",
      "Train accuracy: 0.375\n",
      "Validation accuracy: 0.5053763440860215\n",
      "Epoch: 5\n",
      "Train accuracy: 0.6875\n",
      "Validation accuracy: 0.49227150537634407\n",
      "Epoch: 6\n",
      "Train accuracy: 0.46875\n",
      "Validation accuracy: 0.4959677419354839\n",
      "Epoch: 7\n",
      "Train accuracy: 0.625\n",
      "Validation accuracy: 0.49731182795698925\n",
      "Epoch: 8\n",
      "Train accuracy: 0.59375\n",
      "Validation accuracy: 0.49294354838709675\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5625\n",
      "Validation accuracy: 0.4986559139784946\n",
      "!\n",
      "training\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,192,39,39] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_24/Adam/gradients/sequential_26/max_pooling2d_52/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](sequential_26/conv2d_77/Relu, sequential_26/max_pooling2d_52/MaxPool, training_24/Adam/gradients/sequential_26/conv2d_78/convolution_grad/Conv2DBackpropInput)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1565b30ed935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0macq_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Expected Improvement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                             x0=default_parameters)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-fc7832c003a2>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(**params)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_batch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtrain_batch_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,192,39,39] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_24/Adam/gradients/sequential_26/max_pooling2d_52/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](sequential_26/conv2d_77/Relu, sequential_26/max_pooling2d_52/MaxPool, training_24/Adam/gradients/sequential_26/conv2d_78/convolution_grad/Conv2DBackpropInput)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=30,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f17dd3bd630>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_objective(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.7445652173913043,\n",
       "  [1.5178328855285662e-06,\n",
       "   0.0004895539356438342,\n",
       "   0.0020969879904624285,\n",
       "   6.214158600010628e-06,\n",
       "   9.135323619965988e-05,\n",
       "   4.5481429094701505e-07,\n",
       "   8,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   48,\n",
       "   208,\n",
       "   256,\n",
       "   192,\n",
       "   2560,\n",
       "   1e-05,\n",
       "   0.9999,\n",
       "   64]),\n",
       " (-0.7245244565217391,\n",
       "  [1.3033188318149699e-06,\n",
       "   0.010520064725614906,\n",
       "   2.499583831538053e-07,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   0.0014668749087331124,\n",
       "   5,\n",
       "   3,\n",
       "   3,\n",
       "   6,\n",
       "   96,\n",
       "   80,\n",
       "   48,\n",
       "   192,\n",
       "   2560,\n",
       "   0.013097073829578391,\n",
       "   0.5585870123208373,\n",
       "   64]),\n",
       " (-0.71875,\n",
       "  [1.8176001939748006e-06,\n",
       "   0.00518751877267737,\n",
       "   0.1,\n",
       "   1.3916145310640059e-07,\n",
       "   0.004910254336438996,\n",
       "   4.715725769220243e-07,\n",
       "   8,\n",
       "   10,\n",
       "   3,\n",
       "   5,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   0.1142465464862375,\n",
       "   0.529921974682424,\n",
       "   64]),\n",
       " (-0.7007472826086957,\n",
       "  [1.3501981392404177e-06,\n",
       "   0.006593009505125237,\n",
       "   0.0009600887444387682,\n",
       "   1.0811711460849992e-08,\n",
       "   0.0035589595657425904,\n",
       "   8.723934974406631e-07,\n",
       "   6,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   0.1906948287763293,\n",
       "   0.014700459585290586,\n",
       "   64]),\n",
       " (-0.6905570652173914,\n",
       "  [1.5271087834413554e-06,\n",
       "   0.007888906971196964,\n",
       "   5.395714195501132e-08,\n",
       "   1.0956650929129386e-06,\n",
       "   8.024533253365652e-05,\n",
       "   1.0938355591900463e-08,\n",
       "   8,\n",
       "   5,\n",
       "   7,\n",
       "   4,\n",
       "   224,\n",
       "   176,\n",
       "   256,\n",
       "   16,\n",
       "   2560,\n",
       "   1e-05,\n",
       "   0.9999,\n",
       "   64]),\n",
       " (-0.6895161290322581,\n",
       "  [1.0555940878921794e-07,\n",
       "   0.09317373040508546,\n",
       "   0.0005202358529301968,\n",
       "   1.653231861582997e-08,\n",
       "   0.003380047187571669,\n",
       "   3.599907586274443e-07,\n",
       "   8,\n",
       "   8,\n",
       "   7,\n",
       "   5,\n",
       "   64,\n",
       "   112,\n",
       "   128,\n",
       "   256,\n",
       "   2048,\n",
       "   0.012709323330334494,\n",
       "   0.006521082839045292,\n",
       "   32]),\n",
       " (-0.6871603260869565,\n",
       "  [1.536613113991343e-06,\n",
       "   2.1945902026804268e-07,\n",
       "   2.013005567194683e-08,\n",
       "   0.0061658296743864445,\n",
       "   0.021683252838781465,\n",
       "   1.8405247250166618e-06,\n",
       "   5,\n",
       "   4,\n",
       "   7,\n",
       "   4,\n",
       "   64,\n",
       "   112,\n",
       "   64,\n",
       "   144,\n",
       "   2560,\n",
       "   1e-05,\n",
       "   0.9999,\n",
       "   64]),\n",
       " (-0.677755376344086,\n",
       "  [1.3697546212118618e-06,\n",
       "   0.00019993448872016327,\n",
       "   0.03198496007230006,\n",
       "   1.3036334237082671e-08,\n",
       "   9.489981319771628e-08,\n",
       "   1.2311389714024988e-05,\n",
       "   5,\n",
       "   6,\n",
       "   3,\n",
       "   6,\n",
       "   48,\n",
       "   144,\n",
       "   112,\n",
       "   176,\n",
       "   2560,\n",
       "   0.8972696493803541,\n",
       "   0.902409288920951,\n",
       "   32]),\n",
       " (-0.6691576086956522,\n",
       "  [6.74166144683711e-06,\n",
       "   0.00019115498036790615,\n",
       "   0.0003209438029110774,\n",
       "   2.1248941737008284e-06,\n",
       "   0.1,\n",
       "   1.7996620800780862e-08,\n",
       "   4,\n",
       "   7,\n",
       "   7,\n",
       "   3,\n",
       "   64,\n",
       "   80,\n",
       "   128,\n",
       "   256,\n",
       "   2048,\n",
       "   0.4685215381722805,\n",
       "   0.9534106502346201,\n",
       "   64]),\n",
       " (-0.6603260869565217,\n",
       "  [1.4701272990387835e-06,\n",
       "   3.3078620735208596e-08,\n",
       "   0.005802631773594926,\n",
       "   0.001526156308703228,\n",
       "   0.09896520070339151,\n",
       "   1e-08,\n",
       "   10,\n",
       "   5,\n",
       "   7,\n",
       "   4,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   0.8790867410306067,\n",
       "   1e-05,\n",
       "   64]),\n",
       " (-0.6348505434782609,\n",
       "  [6e-05,\n",
       "   0.0002,\n",
       "   0.0002,\n",
       "   0.0002,\n",
       "   0.0002,\n",
       "   0.001,\n",
       "   10,\n",
       "   7,\n",
       "   4,\n",
       "   4,\n",
       "   64,\n",
       "   128,\n",
       "   128,\n",
       "   256,\n",
       "   2048,\n",
       "   0.9,\n",
       "   0.999,\n",
       "   64]),\n",
       " (-0.6073369565217391,\n",
       "  [1.2122721305455692e-05,\n",
       "   0.01175790323881513,\n",
       "   0.0037113504783813874,\n",
       "   2.8440008886486996e-07,\n",
       "   1.3318471316006523e-06,\n",
       "   0.01074515048486014,\n",
       "   11,\n",
       "   8,\n",
       "   3,\n",
       "   4,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   1e-05,\n",
       "   0.3377741507095172,\n",
       "   64]),\n",
       " (-0.5985054347826086,\n",
       "  [1.7424131400864444e-05,\n",
       "   6.007211029488985e-07,\n",
       "   7.025826647330092e-08,\n",
       "   3.2221806955520504e-08,\n",
       "   1.9969767055635207e-05,\n",
       "   3.030716461229088e-08,\n",
       "   8,\n",
       "   9,\n",
       "   7,\n",
       "   4,\n",
       "   48,\n",
       "   144,\n",
       "   112,\n",
       "   160,\n",
       "   2560,\n",
       "   0.9167463853152296,\n",
       "   0.48502131446374575,\n",
       "   64]),\n",
       " (-0.5981182795698925,\n",
       "  [0.0054127287908113456,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   2.2241158908618434e-08,\n",
       "   1e-08,\n",
       "   6,\n",
       "   6,\n",
       "   8,\n",
       "   3,\n",
       "   48,\n",
       "   144,\n",
       "   112,\n",
       "   160,\n",
       "   2560,\n",
       "   0.7481337994242019,\n",
       "   0.3359416908325563,\n",
       "   32]),\n",
       " (-0.5940896739130435,\n",
       "  [0.00013364624916903647,\n",
       "   1.0416618992098408e-07,\n",
       "   1.4833579164775348e-06,\n",
       "   0.0013636673304319711,\n",
       "   0.007563209768960482,\n",
       "   1.6192011872360527e-06,\n",
       "   11,\n",
       "   9,\n",
       "   6,\n",
       "   4,\n",
       "   80,\n",
       "   48,\n",
       "   16,\n",
       "   112,\n",
       "   2304,\n",
       "   0.8405727463155327,\n",
       "   0.5642471524070105,\n",
       "   64]),\n",
       " (-0.5665322580645161,\n",
       "  [7.745479653549314e-05,\n",
       "   0.060902900439212294,\n",
       "   0.00021505678541084607,\n",
       "   0.0057636450782077835,\n",
       "   0.00043803173785116495,\n",
       "   0.04889117953096646,\n",
       "   6,\n",
       "   9,\n",
       "   4,\n",
       "   5,\n",
       "   160,\n",
       "   32,\n",
       "   176,\n",
       "   32,\n",
       "   3072,\n",
       "   0.030556747654708317,\n",
       "   0.4098555713877424,\n",
       "   32]),\n",
       " (-0.5641983695652174,\n",
       "  [1.9287831836458686e-05,\n",
       "   0.0377846894599227,\n",
       "   1.2449882347712405e-07,\n",
       "   0.00015955844941531686,\n",
       "   0.0012025902163508362,\n",
       "   0.004429230280166121,\n",
       "   9,\n",
       "   7,\n",
       "   6,\n",
       "   4,\n",
       "   144,\n",
       "   112,\n",
       "   160,\n",
       "   32,\n",
       "   2560,\n",
       "   0.9177878217050816,\n",
       "   0.5339299069637019,\n",
       "   64]),\n",
       " (-0.5547715053763441,\n",
       "  [3.567149331545089e-05,\n",
       "   0.0008885694224615459,\n",
       "   0.006744263085432023,\n",
       "   3.606498234980932e-05,\n",
       "   0.1,\n",
       "   2.7415988727220302e-06,\n",
       "   6,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   208,\n",
       "   64,\n",
       "   128,\n",
       "   256,\n",
       "   2048,\n",
       "   0.13036487604963587,\n",
       "   1e-05,\n",
       "   32]),\n",
       " (-0.5504032258064516,\n",
       "  [0.0003905456133842435,\n",
       "   6.419655671468e-06,\n",
       "   0.00010677167211585777,\n",
       "   1e-08,\n",
       "   4.3940056650352265e-05,\n",
       "   5.691218566431342e-07,\n",
       "   11,\n",
       "   3,\n",
       "   7,\n",
       "   6,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   0.2968535997888272,\n",
       "   1e-05,\n",
       "   32]),\n",
       " (-0.5186820652173914,\n",
       "  [0.0008460674278485597,\n",
       "   5.334077536622421e-08,\n",
       "   0.0010124931868582979,\n",
       "   8.663499284358695e-07,\n",
       "   0.1,\n",
       "   1.6520082939254864e-07,\n",
       "   10,\n",
       "   5,\n",
       "   4,\n",
       "   5,\n",
       "   240,\n",
       "   160,\n",
       "   128,\n",
       "   256,\n",
       "   2048,\n",
       "   1e-05,\n",
       "   0.6841666334139775,\n",
       "   64]),\n",
       " (-0.5181451612903226,\n",
       "  [0.04000510344086536,\n",
       "   0.005835666092930939,\n",
       "   0.006002676494331097,\n",
       "   0.002255756482601523,\n",
       "   2.624387408217529e-07,\n",
       "   2.5560157933955434e-06,\n",
       "   6,\n",
       "   8,\n",
       "   7,\n",
       "   6,\n",
       "   64,\n",
       "   256,\n",
       "   176,\n",
       "   16,\n",
       "   2816,\n",
       "   0.5021592073815234,\n",
       "   0.42847170334106854,\n",
       "   32]),\n",
       " (-0.5122282608695652,\n",
       "  [1e-07,\n",
       "   1e-08,\n",
       "   0.003947642474542367,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   3,\n",
       "   3,\n",
       "   8,\n",
       "   6,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   0.9999,\n",
       "   1e-05,\n",
       "   64]),\n",
       " (-0.5067934782608695,\n",
       "  [0.07815886159344815,\n",
       "   1.1123807020692621e-07,\n",
       "   1.1309675800102237e-08,\n",
       "   1.8548778155491597e-08,\n",
       "   1.7958569767220705e-05,\n",
       "   3.7299163971015324e-05,\n",
       "   5,\n",
       "   10,\n",
       "   6,\n",
       "   5,\n",
       "   160,\n",
       "   128,\n",
       "   160,\n",
       "   176,\n",
       "   3840,\n",
       "   0.8718364920330032,\n",
       "   0.6492092039313555,\n",
       "   64]),\n",
       " (-0.5027173913043478,\n",
       "  [1e-07,\n",
       "   1e-08,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   3,\n",
       "   3,\n",
       "   8,\n",
       "   3,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   160,\n",
       "   2560,\n",
       "   0.9999,\n",
       "   1e-05,\n",
       "   64]),\n",
       " (-0.49898097826086957,\n",
       "  [0.0005454801910240061,\n",
       "   8.034983947365333e-08,\n",
       "   0.08502519186966044,\n",
       "   0.049621437355391554,\n",
       "   0.00013923961041148886,\n",
       "   1.745240655511307e-06,\n",
       "   7,\n",
       "   4,\n",
       "   7,\n",
       "   4,\n",
       "   64,\n",
       "   112,\n",
       "   144,\n",
       "   176,\n",
       "   768,\n",
       "   0.7620668426783267,\n",
       "   0.977514031234138,\n",
       "   64]),\n",
       " (-0.49626358695652173,\n",
       "  [1e-07,\n",
       "   4.407273256559331e-08,\n",
       "   1.2277982028615515e-07,\n",
       "   2.063145545117272e-05,\n",
       "   0.0942499826713531,\n",
       "   2.2578129442394752e-07,\n",
       "   5,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   1024,\n",
       "   0.5747104167729952,\n",
       "   0.6788394165866407,\n",
       "   64]),\n",
       " (-0.490255376344086,\n",
       "  [1e-07,\n",
       "   0.0002707178456494346,\n",
       "   1.1322080026803123e-08,\n",
       "   0.013282683033298567,\n",
       "   0.1,\n",
       "   5.730527884860132e-07,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   6,\n",
       "   48,\n",
       "   144,\n",
       "   224,\n",
       "   160,\n",
       "   512,\n",
       "   0.10253784802269071,\n",
       "   0.7231794730931498,\n",
       "   32]),\n",
       " (-0.4885752688172043,\n",
       "  [0.0014199467314752108,\n",
       "   0.0002985203808851363,\n",
       "   0.009088012885372188,\n",
       "   2.5950300380283493e-07,\n",
       "   2.2975222468857063e-05,\n",
       "   0.033036347942765054,\n",
       "   5,\n",
       "   6,\n",
       "   6,\n",
       "   3,\n",
       "   64,\n",
       "   208,\n",
       "   80,\n",
       "   176,\n",
       "   768,\n",
       "   0.39173489454061877,\n",
       "   0.5807755813295862,\n",
       "   32]),\n",
       " (-0.48029891304347827,\n",
       "  [0.06899900263532611,\n",
       "   2.781930180888892e-06,\n",
       "   4.483831288432357e-05,\n",
       "   0.0009325436794948076,\n",
       "   0.03579306568176825,\n",
       "   1.213977966175106e-06,\n",
       "   7,\n",
       "   5,\n",
       "   4,\n",
       "   6,\n",
       "   208,\n",
       "   176,\n",
       "   64,\n",
       "   192,\n",
       "   3072,\n",
       "   0.7173247575040793,\n",
       "   0.4287773957596433,\n",
       "   64]),\n",
       " (-0.438179347826087,\n",
       "  [0.005008857808128422,\n",
       "   0.002487788471814195,\n",
       "   1.5715422195054192e-08,\n",
       "   0.00044348638492298047,\n",
       "   1e-08,\n",
       "   9.514840735337916e-08,\n",
       "   6,\n",
       "   4,\n",
       "   5,\n",
       "   4,\n",
       "   176,\n",
       "   176,\n",
       "   112,\n",
       "   128,\n",
       "   2560,\n",
       "   0.9671124636939593,\n",
       "   0.49061608698669285,\n",
       "   64])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5178328855285662e-06,\n",
       " 0.0004895539356438342,\n",
       " 0.0020969879904624285,\n",
       " 6.214158600010628e-06,\n",
       " 9.135323619965988e-05,\n",
       " 4.5481429094701505e-07,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 48,\n",
       " 208,\n",
       " 256,\n",
       " 192,\n",
       " 2560,\n",
       " 1e-05,\n",
       " 0.9999,\n",
       " 64]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
