{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rnd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "data_subsets = ['train', 'val']\n",
    "data = {}\n",
    "\n",
    "for name in data_subsets:\n",
    "    with BytesIO() as files:\n",
    "        path = \"omniglot_images/\" +name+ \".pickle\"\n",
    "        s3.Bucket(\"research-paper-omniglot-data\").download_fileobj(path, files)\n",
    "        files.seek(0)    # move back to the beginning after writing\n",
    "        (X,c) = pickle.load(files)\n",
    "        data[name] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = '../../omniglot_images/'\n",
    "data_subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "data = {}\n",
    "categories = {}\n",
    "info = {}\n",
    "        \n",
    "for name in data_subsets:\n",
    "    file_path = os.path.join(path, name + \".pickle\")\n",
    "    print(\"loading data from {}\".format(file_path))\n",
    "    with open(file_path,\"rb\") as f:\n",
    "        (X,c) = pickle.load(f)\n",
    "        data[name] = X\n",
    "        categories[name] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(size, s='train'):\n",
    "    #get train data and shape\n",
    "    X=data[s]\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    #initialize 2 empty arrays for the input size in a list\n",
    "    pairs=[np.zeros((size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    #initialize vector for the targets\n",
    "    targets=np.zeros((size,1))\n",
    "    \n",
    "    for x in range(size):\n",
    "        #randomly sample one class (character)\n",
    "        category = rnd.choice(n_classes,1,replace=False)\n",
    "        #randomly sample one example from class (1-20 characters)\n",
    "        idx_1 = rnd.randint(0, n_examples)\n",
    "        pairs[0][x,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        #randomly sample again one example from class and add last class with modulo\n",
    "        # ..to ensure not same class pairs are created\n",
    "        idx_2 = (idx_1 + rnd.randint(0, n_examples)) % n_examples\n",
    "        #pick images of different class for 1st half and same class for 2nd half\n",
    "        if x >= size // 2:\n",
    "            category_2 = category\n",
    "            targets[x] = 1\n",
    "        else: \n",
    "        #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "        # ..different category\n",
    "            idx_2 = rnd.randint(0, n_examples) \n",
    "            category_2 = (category + rnd.randint(1,n_classes)) % n_classes\n",
    "            targets[x] = 0\n",
    "        pairs[1][x,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "        \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_labels = create_train_data(10000)\n",
    "val_set, val_labels = create_train_data(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-6, high=1, prior='log-uniform', name='learning_rate')\n",
    "dim_reg_layer1 = Real(low=0.0000001, high=0.1, prior='log-uniform', name='reg_layer1')\n",
    "dim_reg_layer2 = Real(low=0.0000001, high=0.1, prior='log-uniform', name='reg_layer2')\n",
    "dim_reg_layer3 = Real(low=0.0000001, high=0.1, prior='log-uniform', name='reg_layer3')\n",
    "dim_reg_layer4 = Real(low=0.0000001, high=0.1, prior='log-uniform', name='reg_layer4')\n",
    "dim_reg_layer5 = Real(low=0.0000001, high=0.1, prior='log-uniform', name='reg_layer5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [dim_learning_rate,\n",
    "             dim_reg_layer1,\n",
    "             dim_reg_layer2,\n",
    "             dim_reg_layer3,\n",
    "             dim_reg_layer4,\n",
    "             dim_reg_layer5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_parameters = [0.00006, 2e-4, 2e-4, 2e-4, 2e-4, 1e-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rnd.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rnd.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(learning_rate, reg_layer1, reg_layer2, reg_layer3, reg_layer4, reg_layer5):\n",
    "    input_shape = (105, 105, 1)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    #build convnet to use in each siamese 'leg'\n",
    "    convnet = Sequential()\n",
    "    convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,\n",
    "                   kernel_initializer=W_init,kernel_regularizer=l2(reg_layer1)))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(128,(7,7),activation='relu',\n",
    "                   kernel_regularizer=l2(reg_layer2),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(reg_layer3),bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(reg_layer4),bias_initializer=b_init))\n",
    "    convnet.add(Flatten())\n",
    "    convnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(reg_layer5),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "\n",
    "    #call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "    encoded_l = convnet(left_input)\n",
    "    encoded_r = convnet(right_input)\n",
    "    #layer to merge two encoded inputs with the l1 distance between them\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    #call this layer on list of two input tensors.\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "\n",
    "    optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "\n",
    "    #siamese_net.count_params()\n",
    "    \n",
    "    return(siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, true_val):\n",
    "    acc_bool = np.equal(np.round_(pred), true_val)\n",
    "    acc = np.mean(acc_bool.astype(int))\n",
    "    \n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just as fast as the Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, reg_layer1, reg_layer2, reg_layer3, reg_layer4, reg_layer5):\n",
    "    #Training loop\n",
    "    siamese_net = create_network(learning_rate, reg_layer1, reg_layer2, reg_layer3, reg_layer4, reg_layer5)\n",
    "    print(\"!\")\n",
    "    batch_size = 128\n",
    "    total_batch = int(10000/batch_size)\n",
    "    total_batch_val = int(3000/batch_size)\n",
    "    epoch = 10\n",
    "\n",
    "    print(\"training\")\n",
    "    for i in range(epoch):\n",
    "        batch_x1, batch_x2, batch_y = shuffle(train_set[0],train_set[1], train_labels, n_samples = batch_size)\n",
    "        train_batch_acc = 0\n",
    "        for j in range(total_batch):\n",
    "            loss=siamese_net.train_on_batch([batch_x1, batch_x2],batch_y)\n",
    "            probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "            train_batch_acc += accuracy(probs, batch_y)\n",
    "            #print('Loss:', loss)\n",
    "            #print('Batch:', j)\n",
    "        train_acc = train_batch_acc/total_batch\n",
    "        val_batch_acc = 0\n",
    "        for validation in range(total_batch_val):\n",
    "            batch_x1, batch_x2, batch_y = shuffle(val_set[0],val_set[1], val_labels, n_samples = batch_size)\n",
    "            probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "            val_batch_acc += accuracy(probs, batch_y)\n",
    "        val_acc = val_batch_acc/total_batch_val\n",
    "        print('Epoch:', i)\n",
    "        print('Train accuracy:', train_acc)\n",
    "        print('Validation accuracy:', val_acc)\n",
    "    \n",
    "    return(-val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "Loss: 4.4984665\n",
      "Batch: 0\n",
      "Loss: 4.403412\n",
      "Batch: 1\n",
      "Loss: 4.330426\n",
      "Batch: 2\n",
      "Loss: 4.24168\n",
      "Batch: 3\n",
      "Loss: 4.1520605\n",
      "Batch: 4\n",
      "Loss: 4.066664\n",
      "Batch: 5\n",
      "Loss: 3.988257\n",
      "Batch: 6\n",
      "Loss: 3.9156978\n",
      "Batch: 7\n",
      "Loss: 3.8498886\n",
      "Batch: 8\n",
      "Loss: 3.791628\n",
      "Batch: 9\n",
      "Loss: 3.7408202\n",
      "Batch: 10\n",
      "Loss: 3.6968129\n",
      "Batch: 11\n",
      "Loss: 3.658761\n",
      "Batch: 12\n",
      "Loss: 3.6255662\n",
      "Batch: 13\n",
      "Loss: 3.5960796\n",
      "Batch: 14\n",
      "Loss: 3.5691466\n",
      "Batch: 15\n",
      "Loss: 3.5438907\n",
      "Batch: 16\n",
      "Loss: 3.5197856\n",
      "Batch: 17\n",
      "Loss: 3.496443\n",
      "Batch: 18\n",
      "Loss: 3.4735928\n",
      "Batch: 19\n",
      "Loss: 3.4510489\n",
      "Batch: 20\n",
      "Loss: 3.4286983\n",
      "Batch: 21\n",
      "Loss: 3.4064739\n",
      "Batch: 22\n",
      "Loss: 3.384332\n",
      "Batch: 23\n",
      "Loss: 3.3622463\n",
      "Batch: 24\n",
      "Loss: 3.3402033\n",
      "Batch: 25\n",
      "Loss: 3.318196\n",
      "Batch: 26\n",
      "Loss: 3.2962224\n",
      "Batch: 27\n",
      "Loss: 3.2742836\n",
      "Batch: 28\n",
      "Loss: 3.252379\n",
      "Batch: 29\n",
      "Loss: 3.2305143\n",
      "Batch: 30\n",
      "Loss: 3.2086942\n",
      "Batch: 31\n",
      "Loss: 3.186924\n",
      "Batch: 32\n",
      "Loss: 3.165208\n",
      "Batch: 33\n",
      "Loss: 3.1435518\n",
      "Batch: 34\n",
      "Loss: 3.121961\n",
      "Batch: 35\n",
      "Loss: 3.10044\n",
      "Batch: 36\n",
      "Loss: 3.078994\n",
      "Batch: 37\n",
      "Loss: 3.0576262\n",
      "Batch: 38\n",
      "Loss: 3.0363412\n",
      "Batch: 39\n",
      "Loss: 3.0151424\n",
      "Batch: 40\n",
      "Loss: 2.9940333\n",
      "Batch: 41\n",
      "Loss: 2.9730182\n",
      "Batch: 42\n",
      "Loss: 2.9520986\n",
      "Batch: 43\n",
      "Loss: 2.931277\n",
      "Batch: 44\n",
      "Loss: 2.9105566\n",
      "Batch: 45\n",
      "Loss: 2.8899386\n",
      "Batch: 46\n",
      "Loss: 2.8694258\n",
      "Batch: 47\n",
      "Loss: 2.8490198\n",
      "Batch: 48\n",
      "Loss: 2.828722\n",
      "Batch: 49\n",
      "Loss: 2.808533\n",
      "Batch: 50\n",
      "Loss: 2.7884555\n",
      "Batch: 51\n",
      "Loss: 2.7684894\n",
      "Batch: 52\n",
      "Loss: 2.7486362\n",
      "Batch: 53\n",
      "Loss: 2.7288964\n",
      "Batch: 54\n",
      "Loss: 2.709271\n",
      "Batch: 55\n",
      "Loss: 2.6897604\n",
      "Batch: 56\n",
      "Loss: 2.6703656\n",
      "Batch: 57\n",
      "Loss: 2.651086\n",
      "Batch: 58\n",
      "Loss: 2.631923\n",
      "Batch: 59\n",
      "Loss: 2.6128764\n",
      "Batch: 60\n",
      "Loss: 2.5939467\n",
      "Batch: 61\n",
      "Loss: 2.5751336\n",
      "Batch: 62\n",
      "Loss: 2.556437\n",
      "Batch: 63\n",
      "Loss: 2.537858\n",
      "Batch: 64\n",
      "Loss: 2.519395\n",
      "Batch: 65\n",
      "Loss: 2.5010495\n",
      "Batch: 66\n",
      "Loss: 2.4828203\n",
      "Batch: 67\n",
      "Loss: 2.464708\n",
      "Batch: 68\n",
      "Loss: 2.4467118\n",
      "Batch: 69\n",
      "Loss: 2.428831\n",
      "Batch: 70\n",
      "Loss: 2.4110668\n",
      "Batch: 71\n",
      "Loss: 2.3934178\n",
      "Batch: 72\n",
      "Loss: 2.375884\n",
      "Batch: 73\n",
      "Loss: 2.3584652\n",
      "Batch: 74\n",
      "Loss: 2.341161\n",
      "Batch: 75\n",
      "Loss: 2.3239706\n",
      "Batch: 76\n",
      "Loss: 2.3068943\n",
      "Batch: 77\n",
      "Train accuracy: 0.9938902243589743\n",
      "Validation accuracy: 0.6769701086956522\n",
      "Loss: 3.0459366\n",
      "Batch: 0\n",
      "Loss: 2.739714\n",
      "Batch: 1\n",
      "Loss: 2.5221207\n",
      "Batch: 2\n",
      "Loss: 2.4402697\n",
      "Batch: 3\n",
      "Loss: 2.9450927\n",
      "Batch: 4\n",
      "Loss: 2.3504941\n",
      "Batch: 5\n",
      "Loss: 2.3620136\n",
      "Batch: 6\n",
      "Loss: 2.3611927\n",
      "Batch: 7\n",
      "Loss: 2.3569481\n",
      "Batch: 8\n",
      "Loss: 2.321332\n",
      "Batch: 9\n",
      "Loss: 2.2981665\n",
      "Batch: 10\n",
      "Loss: 2.2569647\n",
      "Batch: 11\n",
      "Loss: 2.2380733\n",
      "Batch: 12\n",
      "Loss: 2.2247086\n",
      "Batch: 13\n",
      "Loss: 2.2093043\n",
      "Batch: 14\n",
      "Loss: 2.1959233\n",
      "Batch: 15\n",
      "Loss: 2.1854534\n",
      "Batch: 16\n",
      "Loss: 2.1763175\n",
      "Batch: 17\n",
      "Loss: 2.1672475\n",
      "Batch: 18\n",
      "Loss: 2.1579008\n",
      "Batch: 19\n",
      "Loss: 2.1485438\n",
      "Batch: 20\n",
      "Loss: 2.1394808\n",
      "Batch: 21\n",
      "Loss: 2.1308556\n",
      "Batch: 22\n",
      "Loss: 2.1226432\n",
      "Batch: 23\n",
      "Loss: 2.114736\n",
      "Batch: 24\n",
      "Loss: 2.107013\n",
      "Batch: 25\n",
      "Loss: 2.0993977\n",
      "Batch: 26\n",
      "Loss: 2.0918615\n",
      "Batch: 27\n",
      "Loss: 2.084403\n",
      "Batch: 28\n",
      "Loss: 2.0770338\n",
      "Batch: 29\n",
      "Loss: 2.0697644\n",
      "Batch: 30\n",
      "Loss: 2.0625944\n",
      "Batch: 31\n",
      "Loss: 2.0555203\n",
      "Batch: 32\n",
      "Loss: 2.048532\n",
      "Batch: 33\n",
      "Loss: 2.0416203\n",
      "Batch: 34\n",
      "Loss: 2.0347724\n",
      "Batch: 35\n",
      "Loss: 2.027978\n",
      "Batch: 36\n",
      "Loss: 2.0212264\n",
      "Batch: 37\n",
      "Loss: 2.0145106\n",
      "Batch: 38\n",
      "Loss: 2.0078266\n",
      "Batch: 39\n",
      "Loss: 2.0011725\n",
      "Batch: 40\n",
      "Loss: 1.9945476\n",
      "Batch: 41\n",
      "Loss: 1.9879524\n",
      "Batch: 42\n",
      "Loss: 1.9813871\n",
      "Batch: 43\n",
      "Loss: 1.974852\n",
      "Batch: 44\n",
      "Loss: 1.9683465\n",
      "Batch: 45\n",
      "Loss: 1.9618701\n",
      "Batch: 46\n",
      "Loss: 1.955422\n",
      "Batch: 47\n",
      "Loss: 1.9490012\n",
      "Batch: 48\n",
      "Loss: 1.9426072\n",
      "Batch: 49\n",
      "Loss: 1.9362389\n",
      "Batch: 50\n",
      "Loss: 1.9298962\n",
      "Batch: 51\n",
      "Loss: 1.9235791\n",
      "Batch: 52\n",
      "Loss: 1.9172871\n",
      "Batch: 53\n",
      "Loss: 1.9110205\n",
      "Batch: 54\n",
      "Loss: 1.9047788\n",
      "Batch: 55\n",
      "Loss: 1.8985623\n",
      "Batch: 56\n",
      "Loss: 1.8923708\n",
      "Batch: 57\n",
      "Loss: 1.886204\n",
      "Batch: 58\n",
      "Loss: 1.8800616\n",
      "Batch: 59\n",
      "Loss: 1.8739433\n",
      "Batch: 60\n",
      "Loss: 1.8678491\n",
      "Batch: 61\n",
      "Loss: 1.8617784\n",
      "Batch: 62\n",
      "Loss: 1.8557315\n",
      "Batch: 63\n",
      "Loss: 1.8497082\n",
      "Batch: 64\n",
      "Loss: 1.8437086\n",
      "Batch: 65\n",
      "Loss: 1.8377331\n",
      "Batch: 66\n",
      "Loss: 1.8317809\n",
      "Batch: 67\n",
      "Loss: 1.8258522\n",
      "Batch: 68\n",
      "Loss: 1.8199469\n",
      "Batch: 69\n",
      "Loss: 1.8140651\n",
      "Batch: 70\n",
      "Loss: 1.8082062\n",
      "Batch: 71\n",
      "Loss: 1.8023704\n",
      "Batch: 72\n",
      "Loss: 1.7965577\n",
      "Batch: 73\n",
      "Loss: 1.7907677\n",
      "Batch: 74\n",
      "Loss: 1.7850004\n",
      "Batch: 75\n",
      "Loss: 1.779256\n",
      "Batch: 76\n",
      "Loss: 1.7735342\n",
      "Batch: 77\n",
      "Train accuracy: 0.9886818910256411\n",
      "Validation accuracy: 0.6885190217391305\n",
      "Loss: 2.4116068\n",
      "Batch: 0\n",
      "Loss: 2.0978696\n",
      "Batch: 1\n",
      "Loss: 1.9236507\n",
      "Batch: 2\n",
      "Loss: 1.8640379\n",
      "Batch: 3\n",
      "Loss: 1.8124716\n",
      "Batch: 4\n",
      "Loss: 1.8012815\n",
      "Batch: 5\n",
      "Loss: 1.782497\n",
      "Batch: 6\n",
      "Loss: 1.7683357\n",
      "Batch: 7\n",
      "Loss: 1.7590804\n",
      "Batch: 8\n",
      "Loss: 1.7491661\n",
      "Batch: 9\n",
      "Loss: 1.73991\n",
      "Batch: 10\n",
      "Loss: 1.7325654\n",
      "Batch: 11\n",
      "Loss: 1.7264568\n",
      "Batch: 12\n",
      "Loss: 1.7206893\n",
      "Batch: 13\n",
      "Loss: 1.7149061\n",
      "Batch: 14\n",
      "Loss: 1.7092019\n",
      "Batch: 15\n",
      "Loss: 1.7037178\n",
      "Batch: 16\n",
      "Loss: 1.6984788\n",
      "Batch: 17\n",
      "Loss: 1.693422\n",
      "Batch: 18\n",
      "Loss: 1.68846\n",
      "Batch: 19\n",
      "Loss: 1.6835389\n",
      "Batch: 20\n",
      "Loss: 1.6786492\n",
      "Batch: 21\n",
      "Loss: 1.6738027\n",
      "Batch: 22\n",
      "Loss: 1.6690087\n",
      "Batch: 23\n",
      "Loss: 1.6642683\n",
      "Batch: 24\n",
      "Loss: 1.6595743\n",
      "Batch: 25\n",
      "Loss: 1.6549157\n",
      "Batch: 26\n",
      "Loss: 1.6502811\n",
      "Batch: 27\n",
      "Loss: 1.6456624\n",
      "Batch: 28\n",
      "Loss: 1.641055\n",
      "Batch: 29\n",
      "Loss: 1.6364573\n",
      "Batch: 30\n",
      "Loss: 1.6318709\n",
      "Batch: 31\n",
      "Loss: 1.6272967\n",
      "Batch: 32\n",
      "Loss: 1.6227374\n",
      "Batch: 33\n",
      "Loss: 1.618194\n",
      "Batch: 34\n",
      "Loss: 1.6136677\n",
      "Batch: 35\n",
      "Loss: 1.6091578\n",
      "Batch: 36\n",
      "Loss: 1.6046641\n",
      "Batch: 37\n",
      "Loss: 1.6001855\n",
      "Batch: 38\n",
      "Loss: 1.5957208\n",
      "Batch: 39\n",
      "Loss: 1.5912693\n",
      "Batch: 40\n",
      "Loss: 1.5868299\n",
      "Batch: 41\n",
      "Loss: 1.5824023\n",
      "Batch: 42\n",
      "Loss: 1.5779872\n",
      "Batch: 43\n",
      "Loss: 1.5735847\n",
      "Batch: 44\n",
      "Loss: 1.5691955\n",
      "Batch: 45\n",
      "Loss: 1.5648202\n",
      "Batch: 46\n",
      "Loss: 1.5604581\n",
      "Batch: 47\n",
      "Loss: 1.5561103\n",
      "Batch: 48\n",
      "Loss: 1.551776\n",
      "Batch: 49\n",
      "Loss: 1.5474557\n",
      "Batch: 50\n",
      "Loss: 1.5431489\n",
      "Batch: 51\n",
      "Loss: 1.5388556\n",
      "Batch: 52\n",
      "Loss: 1.5345757\n",
      "Batch: 53\n",
      "Loss: 1.5303088\n",
      "Batch: 54\n",
      "Loss: 1.5260559\n",
      "Batch: 55\n",
      "Loss: 1.521816\n",
      "Batch: 56\n",
      "Loss: 1.5175892\n",
      "Batch: 57\n",
      "Loss: 1.5133762\n",
      "Batch: 58\n",
      "Loss: 1.5091763\n",
      "Batch: 59\n",
      "Loss: 1.50499\n",
      "Batch: 60\n",
      "Loss: 1.500817\n",
      "Batch: 61\n",
      "Loss: 1.4966577\n",
      "Batch: 62\n",
      "Loss: 1.4925116\n",
      "Batch: 63\n",
      "Loss: 1.4883791\n",
      "Batch: 64\n",
      "Loss: 1.4842596\n",
      "Batch: 65\n",
      "Loss: 1.4801533\n",
      "Batch: 66\n",
      "Loss: 1.4760605\n",
      "Batch: 67\n",
      "Loss: 1.4719807\n",
      "Batch: 68\n",
      "Loss: 1.4679137\n",
      "Batch: 69\n",
      "Loss: 1.46386\n",
      "Batch: 70\n",
      "Loss: 1.4598191\n",
      "Batch: 71\n",
      "Loss: 1.4557912\n",
      "Batch: 72\n",
      "Loss: 1.4517763\n",
      "Batch: 73\n",
      "Loss: 1.4477742\n",
      "Batch: 74\n",
      "Loss: 1.4437846\n",
      "Batch: 75\n",
      "Loss: 1.4398079\n",
      "Batch: 76\n",
      "Loss: 1.4358443\n",
      "Batch: 77\n",
      "Train accuracy: 0.9982972756410257\n",
      "Validation accuracy: 0.6698369565217391\n",
      "Loss: 2.4583118\n",
      "Batch: 0\n",
      "Loss: 1.9351004\n",
      "Batch: 1\n",
      "Loss: 1.6917096\n",
      "Batch: 2\n",
      "Loss: 1.5674249\n",
      "Batch: 3\n",
      "Loss: 1.5247016\n",
      "Batch: 4\n",
      "Loss: 1.4929019\n",
      "Batch: 5\n",
      "Loss: 1.4727852\n",
      "Batch: 6\n",
      "Loss: 1.4599686\n",
      "Batch: 7\n",
      "Loss: 1.4519944\n",
      "Batch: 8\n",
      "Loss: 1.444861\n",
      "Batch: 9\n",
      "Loss: 1.4370844\n",
      "Batch: 10\n",
      "Loss: 1.4295865\n",
      "Batch: 11\n",
      "Loss: 1.4233198\n",
      "Batch: 12\n",
      "Loss: 1.41837\n",
      "Batch: 13\n",
      "Loss: 1.414305\n",
      "Batch: 14\n",
      "Loss: 1.4105344\n",
      "Batch: 15\n",
      "Loss: 1.4066947\n",
      "Batch: 16\n",
      "Loss: 1.4028361\n",
      "Batch: 17\n",
      "Loss: 1.3991036\n",
      "Batch: 18\n",
      "Loss: 1.3955147\n",
      "Batch: 19\n",
      "Loss: 1.3920304\n",
      "Batch: 20\n",
      "Loss: 1.3886316\n",
      "Batch: 21\n",
      "Loss: 1.3853152\n",
      "Batch: 22\n",
      "Loss: 1.382062\n",
      "Batch: 23\n",
      "Loss: 1.3788428\n",
      "Batch: 24\n",
      "Loss: 1.375637\n",
      "Batch: 25\n",
      "Loss: 1.3724406\n",
      "Batch: 26\n",
      "Loss: 1.3692571\n",
      "Batch: 27\n",
      "Loss: 1.36609\n",
      "Batch: 28\n",
      "Loss: 1.3629397\n",
      "Batch: 29\n",
      "Loss: 1.3598043\n",
      "Batch: 30\n",
      "Loss: 1.3566797\n",
      "Batch: 31\n",
      "Loss: 1.3535639\n",
      "Batch: 32\n",
      "Loss: 1.3504541\n",
      "Batch: 33\n",
      "Loss: 1.3473488\n",
      "Batch: 34\n",
      "Loss: 1.3442473\n",
      "Batch: 35\n",
      "Loss: 1.341149\n",
      "Batch: 36\n",
      "Loss: 1.3380538\n",
      "Batch: 37\n",
      "Loss: 1.3349622\n",
      "Batch: 38\n",
      "Loss: 1.3318743\n",
      "Batch: 39\n",
      "Loss: 1.3287907\n",
      "Batch: 40\n",
      "Loss: 1.3257115\n",
      "Batch: 41\n",
      "Loss: 1.3226379\n",
      "Batch: 42\n",
      "Loss: 1.3195701\n",
      "Batch: 43\n",
      "Loss: 1.316508\n",
      "Batch: 44\n",
      "Loss: 1.3134528\n",
      "Batch: 45\n",
      "Loss: 1.3104043\n",
      "Batch: 46\n",
      "Loss: 1.3073628\n",
      "Batch: 47\n",
      "Loss: 1.3043282\n",
      "Batch: 48\n",
      "Loss: 1.3013008\n",
      "Batch: 49\n",
      "Loss: 1.2982804\n",
      "Batch: 50\n",
      "Loss: 1.2952669\n",
      "Batch: 51\n",
      "Loss: 1.2922606\n",
      "Batch: 52\n",
      "Loss: 1.2892611\n",
      "Batch: 53\n",
      "Loss: 1.2862688\n",
      "Batch: 54\n",
      "Loss: 1.2832835\n",
      "Batch: 55\n",
      "Loss: 1.2803055\n",
      "Batch: 56\n",
      "Loss: 1.2773348\n",
      "Batch: 57\n",
      "Loss: 1.2743714\n",
      "Batch: 58\n",
      "Loss: 1.2714154\n",
      "Batch: 59\n",
      "Loss: 1.2684668\n",
      "Batch: 60\n",
      "Loss: 1.2655255\n",
      "Batch: 61\n",
      "Loss: 1.262592\n",
      "Batch: 62\n",
      "Loss: 1.2596656\n",
      "Batch: 63\n",
      "Loss: 1.2567468\n",
      "Batch: 64\n",
      "Loss: 1.2538352\n",
      "Batch: 65\n",
      "Loss: 1.2509313\n",
      "Batch: 66\n",
      "Loss: 1.2480346\n",
      "Batch: 67\n",
      "Loss: 1.2451456\n",
      "Batch: 68\n",
      "Loss: 1.2422636\n",
      "Batch: 69\n",
      "Loss: 1.2393893\n",
      "Batch: 70\n",
      "Loss: 1.2365218\n",
      "Batch: 71\n",
      "Loss: 1.2336622\n",
      "Batch: 72\n",
      "Loss: 1.2308095\n",
      "Batch: 73\n",
      "Loss: 1.2279643\n",
      "Batch: 74\n",
      "Loss: 1.2251261\n",
      "Batch: 75\n",
      "Loss: 1.2222955\n",
      "Batch: 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2194717\n",
      "Batch: 77\n",
      "Train accuracy: 0.9955929487179487\n",
      "Validation accuracy: 0.681046195652174\n",
      "Loss: 2.047232\n",
      "Batch: 0\n",
      "Loss: 1.6528767\n",
      "Batch: 1\n",
      "Loss: 1.3625965\n",
      "Batch: 2\n",
      "Loss: 1.2733976\n",
      "Batch: 3\n",
      "Loss: 1.2483989\n",
      "Batch: 4\n",
      "Loss: 1.2347552\n",
      "Batch: 5\n",
      "Loss: 1.218029\n",
      "Batch: 6\n",
      "Loss: 1.2065902\n",
      "Batch: 7\n",
      "Loss: 1.198956\n",
      "Batch: 8\n",
      "Loss: 1.192144\n",
      "Batch: 9\n",
      "Loss: 1.1861367\n",
      "Batch: 10\n",
      "Loss: 1.1815007\n",
      "Batch: 11\n",
      "Loss: 1.1777263\n",
      "Batch: 12\n",
      "Loss: 1.1742325\n",
      "Batch: 13\n",
      "Loss: 1.1709801\n",
      "Batch: 14\n",
      "Loss: 1.1680155\n",
      "Batch: 15\n",
      "Loss: 1.1651844\n",
      "Batch: 16\n",
      "Loss: 1.1623319\n",
      "Batch: 17\n",
      "Loss: 1.1594843\n",
      "Batch: 18\n",
      "Loss: 1.1567261\n",
      "Batch: 19\n",
      "Loss: 1.154084\n",
      "Batch: 20\n",
      "Loss: 1.1515338\n",
      "Batch: 21\n",
      "Loss: 1.1490407\n",
      "Batch: 22\n",
      "Loss: 1.146579\n",
      "Batch: 23\n",
      "Loss: 1.144134\n",
      "Batch: 24\n",
      "Loss: 1.1416973\n",
      "Batch: 25\n",
      "Loss: 1.1392659\n",
      "Batch: 26\n",
      "Loss: 1.136837\n",
      "Batch: 27\n",
      "Loss: 1.1344097\n",
      "Batch: 28\n",
      "Loss: 1.1319836\n",
      "Batch: 29\n",
      "Loss: 1.1295587\n",
      "Batch: 30\n",
      "Loss: 1.1271353\n",
      "Batch: 31\n",
      "Loss: 1.1247138\n",
      "Batch: 32\n",
      "Loss: 1.1222942\n",
      "Batch: 33\n",
      "Loss: 1.1198772\n",
      "Batch: 34\n",
      "Loss: 1.1174628\n",
      "Batch: 35\n",
      "Loss: 1.1150512\n",
      "Batch: 36\n",
      "Loss: 1.1126425\n",
      "Batch: 37\n",
      "Loss: 1.1102363\n",
      "Batch: 38\n",
      "Loss: 1.1078328\n",
      "Batch: 39\n",
      "Loss: 1.1054322\n",
      "Batch: 40\n",
      "Loss: 1.1030343\n",
      "Batch: 41\n",
      "Loss: 1.1006393\n",
      "Batch: 42\n",
      "Loss: 1.098248\n",
      "Batch: 43\n",
      "Loss: 1.0958604\n",
      "Batch: 44\n",
      "Loss: 1.093477\n",
      "Batch: 45\n",
      "Loss: 1.0910981\n",
      "Batch: 46\n",
      "Loss: 1.0887237\n",
      "Batch: 47\n",
      "Loss: 1.086354\n",
      "Batch: 48\n",
      "Loss: 1.083989\n",
      "Batch: 49\n",
      "Loss: 1.0816288\n",
      "Batch: 50\n",
      "Loss: 1.079273\n",
      "Batch: 51\n",
      "Loss: 1.076922\n",
      "Batch: 52\n",
      "Loss: 1.0745755\n",
      "Batch: 53\n",
      "Loss: 1.0722342\n",
      "Batch: 54\n",
      "Loss: 1.069897\n",
      "Batch: 55\n",
      "Loss: 1.067565\n",
      "Batch: 56\n",
      "Loss: 1.0652382\n",
      "Batch: 57\n",
      "Loss: 1.0629164\n",
      "Batch: 58\n",
      "Loss: 1.0605997\n",
      "Batch: 59\n",
      "Loss: 1.0582882\n",
      "Batch: 60\n",
      "Loss: 1.0559818\n",
      "Batch: 61\n",
      "Loss: 1.0536807\n",
      "Batch: 62\n",
      "Loss: 1.0513848\n",
      "Batch: 63\n",
      "Loss: 1.0490936\n",
      "Batch: 64\n",
      "Loss: 1.0468079\n",
      "Batch: 65\n",
      "Loss: 1.0445275\n",
      "Batch: 66\n",
      "Loss: 1.0422521\n",
      "Batch: 67\n",
      "Loss: 1.0399818\n",
      "Batch: 68\n",
      "Loss: 1.0377167\n",
      "Batch: 69\n",
      "Loss: 1.0354568\n",
      "Batch: 70\n",
      "Loss: 1.0332019\n",
      "Batch: 71\n",
      "Loss: 1.0309523\n",
      "Batch: 72\n",
      "Loss: 1.0287076\n",
      "Batch: 73\n",
      "Loss: 1.0264683\n",
      "Batch: 74\n",
      "Loss: 1.0242338\n",
      "Batch: 75\n",
      "Loss: 1.0220046\n",
      "Batch: 76\n",
      "Loss: 1.0197803\n",
      "Batch: 77\n",
      "Train accuracy: 0.996895032051282\n",
      "Validation accuracy: 0.7027853260869565\n",
      "Loss: 1.6389763\n",
      "Batch: 0\n",
      "Loss: 1.3091729\n",
      "Batch: 1\n",
      "Loss: 1.1467144\n",
      "Batch: 2\n",
      "Loss: 1.0931153\n",
      "Batch: 3\n",
      "Loss: 1.0689673\n",
      "Batch: 4\n",
      "Loss: 1.0627166\n",
      "Batch: 5\n",
      "Loss: 1.04995\n",
      "Batch: 6\n",
      "Loss: 1.0389872\n",
      "Batch: 7\n",
      "Loss: 1.0329069\n",
      "Batch: 8\n",
      "Loss: 1.0289795\n",
      "Batch: 9\n",
      "Loss: 1.0255647\n",
      "Batch: 10\n",
      "Loss: 1.0222299\n",
      "Batch: 11\n",
      "Loss: 1.0190207\n",
      "Batch: 12\n",
      "Loss: 1.0160377\n",
      "Batch: 13\n",
      "Loss: 1.0133082\n",
      "Batch: 14\n",
      "Loss: 1.010803\n",
      "Batch: 15\n",
      "Loss: 1.0084656\n",
      "Batch: 16\n",
      "Loss: 1.0062324\n",
      "Batch: 17\n",
      "Loss: 1.004049\n",
      "Batch: 18\n",
      "Loss: 1.0018888\n",
      "Batch: 19\n",
      "Loss: 0.99975806\n",
      "Batch: 20\n",
      "Loss: 0.99768215\n",
      "Batch: 21\n",
      "Loss: 0.9956751\n",
      "Batch: 22\n",
      "Loss: 0.9937209\n",
      "Batch: 23\n",
      "Loss: 0.9917778\n",
      "Batch: 24\n",
      "Loss: 0.989817\n",
      "Batch: 25\n",
      "Loss: 0.98784417\n",
      "Batch: 26\n",
      "Loss: 0.9858779\n",
      "Batch: 27\n",
      "Loss: 0.9839246\n",
      "Batch: 28\n",
      "Loss: 0.9819781\n",
      "Batch: 29\n",
      "Loss: 0.98003113\n",
      "Batch: 30\n",
      "Loss: 0.9780824\n",
      "Batch: 31\n",
      "Loss: 0.9761351\n",
      "Batch: 32\n",
      "Loss: 0.974192\n",
      "Batch: 33\n",
      "Loss: 0.97225404\n",
      "Batch: 34\n",
      "Loss: 0.97032\n",
      "Batch: 35\n",
      "Loss: 0.96838796\n",
      "Batch: 36\n",
      "Loss: 0.96645665\n",
      "Batch: 37\n",
      "Loss: 0.96452475\n",
      "Batch: 38\n",
      "Loss: 0.9625922\n",
      "Batch: 39\n",
      "Loss: 0.960659\n",
      "Batch: 40\n",
      "Loss: 0.9587263\n",
      "Batch: 41\n",
      "Loss: 0.9567944\n",
      "Batch: 42\n",
      "Loss: 0.9548641\n",
      "Batch: 43\n",
      "Loss: 0.9529358\n",
      "Batch: 44\n",
      "Loss: 0.9510101\n",
      "Batch: 45\n",
      "Loss: 0.9490872\n",
      "Batch: 46\n",
      "Loss: 0.9471673\n",
      "Batch: 47\n",
      "Loss: 0.94525045\n",
      "Batch: 48\n",
      "Loss: 0.94333667\n",
      "Batch: 49\n",
      "Loss: 0.94142616\n",
      "Batch: 50\n",
      "Loss: 0.939519\n",
      "Batch: 51\n",
      "Loss: 0.93761516\n",
      "Batch: 52\n",
      "Loss: 0.935715\n",
      "Batch: 53\n",
      "Loss: 0.93381834\n",
      "Batch: 54\n",
      "Loss: 0.93192554\n",
      "Batch: 55\n",
      "Loss: 0.9300364\n",
      "Batch: 56\n",
      "Loss: 0.928151\n",
      "Batch: 57\n",
      "Loss: 0.92626953\n",
      "Batch: 58\n",
      "Loss: 0.9243915\n",
      "Batch: 59\n",
      "Loss: 0.9225175\n",
      "Batch: 60\n",
      "Loss: 0.9206473\n",
      "Batch: 61\n",
      "Loss: 0.918781\n",
      "Batch: 62\n",
      "Loss: 0.9169186\n",
      "Batch: 63\n",
      "Loss: 0.91506016\n",
      "Batch: 64\n",
      "Loss: 0.9132057\n",
      "Batch: 65\n",
      "Loss: 0.911355\n",
      "Batch: 66\n",
      "Loss: 0.90950793\n",
      "Batch: 67\n",
      "Loss: 0.9076646\n",
      "Batch: 68\n",
      "Loss: 0.90582526\n",
      "Batch: 69\n",
      "Loss: 0.90398973\n",
      "Batch: 70\n",
      "Loss: 0.90215814\n",
      "Batch: 71\n",
      "Loss: 0.90033036\n",
      "Batch: 72\n",
      "Loss: 0.89850634\n",
      "Batch: 73\n",
      "Loss: 0.89668614\n",
      "Batch: 74\n",
      "Loss: 0.89486974\n",
      "Batch: 75\n",
      "Loss: 0.893057\n",
      "Batch: 76\n",
      "Loss: 0.89124817\n",
      "Batch: 77\n",
      "Train accuracy: 0.9984975961538461\n",
      "Validation accuracy: 0.6793478260869565\n",
      "Loss: 1.4110727\n",
      "Batch: 0\n",
      "Loss: 1.0909864\n",
      "Batch: 1\n",
      "Loss: 0.9459318\n",
      "Batch: 2\n",
      "Loss: 0.9242387\n",
      "Batch: 3\n",
      "Loss: 0.9006247\n",
      "Batch: 4\n",
      "Loss: 0.890997\n",
      "Batch: 5\n",
      "Loss: 0.88526475\n",
      "Batch: 6\n",
      "Loss: 0.8785959\n",
      "Batch: 7\n",
      "Loss: 0.8727845\n",
      "Batch: 8\n",
      "Loss: 0.86855227\n",
      "Batch: 9\n",
      "Loss: 0.865387\n",
      "Batch: 10\n",
      "Loss: 0.8627663\n",
      "Batch: 11\n",
      "Loss: 0.86040413\n",
      "Batch: 12\n",
      "Loss: 0.8581778\n",
      "Batch: 13\n",
      "Loss: 0.85605776\n",
      "Batch: 14\n",
      "Loss: 0.85403776\n",
      "Batch: 15\n",
      "Loss: 0.8521053\n",
      "Batch: 16\n",
      "Loss: 0.85024947\n",
      "Batch: 17\n",
      "Loss: 0.8484558\n",
      "Batch: 18\n",
      "Loss: 0.84670806\n",
      "Batch: 19\n",
      "Loss: 0.8449929\n",
      "Batch: 20\n",
      "Loss: 0.8433\n",
      "Batch: 21\n",
      "Loss: 0.84162116\n",
      "Batch: 22\n",
      "Loss: 0.8399519\n",
      "Batch: 23\n",
      "Loss: 0.83828837\n",
      "Batch: 24\n",
      "Loss: 0.8366289\n",
      "Batch: 25\n",
      "Loss: 0.8349723\n",
      "Batch: 26\n",
      "Loss: 0.83331776\n",
      "Batch: 27\n",
      "Loss: 0.8316651\n",
      "Batch: 28\n",
      "Loss: 0.8300141\n",
      "Batch: 29\n",
      "Loss: 0.8283647\n",
      "Batch: 30\n",
      "Loss: 0.826717\n",
      "Batch: 31\n",
      "Loss: 0.82507116\n",
      "Batch: 32\n",
      "Loss: 0.8234272\n",
      "Batch: 33\n",
      "Loss: 0.82178503\n",
      "Batch: 34\n",
      "Loss: 0.8201449\n",
      "Batch: 35\n",
      "Loss: 0.8185068\n",
      "Batch: 36\n",
      "Loss: 0.816871\n",
      "Batch: 37\n",
      "Loss: 0.8152374\n",
      "Batch: 38\n",
      "Loss: 0.81360614\n",
      "Batch: 39\n",
      "Loss: 0.81197727\n",
      "Batch: 40\n",
      "Loss: 0.81035066\n",
      "Batch: 41\n",
      "Loss: 0.8087264\n",
      "Batch: 42\n",
      "Loss: 0.80710477\n",
      "Batch: 43\n",
      "Loss: 0.8054857\n",
      "Batch: 44\n",
      "Loss: 0.8038692\n",
      "Batch: 45\n",
      "Loss: 0.8022554\n",
      "Batch: 46\n",
      "Loss: 0.80064434\n",
      "Batch: 47\n",
      "Loss: 0.79903597\n",
      "Batch: 48\n",
      "Loss: 0.79743046\n",
      "Batch: 49\n",
      "Loss: 0.7958277\n",
      "Batch: 50\n",
      "Loss: 0.79422784\n",
      "Batch: 51\n",
      "Loss: 0.7926311\n",
      "Batch: 52\n",
      "Loss: 0.79103696\n",
      "Batch: 53\n",
      "Loss: 0.789446\n",
      "Batch: 54\n",
      "Loss: 0.7878579\n",
      "Batch: 55\n",
      "Loss: 0.786273\n",
      "Batch: 56\n",
      "Loss: 0.784691\n",
      "Batch: 57\n",
      "Loss: 0.783112\n",
      "Batch: 58\n",
      "Loss: 0.7815361\n",
      "Batch: 59\n",
      "Loss: 0.7799631\n",
      "Batch: 60\n",
      "Loss: 0.7783933\n",
      "Batch: 61\n",
      "Loss: 0.7768266\n",
      "Batch: 62\n",
      "Loss: 0.77526295\n",
      "Batch: 63\n",
      "Loss: 0.7737023\n",
      "Batch: 64\n",
      "Loss: 0.77214473\n",
      "Batch: 65\n",
      "Loss: 0.7705903\n",
      "Batch: 66\n",
      "Loss: 0.7690388\n",
      "Batch: 67\n",
      "Loss: 0.7674905\n",
      "Batch: 68\n",
      "Loss: 0.7659451\n",
      "Batch: 69\n",
      "Loss: 0.7644027\n",
      "Batch: 70\n",
      "Loss: 0.7628635\n",
      "Batch: 71\n",
      "Loss: 0.76132727\n",
      "Batch: 72\n",
      "Loss: 0.7597941\n",
      "Batch: 73\n",
      "Loss: 0.7582639\n",
      "Batch: 74\n",
      "Loss: 0.7567368\n",
      "Batch: 75\n",
      "Loss: 0.75521266\n",
      "Batch: 76\n",
      "Loss: 0.7536915\n",
      "Batch: 77\n",
      "Train accuracy: 0.999198717948718\n",
      "Validation accuracy: 0.6820652173913043\n",
      "Loss: 1.3868016\n",
      "Batch: 0\n",
      "Loss: 1.0358479\n",
      "Batch: 1\n",
      "Loss: 0.8219839\n",
      "Batch: 2\n",
      "Loss: 0.8144639\n",
      "Batch: 3\n",
      "Loss: 0.7964127\n",
      "Batch: 4\n",
      "Loss: 0.78090435\n",
      "Batch: 5\n",
      "Loss: 0.7715162\n",
      "Batch: 6\n",
      "Loss: 0.76583576\n",
      "Batch: 7\n",
      "Loss: 0.76113707\n",
      "Batch: 8\n",
      "Loss: 0.7564961\n",
      "Batch: 9\n",
      "Loss: 0.7524457\n",
      "Batch: 10\n",
      "Loss: 0.749241\n",
      "Batch: 11\n",
      "Loss: 0.746696\n",
      "Batch: 12\n",
      "Loss: 0.74451256\n",
      "Batch: 13\n",
      "Loss: 0.74247783\n",
      "Batch: 14\n",
      "Loss: 0.7405241\n",
      "Batch: 15\n",
      "Loss: 0.73869\n",
      "Batch: 16\n",
      "Loss: 0.73703057\n",
      "Batch: 17\n",
      "Loss: 0.7355357\n",
      "Batch: 18\n",
      "Loss: 0.7341207\n",
      "Batch: 19\n",
      "Loss: 0.73270786\n",
      "Batch: 20\n",
      "Loss: 0.73128664\n",
      "Batch: 21\n",
      "Loss: 0.7298837\n",
      "Batch: 22\n",
      "Loss: 0.72850144\n",
      "Batch: 23\n",
      "Loss: 0.72712314\n",
      "Batch: 24\n",
      "Loss: 0.72574264\n",
      "Batch: 25\n",
      "Loss: 0.72436726\n",
      "Batch: 26\n",
      "Loss: 0.7230025\n",
      "Batch: 27\n",
      "Loss: 0.7216453\n",
      "Batch: 28\n",
      "Loss: 0.72029066\n",
      "Batch: 29\n",
      "Loss: 0.71893543\n",
      "Batch: 30\n",
      "Loss: 0.71757877\n",
      "Batch: 31\n",
      "Loss: 0.71622086\n",
      "Batch: 32\n",
      "Loss: 0.714862\n",
      "Batch: 33\n",
      "Loss: 0.7135021\n",
      "Batch: 34\n",
      "Loss: 0.7121406\n",
      "Batch: 35\n",
      "Loss: 0.71077716\n",
      "Batch: 36\n",
      "Loss: 0.7094109\n",
      "Batch: 37\n",
      "Loss: 0.70804167\n",
      "Batch: 38\n",
      "Loss: 0.70666975\n",
      "Batch: 39\n",
      "Loss: 0.70529586\n",
      "Batch: 40\n",
      "Loss: 0.7039206\n",
      "Batch: 41\n",
      "Loss: 0.70254457\n",
      "Batch: 42\n",
      "Loss: 0.7011685\n",
      "Batch: 43\n",
      "Loss: 0.6997928\n",
      "Batch: 44\n",
      "Loss: 0.698418\n",
      "Batch: 45\n",
      "Loss: 0.6970443\n",
      "Batch: 46\n",
      "Loss: 0.6956722\n",
      "Batch: 47\n",
      "Loss: 0.6943015\n",
      "Batch: 48\n",
      "Loss: 0.69293237\n",
      "Batch: 49\n",
      "Loss: 0.69156504\n",
      "Batch: 50\n",
      "Loss: 0.69019955\n",
      "Batch: 51\n",
      "Loss: 0.6888358\n",
      "Batch: 52\n",
      "Loss: 0.68747413\n",
      "Batch: 53\n",
      "Loss: 0.6861147\n",
      "Batch: 54\n",
      "Loss: 0.68475765\n",
      "Batch: 55\n",
      "Loss: 0.68340284\n",
      "Batch: 56\n",
      "Loss: 0.6820505\n",
      "Batch: 57\n",
      "Loss: 0.6807009\n",
      "Batch: 58\n",
      "Loss: 0.67935383\n",
      "Batch: 59\n",
      "Loss: 0.67800945\n",
      "Batch: 60\n",
      "Loss: 0.6766679\n",
      "Batch: 61\n",
      "Loss: 0.67532897\n",
      "Batch: 62\n",
      "Loss: 0.6739928\n",
      "Batch: 63\n",
      "Loss: 0.6726593\n",
      "Batch: 64\n",
      "Loss: 0.67132837\n",
      "Batch: 65\n",
      "Loss: 0.67000026\n",
      "Batch: 66\n",
      "Loss: 0.66867477\n",
      "Batch: 67\n",
      "Loss: 0.667352\n",
      "Batch: 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6660321\n",
      "Batch: 69\n",
      "Loss: 0.66471463\n",
      "Batch: 70\n",
      "Loss: 0.66340005\n",
      "Batch: 71\n",
      "Loss: 0.66208804\n",
      "Batch: 72\n",
      "Loss: 0.6607786\n",
      "Batch: 73\n",
      "Loss: 0.6594721\n",
      "Batch: 74\n",
      "Loss: 0.65816814\n",
      "Batch: 75\n",
      "Loss: 0.6568668\n",
      "Batch: 76\n",
      "Loss: 0.6555682\n",
      "Batch: 77\n",
      "Train accuracy: 0.9983974358974359\n",
      "Validation accuracy: 0.7088994565217391\n",
      "Loss: 1.185165\n",
      "Batch: 0\n",
      "Loss: 0.92216676\n",
      "Batch: 1\n",
      "Loss: 0.74742746\n",
      "Batch: 2\n",
      "Loss: 0.7317742\n",
      "Batch: 3\n",
      "Loss: 0.7031685\n",
      "Batch: 4\n",
      "Loss: 0.68872476\n",
      "Batch: 5\n",
      "Loss: 0.68291694\n",
      "Batch: 6\n",
      "Loss: 0.6806782\n",
      "Batch: 7\n",
      "Loss: 0.6785071\n",
      "Batch: 8\n",
      "Loss: 0.674973\n",
      "Batch: 9\n",
      "Loss: 0.67100304\n",
      "Batch: 10\n",
      "Loss: 0.6676639\n",
      "Batch: 11\n",
      "Loss: 0.665144\n",
      "Batch: 12\n",
      "Loss: 0.66321176\n",
      "Batch: 13\n",
      "Loss: 0.6616208\n",
      "Batch: 14\n",
      "Loss: 0.6601999\n",
      "Batch: 15\n",
      "Loss: 0.6588511\n",
      "Batch: 16\n",
      "Loss: 0.657523\n",
      "Batch: 17\n",
      "Loss: 0.6561891\n",
      "Batch: 18\n",
      "Loss: 0.6548443\n",
      "Batch: 19\n",
      "Loss: 0.6534978\n",
      "Batch: 20\n",
      "Loss: 0.65216553\n",
      "Batch: 21\n",
      "Loss: 0.6508656\n",
      "Batch: 22\n",
      "Loss: 0.6496026\n",
      "Batch: 23\n",
      "Loss: 0.6483559\n",
      "Batch: 24\n",
      "Loss: 0.6471102\n",
      "Batch: 25\n",
      "Loss: 0.6458692\n",
      "Batch: 26\n",
      "Loss: 0.64464\n",
      "Batch: 27\n",
      "Loss: 0.6434247\n",
      "Batch: 28\n",
      "Loss: 0.64222014\n",
      "Batch: 29\n",
      "Loss: 0.64102155\n",
      "Batch: 30\n",
      "Loss: 0.6398255\n",
      "Batch: 31\n",
      "Loss: 0.63862973\n",
      "Batch: 32\n",
      "Loss: 0.6374339\n",
      "Batch: 33\n",
      "Loss: 0.63623816\n",
      "Batch: 34\n",
      "Loss: 0.6350436\n",
      "Batch: 35\n",
      "Loss: 0.6338509\n",
      "Batch: 36\n",
      "Loss: 0.63266057\n",
      "Batch: 37\n",
      "Loss: 0.6314725\n",
      "Batch: 38\n",
      "Loss: 0.63028574\n",
      "Batch: 39\n",
      "Loss: 0.62909985\n",
      "Batch: 40\n",
      "Loss: 0.62791455\n",
      "Batch: 41\n",
      "Loss: 0.62672985\n",
      "Batch: 42\n",
      "Loss: 0.62554675\n",
      "Batch: 43\n",
      "Loss: 0.62436545\n",
      "Batch: 44\n",
      "Loss: 0.62318647\n",
      "Batch: 45\n",
      "Loss: 0.62200963\n",
      "Batch: 46\n",
      "Loss: 0.62083495\n",
      "Batch: 47\n",
      "Loss: 0.6196621\n",
      "Batch: 48\n",
      "Loss: 0.61849123\n",
      "Batch: 49\n",
      "Loss: 0.6173219\n",
      "Batch: 50\n",
      "Loss: 0.61615443\n",
      "Batch: 51\n",
      "Loss: 0.61498904\n",
      "Batch: 52\n",
      "Loss: 0.61382556\n",
      "Batch: 53\n",
      "Loss: 0.6126644\n",
      "Batch: 54\n",
      "Loss: 0.61150557\n",
      "Batch: 55\n",
      "Loss: 0.61034894\n",
      "Batch: 56\n",
      "Loss: 0.60919446\n",
      "Batch: 57\n",
      "Loss: 0.6080422\n",
      "Batch: 58\n",
      "Loss: 0.60689217\n",
      "Batch: 59\n",
      "Loss: 0.6057441\n",
      "Batch: 60\n",
      "Loss: 0.60459834\n",
      "Batch: 61\n",
      "Loss: 0.6034547\n",
      "Batch: 62\n",
      "Loss: 0.60231334\n",
      "Batch: 63\n",
      "Loss: 0.60117424\n",
      "Batch: 64\n",
      "Loss: 0.60003734\n",
      "Batch: 65\n",
      "Loss: 0.5989028\n",
      "Batch: 66\n",
      "Loss: 0.5977704\n",
      "Batch: 67\n",
      "Loss: 0.59664017\n",
      "Batch: 68\n",
      "Loss: 0.5955123\n",
      "Batch: 69\n",
      "Loss: 0.5943864\n",
      "Batch: 70\n",
      "Loss: 0.59326273\n",
      "Batch: 71\n",
      "Loss: 0.59214133\n",
      "Batch: 72\n",
      "Loss: 0.591022\n",
      "Batch: 73\n",
      "Loss: 0.589905\n",
      "Batch: 74\n",
      "Loss: 0.5887903\n",
      "Batch: 75\n",
      "Loss: 0.5876776\n",
      "Batch: 76\n",
      "Loss: 0.5865672\n",
      "Batch: 77\n",
      "Train accuracy: 0.9988982371794872\n",
      "Validation accuracy: 0.7068614130434783\n",
      "Loss: 1.3137329\n",
      "Batch: 0\n",
      "Loss: 0.8412391\n",
      "Batch: 1\n",
      "Loss: 0.68440145\n",
      "Batch: 2\n",
      "Loss: 0.6211965\n",
      "Batch: 3\n",
      "Loss: 0.6081442\n",
      "Batch: 4\n",
      "Loss: 0.60226697\n",
      "Batch: 5\n",
      "Loss: 0.59443396\n",
      "Batch: 6\n",
      "Loss: 0.58770174\n",
      "Batch: 7\n",
      "Loss: 0.5832158\n",
      "Batch: 8\n",
      "Loss: 0.58011526\n",
      "Batch: 9\n",
      "Loss: 0.5774893\n",
      "Batch: 10\n",
      "Loss: 0.5752469\n",
      "Batch: 11\n",
      "Loss: 0.5734676\n",
      "Batch: 12\n",
      "Loss: 0.5719652\n",
      "Batch: 13\n",
      "Loss: 0.5705218\n",
      "Batch: 14\n",
      "Loss: 0.56911635\n",
      "Batch: 15\n",
      "Loss: 0.56782603\n",
      "Batch: 16\n",
      "Loss: 0.56666505\n",
      "Batch: 17\n",
      "Loss: 0.56557924\n",
      "Batch: 18\n",
      "Loss: 0.5645136\n",
      "Batch: 19\n",
      "Loss: 0.56345206\n",
      "Batch: 20\n",
      "Loss: 0.56240505\n",
      "Batch: 21\n",
      "Loss: 0.56138355\n",
      "Batch: 22\n",
      "Loss: 0.5603892\n",
      "Batch: 23\n",
      "Loss: 0.55941635\n",
      "Batch: 24\n",
      "Loss: 0.5584558\n",
      "Batch: 25\n",
      "Loss: 0.55749923\n",
      "Batch: 26\n",
      "Loss: 0.55653954\n",
      "Batch: 27\n",
      "Loss: 0.555572\n",
      "Batch: 28\n",
      "Loss: 0.554595\n",
      "Batch: 29\n",
      "Loss: 0.5536081\n",
      "Batch: 30\n",
      "Loss: 0.552613\n",
      "Batch: 31\n",
      "Loss: 0.5516114\n",
      "Batch: 32\n",
      "Loss: 0.55060554\n",
      "Batch: 33\n",
      "Loss: 0.54959667\n",
      "Batch: 34\n",
      "Loss: 0.548586\n",
      "Batch: 35\n",
      "Loss: 0.54757386\n",
      "Batch: 36\n",
      "Loss: 0.5465605\n",
      "Batch: 37\n",
      "Loss: 0.5455459\n",
      "Batch: 38\n",
      "Loss: 0.5445298\n",
      "Batch: 39\n",
      "Loss: 0.5435124\n",
      "Batch: 40\n",
      "Loss: 0.54249394\n",
      "Batch: 41\n",
      "Loss: 0.5414747\n",
      "Batch: 42\n",
      "Loss: 0.54045504\n",
      "Batch: 43\n",
      "Loss: 0.5394353\n",
      "Batch: 44\n",
      "Loss: 0.5384159\n",
      "Batch: 45\n",
      "Loss: 0.5373973\n",
      "Batch: 46\n",
      "Loss: 0.53637964\n",
      "Batch: 47\n",
      "Loss: 0.53536314\n",
      "Batch: 48\n",
      "Loss: 0.53434795\n",
      "Batch: 49\n",
      "Loss: 0.53333426\n",
      "Batch: 50\n",
      "Loss: 0.53232193\n",
      "Batch: 51\n",
      "Loss: 0.531311\n",
      "Batch: 52\n",
      "Loss: 0.53030163\n",
      "Batch: 53\n",
      "Loss: 0.52929384\n",
      "Batch: 54\n",
      "Loss: 0.5282874\n",
      "Batch: 55\n",
      "Loss: 0.5272827\n",
      "Batch: 56\n",
      "Loss: 0.5262796\n",
      "Batch: 57\n",
      "Loss: 0.5252782\n",
      "Batch: 58\n",
      "Loss: 0.5242784\n",
      "Batch: 59\n",
      "Loss: 0.5232806\n",
      "Batch: 60\n",
      "Loss: 0.5222845\n",
      "Batch: 61\n",
      "Loss: 0.5212903\n",
      "Batch: 62\n",
      "Loss: 0.52029806\n",
      "Batch: 63\n",
      "Loss: 0.5193077\n",
      "Batch: 64\n",
      "Loss: 0.51831925\n",
      "Batch: 65\n",
      "Loss: 0.5173329\n",
      "Batch: 66\n",
      "Loss: 0.5163484\n",
      "Batch: 67\n",
      "Loss: 0.515366\n",
      "Batch: 68\n",
      "Loss: 0.5143854\n",
      "Batch: 69\n",
      "Loss: 0.5134068\n",
      "Batch: 70\n",
      "Loss: 0.51243013\n",
      "Batch: 71\n",
      "Loss: 0.5114554\n",
      "Batch: 72\n",
      "Loss: 0.5104826\n",
      "Batch: 73\n",
      "Loss: 0.50951177\n",
      "Batch: 74\n",
      "Loss: 0.50854284\n",
      "Batch: 75\n",
      "Loss: 0.50757575\n",
      "Batch: 76\n",
      "Loss: 0.5066107\n",
      "Batch: 77\n",
      "Train accuracy: 0.9987980769230769\n",
      "Validation accuracy: 0.7238451086956522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.7238451086956522"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9866786858974359\n",
      "Validation accuracy: 0.6878396739130435\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9728565705128205\n",
      "Validation accuracy: 0.6504755434782609\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9975961538461539\n",
      "Validation accuracy: 0.6908967391304348\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9963942307692307\n",
      "Validation accuracy: 0.6885190217391305\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9982972756410257\n",
      "Validation accuracy: 0.6830842391304348\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9985977564102564\n",
      "Validation accuracy: 0.6793478260869565\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9978966346153846\n",
      "Validation accuracy: 0.6875\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9993990384615384\n",
      "Validation accuracy: 0.7021059782608695\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9980969551282052\n",
      "Validation accuracy: 0.6925951086956522\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9995993589743589\n",
      "Validation accuracy: 0.6854619565217391\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9653445512820513\n",
      "Validation accuracy: 0.6851222826086957\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9838741987179487\n",
      "Validation accuracy: 0.686141304347826\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9891826923076923\n",
      "Validation accuracy: 0.7150135869565217\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9852764423076923\n",
      "Validation accuracy: 0.7024456521739131\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9859775641025641\n",
      "Validation accuracy: 0.7432065217391305\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9901842948717948\n",
      "Validation accuracy: 0.751358695652174\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9933894230769231\n",
      "Validation accuracy: 0.7737771739130435\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9915865384615384\n",
      "Validation accuracy: 0.75\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9891826923076923\n",
      "Validation accuracy: 0.7503396739130435\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9907852564102564\n",
      "Validation accuracy: 0.7578125\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.6586538461538461\n",
      "Validation accuracy: 0.5475543478260869\n",
      "Epoch: 1\n",
      "Train accuracy: 0.5991586538461539\n",
      "Validation accuracy: 0.532608695652174\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5940504807692307\n",
      "Validation accuracy: 0.5777853260869565\n",
      "Epoch: 3\n",
      "Train accuracy: 0.5864383012820513\n",
      "Validation accuracy: 0.600203804347826\n",
      "Epoch: 4\n",
      "Train accuracy: 0.559395032051282\n",
      "Validation accuracy: 0.6246603260869565\n",
      "Epoch: 5\n",
      "Train accuracy: 0.6192908653846154\n",
      "Validation accuracy: 0.5838994565217391\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5838341346153846\n",
      "Validation accuracy: 0.5529891304347826\n",
      "Epoch: 7\n",
      "Train accuracy: 0.6376201923076923\n",
      "Validation accuracy: 0.5587635869565217\n",
      "Epoch: 8\n",
      "Train accuracy: 0.6188902243589743\n",
      "Validation accuracy: 0.623641304347826\n",
      "Epoch: 9\n",
      "Train accuracy: 0.6713741987179487\n",
      "Validation accuracy: 0.5798233695652174\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.6049679487179487\n",
      "Validation accuracy: 0.5512907608695652\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,64,96,96] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: sequential_9/conv2d_33/convolution = Conv2D[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](sequential_9/conv2d_33/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_33/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: sequential_9/flatten_9/Shape/_1837 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_170_sequential_9/flatten_9/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d34b5452d3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0macq_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Expected Improvement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                             x0=default_parameters)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/skopt/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;31m# Call the wrapped objective function with the named arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             \u001b[0mobjective_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-197abc6f3b12>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(learning_rate, reg_layer1, reg_layer2, reg_layer3, reg_layer4, reg_layer5)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_batch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtrain_batch_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,64,96,96] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: sequential_9/conv2d_33/convolution = Conv2D[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](sequential_9/conv2d_33/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_33/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: sequential_9/flatten_9/Shape/_1837 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_170_sequential_9/flatten_9/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
