{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdeblaauw/anaconda3/envs/research-paper/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "data_subsets = ['train', 'val']\n",
    "data = {}\n",
    "\n",
    "for name in data_subsets:\n",
    "    with BytesIO() as files:\n",
    "        path = \"omniglot_images/\" +name+ \".pickle\"\n",
    "        s3.Bucket(\"research-paper-omniglot-data\").download_fileobj(path, files)\n",
    "        files.seek(0)    # move back to the beginning after writing\n",
    "        (X,c) = pickle.load(files)\n",
    "        data[name] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = '../../omniglot_images/'\n",
    "data_subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "data = {}\n",
    "categories = {}\n",
    "info = {}\n",
    "        \n",
    "for name in data_subsets:\n",
    "    file_path = os.path.join(path, name + \".pickle\")\n",
    "    print(\"loading data from {}\".format(file_path))\n",
    "    with open(file_path,\"rb\") as f:\n",
    "        (X,c) = pickle.load(f)\n",
    "        data[name] = X\n",
    "        categories[name] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_data(size, s='train'):\n",
    "    #get train data and shape\n",
    "    X=data[s]\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    #initialize 2 empty arrays for the input size in a list\n",
    "    pairs=[np.zeros((size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    #initialize vector for the targets\n",
    "    targets=np.zeros((size,1))\n",
    "    \n",
    "    for x in range(size):\n",
    "        #randomly sample one class (character)\n",
    "        category = rnd.choice(n_classes,1,replace=False)\n",
    "        #randomly sample one example from class (1-20 characters)\n",
    "        idx_1 = rnd.randint(0, n_examples)\n",
    "        pairs[0][x,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        #randomly sample again one example from class and add last class with modulo\n",
    "        # ..to ensure not same class pairs are created\n",
    "        idx_2 = (idx_1 + rnd.randint(0, n_examples)) % n_examples\n",
    "        #pick images of different class for 1st half and same class for 2nd half\n",
    "        if x >= size // 2:\n",
    "            category_2 = category\n",
    "            targets[x] = 1\n",
    "        else: \n",
    "        #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "        # ..different category\n",
    "            idx_2 = rnd.randint(0, n_examples) \n",
    "            category_2 = (category + rnd.randint(1,n_classes)) % n_classes\n",
    "            targets[x] = 0\n",
    "        pairs[1][x,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "        \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, train_labels = create_train_data(10000)\n",
    "val_set, val_labels = create_train_data(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-6, high=1, prior='log-uniform', name='learning_rate')\n",
    "dim_reg_layer1 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer1')\n",
    "dim_reg_layer2 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer2')\n",
    "dim_reg_layer3 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer3')\n",
    "dim_reg_layer4 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer4')\n",
    "dim_reg_layer5 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = [dim_learning_rate,\n",
    "             dim_reg_layer1,\n",
    "             dim_reg_layer2,\n",
    "             dim_reg_layer3,\n",
    "             dim_reg_layer4,\n",
    "             dim_reg_layer5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_parameters = [0.00006, 2e-4, 2e-4, 2e-4, 2e-4, 1e-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rnd.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rnd.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(learning_rate, reg_layer1, reg_layer2, reg_layer3, reg_layer4, reg_layer5):\n",
    "    input_shape = (105, 105, 1)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    #build convnet to use in each siamese 'leg'\n",
    "    convnet = Sequential()\n",
    "    convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,kernel_initializer=W_init,kernel_regularizer=l2(reg_layer1)))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(128,(7,7),activation='relu',kernel_regularizer=l2(reg_layer2),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(reg_layer3),bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(reg_layer4),bias_initializer=b_init))\n",
    "    convnet.add(Flatten())\n",
    "    convnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(reg_layer5),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    #call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "    encoded_l = convnet(left_input)\n",
    "    encoded_r = convnet(right_input)\n",
    "    #layer to merge two encoded inputs with the l1 distance between them\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    #call this layer on list of two input tensors.\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "    #siamese_net.count_params()\n",
    "    return(siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, true_val):\n",
    "    acc_bool = np.equal(np.round_(pred), true_val)\n",
    "    acc = np.mean(acc_bool.astype(int))\n",
    "    \n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just as fast as the Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "###################################\n",
    "\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, reg_layer1, reg_layer2, reg_layer3, reg_layer4, reg_layer5):\n",
    "    #Training loop\n",
    "    siamese_net = create_network(learning_rate, reg_layer1, reg_layer2, reg_layer3, reg_layer4, reg_layer5)\n",
    "        \n",
    "    print(\"!\")\n",
    "    batch_size = 128\n",
    "    total_batch = int(10000/batch_size)\n",
    "    total_batch_val = int(3000/batch_size)\n",
    "    epoch = 2\n",
    "\n",
    "    print(\"training\")\n",
    "    for i in range(epoch):\n",
    "        batch_x1, batch_x2, batch_y = shuffle(train_set[0],train_set[1], train_labels, n_samples = batch_size)\n",
    "        train_batch_acc = 0\n",
    "        for j in range(total_batch):\n",
    "            loss=siamese_net.train_on_batch([batch_x1, batch_x2],batch_y)\n",
    "            probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "            train_batch_acc += accuracy(probs, batch_y)\n",
    "            #print('Loss:', loss)\n",
    "            #print('Batch:', j)\n",
    "        train_acc = train_batch_acc/total_batch\n",
    "        val_batch_acc = 0\n",
    "        for validation in range(total_batch_val):\n",
    "            batch_x1, batch_x2, batch_y = shuffle(val_set[0],val_set[1], val_labels, n_samples = batch_size)\n",
    "            probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "            val_batch_acc += accuracy(probs, batch_y)\n",
    "        val_acc = val_batch_acc/total_batch_val\n",
    "        print('Epoch:', i)\n",
    "        print('Train accuracy:', train_acc)\n",
    "        print('Validation accuracy:', val_acc)\n",
    "        \n",
    "    return(-val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "Loss: 4.4984665\n",
      "Batch: 0\n",
      "Loss: 4.403412\n",
      "Batch: 1\n",
      "Loss: 4.330426\n",
      "Batch: 2\n",
      "Loss: 4.24168\n",
      "Batch: 3\n",
      "Loss: 4.1520605\n",
      "Batch: 4\n",
      "Loss: 4.066664\n",
      "Batch: 5\n",
      "Loss: 3.988257\n",
      "Batch: 6\n",
      "Loss: 3.9156978\n",
      "Batch: 7\n",
      "Loss: 3.8498886\n",
      "Batch: 8\n",
      "Loss: 3.791628\n",
      "Batch: 9\n",
      "Loss: 3.7408202\n",
      "Batch: 10\n",
      "Loss: 3.6968129\n",
      "Batch: 11\n",
      "Loss: 3.658761\n",
      "Batch: 12\n",
      "Loss: 3.6255662\n",
      "Batch: 13\n",
      "Loss: 3.5960796\n",
      "Batch: 14\n",
      "Loss: 3.5691466\n",
      "Batch: 15\n",
      "Loss: 3.5438907\n",
      "Batch: 16\n",
      "Loss: 3.5197856\n",
      "Batch: 17\n",
      "Loss: 3.496443\n",
      "Batch: 18\n",
      "Loss: 3.4735928\n",
      "Batch: 19\n",
      "Loss: 3.4510489\n",
      "Batch: 20\n",
      "Loss: 3.4286983\n",
      "Batch: 21\n",
      "Loss: 3.4064739\n",
      "Batch: 22\n",
      "Loss: 3.384332\n",
      "Batch: 23\n",
      "Loss: 3.3622463\n",
      "Batch: 24\n",
      "Loss: 3.3402033\n",
      "Batch: 25\n",
      "Loss: 3.318196\n",
      "Batch: 26\n",
      "Loss: 3.2962224\n",
      "Batch: 27\n",
      "Loss: 3.2742836\n",
      "Batch: 28\n",
      "Loss: 3.252379\n",
      "Batch: 29\n",
      "Loss: 3.2305143\n",
      "Batch: 30\n",
      "Loss: 3.2086942\n",
      "Batch: 31\n",
      "Loss: 3.186924\n",
      "Batch: 32\n",
      "Loss: 3.165208\n",
      "Batch: 33\n",
      "Loss: 3.1435518\n",
      "Batch: 34\n",
      "Loss: 3.121961\n",
      "Batch: 35\n",
      "Loss: 3.10044\n",
      "Batch: 36\n",
      "Loss: 3.078994\n",
      "Batch: 37\n",
      "Loss: 3.0576262\n",
      "Batch: 38\n",
      "Loss: 3.0363412\n",
      "Batch: 39\n",
      "Loss: 3.0151424\n",
      "Batch: 40\n",
      "Loss: 2.9940333\n",
      "Batch: 41\n",
      "Loss: 2.9730182\n",
      "Batch: 42\n",
      "Loss: 2.9520986\n",
      "Batch: 43\n",
      "Loss: 2.931277\n",
      "Batch: 44\n",
      "Loss: 2.9105566\n",
      "Batch: 45\n",
      "Loss: 2.8899386\n",
      "Batch: 46\n",
      "Loss: 2.8694258\n",
      "Batch: 47\n",
      "Loss: 2.8490198\n",
      "Batch: 48\n",
      "Loss: 2.828722\n",
      "Batch: 49\n",
      "Loss: 2.808533\n",
      "Batch: 50\n",
      "Loss: 2.7884555\n",
      "Batch: 51\n",
      "Loss: 2.7684894\n",
      "Batch: 52\n",
      "Loss: 2.7486362\n",
      "Batch: 53\n",
      "Loss: 2.7288964\n",
      "Batch: 54\n",
      "Loss: 2.709271\n",
      "Batch: 55\n",
      "Loss: 2.6897604\n",
      "Batch: 56\n",
      "Loss: 2.6703656\n",
      "Batch: 57\n",
      "Loss: 2.651086\n",
      "Batch: 58\n",
      "Loss: 2.631923\n",
      "Batch: 59\n",
      "Loss: 2.6128764\n",
      "Batch: 60\n",
      "Loss: 2.5939467\n",
      "Batch: 61\n",
      "Loss: 2.5751336\n",
      "Batch: 62\n",
      "Loss: 2.556437\n",
      "Batch: 63\n",
      "Loss: 2.537858\n",
      "Batch: 64\n",
      "Loss: 2.519395\n",
      "Batch: 65\n",
      "Loss: 2.5010495\n",
      "Batch: 66\n",
      "Loss: 2.4828203\n",
      "Batch: 67\n",
      "Loss: 2.464708\n",
      "Batch: 68\n",
      "Loss: 2.4467118\n",
      "Batch: 69\n",
      "Loss: 2.428831\n",
      "Batch: 70\n",
      "Loss: 2.4110668\n",
      "Batch: 71\n",
      "Loss: 2.3934178\n",
      "Batch: 72\n",
      "Loss: 2.375884\n",
      "Batch: 73\n",
      "Loss: 2.3584652\n",
      "Batch: 74\n",
      "Loss: 2.341161\n",
      "Batch: 75\n",
      "Loss: 2.3239706\n",
      "Batch: 76\n",
      "Loss: 2.3068943\n",
      "Batch: 77\n",
      "Train accuracy: 0.9938902243589743\n",
      "Validation accuracy: 0.6769701086956522\n",
      "Loss: 3.0459366\n",
      "Batch: 0\n",
      "Loss: 2.739714\n",
      "Batch: 1\n",
      "Loss: 2.5221207\n",
      "Batch: 2\n",
      "Loss: 2.4402697\n",
      "Batch: 3\n",
      "Loss: 2.9450927\n",
      "Batch: 4\n",
      "Loss: 2.3504941\n",
      "Batch: 5\n",
      "Loss: 2.3620136\n",
      "Batch: 6\n",
      "Loss: 2.3611927\n",
      "Batch: 7\n",
      "Loss: 2.3569481\n",
      "Batch: 8\n",
      "Loss: 2.321332\n",
      "Batch: 9\n",
      "Loss: 2.2981665\n",
      "Batch: 10\n",
      "Loss: 2.2569647\n",
      "Batch: 11\n",
      "Loss: 2.2380733\n",
      "Batch: 12\n",
      "Loss: 2.2247086\n",
      "Batch: 13\n",
      "Loss: 2.2093043\n",
      "Batch: 14\n",
      "Loss: 2.1959233\n",
      "Batch: 15\n",
      "Loss: 2.1854534\n",
      "Batch: 16\n",
      "Loss: 2.1763175\n",
      "Batch: 17\n",
      "Loss: 2.1672475\n",
      "Batch: 18\n",
      "Loss: 2.1579008\n",
      "Batch: 19\n",
      "Loss: 2.1485438\n",
      "Batch: 20\n",
      "Loss: 2.1394808\n",
      "Batch: 21\n",
      "Loss: 2.1308556\n",
      "Batch: 22\n",
      "Loss: 2.1226432\n",
      "Batch: 23\n",
      "Loss: 2.114736\n",
      "Batch: 24\n",
      "Loss: 2.107013\n",
      "Batch: 25\n",
      "Loss: 2.0993977\n",
      "Batch: 26\n",
      "Loss: 2.0918615\n",
      "Batch: 27\n",
      "Loss: 2.084403\n",
      "Batch: 28\n",
      "Loss: 2.0770338\n",
      "Batch: 29\n",
      "Loss: 2.0697644\n",
      "Batch: 30\n",
      "Loss: 2.0625944\n",
      "Batch: 31\n",
      "Loss: 2.0555203\n",
      "Batch: 32\n",
      "Loss: 2.048532\n",
      "Batch: 33\n",
      "Loss: 2.0416203\n",
      "Batch: 34\n",
      "Loss: 2.0347724\n",
      "Batch: 35\n",
      "Loss: 2.027978\n",
      "Batch: 36\n",
      "Loss: 2.0212264\n",
      "Batch: 37\n",
      "Loss: 2.0145106\n",
      "Batch: 38\n",
      "Loss: 2.0078266\n",
      "Batch: 39\n",
      "Loss: 2.0011725\n",
      "Batch: 40\n",
      "Loss: 1.9945476\n",
      "Batch: 41\n",
      "Loss: 1.9879524\n",
      "Batch: 42\n",
      "Loss: 1.9813871\n",
      "Batch: 43\n",
      "Loss: 1.974852\n",
      "Batch: 44\n",
      "Loss: 1.9683465\n",
      "Batch: 45\n",
      "Loss: 1.9618701\n",
      "Batch: 46\n",
      "Loss: 1.955422\n",
      "Batch: 47\n",
      "Loss: 1.9490012\n",
      "Batch: 48\n",
      "Loss: 1.9426072\n",
      "Batch: 49\n",
      "Loss: 1.9362389\n",
      "Batch: 50\n",
      "Loss: 1.9298962\n",
      "Batch: 51\n",
      "Loss: 1.9235791\n",
      "Batch: 52\n",
      "Loss: 1.9172871\n",
      "Batch: 53\n",
      "Loss: 1.9110205\n",
      "Batch: 54\n",
      "Loss: 1.9047788\n",
      "Batch: 55\n",
      "Loss: 1.8985623\n",
      "Batch: 56\n",
      "Loss: 1.8923708\n",
      "Batch: 57\n",
      "Loss: 1.886204\n",
      "Batch: 58\n",
      "Loss: 1.8800616\n",
      "Batch: 59\n",
      "Loss: 1.8739433\n",
      "Batch: 60\n",
      "Loss: 1.8678491\n",
      "Batch: 61\n",
      "Loss: 1.8617784\n",
      "Batch: 62\n",
      "Loss: 1.8557315\n",
      "Batch: 63\n",
      "Loss: 1.8497082\n",
      "Batch: 64\n",
      "Loss: 1.8437086\n",
      "Batch: 65\n",
      "Loss: 1.8377331\n",
      "Batch: 66\n",
      "Loss: 1.8317809\n",
      "Batch: 67\n",
      "Loss: 1.8258522\n",
      "Batch: 68\n",
      "Loss: 1.8199469\n",
      "Batch: 69\n",
      "Loss: 1.8140651\n",
      "Batch: 70\n",
      "Loss: 1.8082062\n",
      "Batch: 71\n",
      "Loss: 1.8023704\n",
      "Batch: 72\n",
      "Loss: 1.7965577\n",
      "Batch: 73\n",
      "Loss: 1.7907677\n",
      "Batch: 74\n",
      "Loss: 1.7850004\n",
      "Batch: 75\n",
      "Loss: 1.779256\n",
      "Batch: 76\n",
      "Loss: 1.7735342\n",
      "Batch: 77\n",
      "Train accuracy: 0.9886818910256411\n",
      "Validation accuracy: 0.6885190217391305\n",
      "Loss: 2.4116068\n",
      "Batch: 0\n",
      "Loss: 2.0978696\n",
      "Batch: 1\n",
      "Loss: 1.9236507\n",
      "Batch: 2\n",
      "Loss: 1.8640379\n",
      "Batch: 3\n",
      "Loss: 1.8124716\n",
      "Batch: 4\n",
      "Loss: 1.8012815\n",
      "Batch: 5\n",
      "Loss: 1.782497\n",
      "Batch: 6\n",
      "Loss: 1.7683357\n",
      "Batch: 7\n",
      "Loss: 1.7590804\n",
      "Batch: 8\n",
      "Loss: 1.7491661\n",
      "Batch: 9\n",
      "Loss: 1.73991\n",
      "Batch: 10\n",
      "Loss: 1.7325654\n",
      "Batch: 11\n",
      "Loss: 1.7264568\n",
      "Batch: 12\n",
      "Loss: 1.7206893\n",
      "Batch: 13\n",
      "Loss: 1.7149061\n",
      "Batch: 14\n",
      "Loss: 1.7092019\n",
      "Batch: 15\n",
      "Loss: 1.7037178\n",
      "Batch: 16\n",
      "Loss: 1.6984788\n",
      "Batch: 17\n",
      "Loss: 1.693422\n",
      "Batch: 18\n",
      "Loss: 1.68846\n",
      "Batch: 19\n",
      "Loss: 1.6835389\n",
      "Batch: 20\n",
      "Loss: 1.6786492\n",
      "Batch: 21\n",
      "Loss: 1.6738027\n",
      "Batch: 22\n",
      "Loss: 1.6690087\n",
      "Batch: 23\n",
      "Loss: 1.6642683\n",
      "Batch: 24\n",
      "Loss: 1.6595743\n",
      "Batch: 25\n",
      "Loss: 1.6549157\n",
      "Batch: 26\n",
      "Loss: 1.6502811\n",
      "Batch: 27\n",
      "Loss: 1.6456624\n",
      "Batch: 28\n",
      "Loss: 1.641055\n",
      "Batch: 29\n",
      "Loss: 1.6364573\n",
      "Batch: 30\n",
      "Loss: 1.6318709\n",
      "Batch: 31\n",
      "Loss: 1.6272967\n",
      "Batch: 32\n",
      "Loss: 1.6227374\n",
      "Batch: 33\n",
      "Loss: 1.618194\n",
      "Batch: 34\n",
      "Loss: 1.6136677\n",
      "Batch: 35\n",
      "Loss: 1.6091578\n",
      "Batch: 36\n",
      "Loss: 1.6046641\n",
      "Batch: 37\n",
      "Loss: 1.6001855\n",
      "Batch: 38\n",
      "Loss: 1.5957208\n",
      "Batch: 39\n",
      "Loss: 1.5912693\n",
      "Batch: 40\n",
      "Loss: 1.5868299\n",
      "Batch: 41\n",
      "Loss: 1.5824023\n",
      "Batch: 42\n",
      "Loss: 1.5779872\n",
      "Batch: 43\n",
      "Loss: 1.5735847\n",
      "Batch: 44\n",
      "Loss: 1.5691955\n",
      "Batch: 45\n",
      "Loss: 1.5648202\n",
      "Batch: 46\n",
      "Loss: 1.5604581\n",
      "Batch: 47\n",
      "Loss: 1.5561103\n",
      "Batch: 48\n",
      "Loss: 1.551776\n",
      "Batch: 49\n",
      "Loss: 1.5474557\n",
      "Batch: 50\n",
      "Loss: 1.5431489\n",
      "Batch: 51\n",
      "Loss: 1.5388556\n",
      "Batch: 52\n",
      "Loss: 1.5345757\n",
      "Batch: 53\n",
      "Loss: 1.5303088\n",
      "Batch: 54\n",
      "Loss: 1.5260559\n",
      "Batch: 55\n",
      "Loss: 1.521816\n",
      "Batch: 56\n",
      "Loss: 1.5175892\n",
      "Batch: 57\n",
      "Loss: 1.5133762\n",
      "Batch: 58\n",
      "Loss: 1.5091763\n",
      "Batch: 59\n",
      "Loss: 1.50499\n",
      "Batch: 60\n",
      "Loss: 1.500817\n",
      "Batch: 61\n",
      "Loss: 1.4966577\n",
      "Batch: 62\n",
      "Loss: 1.4925116\n",
      "Batch: 63\n",
      "Loss: 1.4883791\n",
      "Batch: 64\n",
      "Loss: 1.4842596\n",
      "Batch: 65\n",
      "Loss: 1.4801533\n",
      "Batch: 66\n",
      "Loss: 1.4760605\n",
      "Batch: 67\n",
      "Loss: 1.4719807\n",
      "Batch: 68\n",
      "Loss: 1.4679137\n",
      "Batch: 69\n",
      "Loss: 1.46386\n",
      "Batch: 70\n",
      "Loss: 1.4598191\n",
      "Batch: 71\n",
      "Loss: 1.4557912\n",
      "Batch: 72\n",
      "Loss: 1.4517763\n",
      "Batch: 73\n",
      "Loss: 1.4477742\n",
      "Batch: 74\n",
      "Loss: 1.4437846\n",
      "Batch: 75\n",
      "Loss: 1.4398079\n",
      "Batch: 76\n",
      "Loss: 1.4358443\n",
      "Batch: 77\n",
      "Train accuracy: 0.9982972756410257\n",
      "Validation accuracy: 0.6698369565217391\n",
      "Loss: 2.4583118\n",
      "Batch: 0\n",
      "Loss: 1.9351004\n",
      "Batch: 1\n",
      "Loss: 1.6917096\n",
      "Batch: 2\n",
      "Loss: 1.5674249\n",
      "Batch: 3\n",
      "Loss: 1.5247016\n",
      "Batch: 4\n",
      "Loss: 1.4929019\n",
      "Batch: 5\n",
      "Loss: 1.4727852\n",
      "Batch: 6\n",
      "Loss: 1.4599686\n",
      "Batch: 7\n",
      "Loss: 1.4519944\n",
      "Batch: 8\n",
      "Loss: 1.444861\n",
      "Batch: 9\n",
      "Loss: 1.4370844\n",
      "Batch: 10\n",
      "Loss: 1.4295865\n",
      "Batch: 11\n",
      "Loss: 1.4233198\n",
      "Batch: 12\n",
      "Loss: 1.41837\n",
      "Batch: 13\n",
      "Loss: 1.414305\n",
      "Batch: 14\n",
      "Loss: 1.4105344\n",
      "Batch: 15\n",
      "Loss: 1.4066947\n",
      "Batch: 16\n",
      "Loss: 1.4028361\n",
      "Batch: 17\n",
      "Loss: 1.3991036\n",
      "Batch: 18\n",
      "Loss: 1.3955147\n",
      "Batch: 19\n",
      "Loss: 1.3920304\n",
      "Batch: 20\n",
      "Loss: 1.3886316\n",
      "Batch: 21\n",
      "Loss: 1.3853152\n",
      "Batch: 22\n",
      "Loss: 1.382062\n",
      "Batch: 23\n",
      "Loss: 1.3788428\n",
      "Batch: 24\n",
      "Loss: 1.375637\n",
      "Batch: 25\n",
      "Loss: 1.3724406\n",
      "Batch: 26\n",
      "Loss: 1.3692571\n",
      "Batch: 27\n",
      "Loss: 1.36609\n",
      "Batch: 28\n",
      "Loss: 1.3629397\n",
      "Batch: 29\n",
      "Loss: 1.3598043\n",
      "Batch: 30\n",
      "Loss: 1.3566797\n",
      "Batch: 31\n",
      "Loss: 1.3535639\n",
      "Batch: 32\n",
      "Loss: 1.3504541\n",
      "Batch: 33\n",
      "Loss: 1.3473488\n",
      "Batch: 34\n",
      "Loss: 1.3442473\n",
      "Batch: 35\n",
      "Loss: 1.341149\n",
      "Batch: 36\n",
      "Loss: 1.3380538\n",
      "Batch: 37\n",
      "Loss: 1.3349622\n",
      "Batch: 38\n",
      "Loss: 1.3318743\n",
      "Batch: 39\n",
      "Loss: 1.3287907\n",
      "Batch: 40\n",
      "Loss: 1.3257115\n",
      "Batch: 41\n",
      "Loss: 1.3226379\n",
      "Batch: 42\n",
      "Loss: 1.3195701\n",
      "Batch: 43\n",
      "Loss: 1.316508\n",
      "Batch: 44\n",
      "Loss: 1.3134528\n",
      "Batch: 45\n",
      "Loss: 1.3104043\n",
      "Batch: 46\n",
      "Loss: 1.3073628\n",
      "Batch: 47\n",
      "Loss: 1.3043282\n",
      "Batch: 48\n",
      "Loss: 1.3013008\n",
      "Batch: 49\n",
      "Loss: 1.2982804\n",
      "Batch: 50\n",
      "Loss: 1.2952669\n",
      "Batch: 51\n",
      "Loss: 1.2922606\n",
      "Batch: 52\n",
      "Loss: 1.2892611\n",
      "Batch: 53\n",
      "Loss: 1.2862688\n",
      "Batch: 54\n",
      "Loss: 1.2832835\n",
      "Batch: 55\n",
      "Loss: 1.2803055\n",
      "Batch: 56\n",
      "Loss: 1.2773348\n",
      "Batch: 57\n",
      "Loss: 1.2743714\n",
      "Batch: 58\n",
      "Loss: 1.2714154\n",
      "Batch: 59\n",
      "Loss: 1.2684668\n",
      "Batch: 60\n",
      "Loss: 1.2655255\n",
      "Batch: 61\n",
      "Loss: 1.262592\n",
      "Batch: 62\n",
      "Loss: 1.2596656\n",
      "Batch: 63\n",
      "Loss: 1.2567468\n",
      "Batch: 64\n",
      "Loss: 1.2538352\n",
      "Batch: 65\n",
      "Loss: 1.2509313\n",
      "Batch: 66\n",
      "Loss: 1.2480346\n",
      "Batch: 67\n",
      "Loss: 1.2451456\n",
      "Batch: 68\n",
      "Loss: 1.2422636\n",
      "Batch: 69\n",
      "Loss: 1.2393893\n",
      "Batch: 70\n",
      "Loss: 1.2365218\n",
      "Batch: 71\n",
      "Loss: 1.2336622\n",
      "Batch: 72\n",
      "Loss: 1.2308095\n",
      "Batch: 73\n",
      "Loss: 1.2279643\n",
      "Batch: 74\n",
      "Loss: 1.2251261\n",
      "Batch: 75\n",
      "Loss: 1.2222955\n",
      "Batch: 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2194717\n",
      "Batch: 77\n",
      "Train accuracy: 0.9955929487179487\n",
      "Validation accuracy: 0.681046195652174\n",
      "Loss: 2.047232\n",
      "Batch: 0\n",
      "Loss: 1.6528767\n",
      "Batch: 1\n",
      "Loss: 1.3625965\n",
      "Batch: 2\n",
      "Loss: 1.2733976\n",
      "Batch: 3\n",
      "Loss: 1.2483989\n",
      "Batch: 4\n",
      "Loss: 1.2347552\n",
      "Batch: 5\n",
      "Loss: 1.218029\n",
      "Batch: 6\n",
      "Loss: 1.2065902\n",
      "Batch: 7\n",
      "Loss: 1.198956\n",
      "Batch: 8\n",
      "Loss: 1.192144\n",
      "Batch: 9\n",
      "Loss: 1.1861367\n",
      "Batch: 10\n",
      "Loss: 1.1815007\n",
      "Batch: 11\n",
      "Loss: 1.1777263\n",
      "Batch: 12\n",
      "Loss: 1.1742325\n",
      "Batch: 13\n",
      "Loss: 1.1709801\n",
      "Batch: 14\n",
      "Loss: 1.1680155\n",
      "Batch: 15\n",
      "Loss: 1.1651844\n",
      "Batch: 16\n",
      "Loss: 1.1623319\n",
      "Batch: 17\n",
      "Loss: 1.1594843\n",
      "Batch: 18\n",
      "Loss: 1.1567261\n",
      "Batch: 19\n",
      "Loss: 1.154084\n",
      "Batch: 20\n",
      "Loss: 1.1515338\n",
      "Batch: 21\n",
      "Loss: 1.1490407\n",
      "Batch: 22\n",
      "Loss: 1.146579\n",
      "Batch: 23\n",
      "Loss: 1.144134\n",
      "Batch: 24\n",
      "Loss: 1.1416973\n",
      "Batch: 25\n",
      "Loss: 1.1392659\n",
      "Batch: 26\n",
      "Loss: 1.136837\n",
      "Batch: 27\n",
      "Loss: 1.1344097\n",
      "Batch: 28\n",
      "Loss: 1.1319836\n",
      "Batch: 29\n",
      "Loss: 1.1295587\n",
      "Batch: 30\n",
      "Loss: 1.1271353\n",
      "Batch: 31\n",
      "Loss: 1.1247138\n",
      "Batch: 32\n",
      "Loss: 1.1222942\n",
      "Batch: 33\n",
      "Loss: 1.1198772\n",
      "Batch: 34\n",
      "Loss: 1.1174628\n",
      "Batch: 35\n",
      "Loss: 1.1150512\n",
      "Batch: 36\n",
      "Loss: 1.1126425\n",
      "Batch: 37\n",
      "Loss: 1.1102363\n",
      "Batch: 38\n",
      "Loss: 1.1078328\n",
      "Batch: 39\n",
      "Loss: 1.1054322\n",
      "Batch: 40\n",
      "Loss: 1.1030343\n",
      "Batch: 41\n",
      "Loss: 1.1006393\n",
      "Batch: 42\n",
      "Loss: 1.098248\n",
      "Batch: 43\n",
      "Loss: 1.0958604\n",
      "Batch: 44\n",
      "Loss: 1.093477\n",
      "Batch: 45\n",
      "Loss: 1.0910981\n",
      "Batch: 46\n",
      "Loss: 1.0887237\n",
      "Batch: 47\n",
      "Loss: 1.086354\n",
      "Batch: 48\n",
      "Loss: 1.083989\n",
      "Batch: 49\n",
      "Loss: 1.0816288\n",
      "Batch: 50\n",
      "Loss: 1.079273\n",
      "Batch: 51\n",
      "Loss: 1.076922\n",
      "Batch: 52\n",
      "Loss: 1.0745755\n",
      "Batch: 53\n",
      "Loss: 1.0722342\n",
      "Batch: 54\n",
      "Loss: 1.069897\n",
      "Batch: 55\n",
      "Loss: 1.067565\n",
      "Batch: 56\n",
      "Loss: 1.0652382\n",
      "Batch: 57\n",
      "Loss: 1.0629164\n",
      "Batch: 58\n",
      "Loss: 1.0605997\n",
      "Batch: 59\n",
      "Loss: 1.0582882\n",
      "Batch: 60\n",
      "Loss: 1.0559818\n",
      "Batch: 61\n",
      "Loss: 1.0536807\n",
      "Batch: 62\n",
      "Loss: 1.0513848\n",
      "Batch: 63\n",
      "Loss: 1.0490936\n",
      "Batch: 64\n",
      "Loss: 1.0468079\n",
      "Batch: 65\n",
      "Loss: 1.0445275\n",
      "Batch: 66\n",
      "Loss: 1.0422521\n",
      "Batch: 67\n",
      "Loss: 1.0399818\n",
      "Batch: 68\n",
      "Loss: 1.0377167\n",
      "Batch: 69\n",
      "Loss: 1.0354568\n",
      "Batch: 70\n",
      "Loss: 1.0332019\n",
      "Batch: 71\n",
      "Loss: 1.0309523\n",
      "Batch: 72\n",
      "Loss: 1.0287076\n",
      "Batch: 73\n",
      "Loss: 1.0264683\n",
      "Batch: 74\n",
      "Loss: 1.0242338\n",
      "Batch: 75\n",
      "Loss: 1.0220046\n",
      "Batch: 76\n",
      "Loss: 1.0197803\n",
      "Batch: 77\n",
      "Train accuracy: 0.996895032051282\n",
      "Validation accuracy: 0.7027853260869565\n",
      "Loss: 1.6389763\n",
      "Batch: 0\n",
      "Loss: 1.3091729\n",
      "Batch: 1\n",
      "Loss: 1.1467144\n",
      "Batch: 2\n",
      "Loss: 1.0931153\n",
      "Batch: 3\n",
      "Loss: 1.0689673\n",
      "Batch: 4\n",
      "Loss: 1.0627166\n",
      "Batch: 5\n",
      "Loss: 1.04995\n",
      "Batch: 6\n",
      "Loss: 1.0389872\n",
      "Batch: 7\n",
      "Loss: 1.0329069\n",
      "Batch: 8\n",
      "Loss: 1.0289795\n",
      "Batch: 9\n",
      "Loss: 1.0255647\n",
      "Batch: 10\n",
      "Loss: 1.0222299\n",
      "Batch: 11\n",
      "Loss: 1.0190207\n",
      "Batch: 12\n",
      "Loss: 1.0160377\n",
      "Batch: 13\n",
      "Loss: 1.0133082\n",
      "Batch: 14\n",
      "Loss: 1.010803\n",
      "Batch: 15\n",
      "Loss: 1.0084656\n",
      "Batch: 16\n",
      "Loss: 1.0062324\n",
      "Batch: 17\n",
      "Loss: 1.004049\n",
      "Batch: 18\n",
      "Loss: 1.0018888\n",
      "Batch: 19\n",
      "Loss: 0.99975806\n",
      "Batch: 20\n",
      "Loss: 0.99768215\n",
      "Batch: 21\n",
      "Loss: 0.9956751\n",
      "Batch: 22\n",
      "Loss: 0.9937209\n",
      "Batch: 23\n",
      "Loss: 0.9917778\n",
      "Batch: 24\n",
      "Loss: 0.989817\n",
      "Batch: 25\n",
      "Loss: 0.98784417\n",
      "Batch: 26\n",
      "Loss: 0.9858779\n",
      "Batch: 27\n",
      "Loss: 0.9839246\n",
      "Batch: 28\n",
      "Loss: 0.9819781\n",
      "Batch: 29\n",
      "Loss: 0.98003113\n",
      "Batch: 30\n",
      "Loss: 0.9780824\n",
      "Batch: 31\n",
      "Loss: 0.9761351\n",
      "Batch: 32\n",
      "Loss: 0.974192\n",
      "Batch: 33\n",
      "Loss: 0.97225404\n",
      "Batch: 34\n",
      "Loss: 0.97032\n",
      "Batch: 35\n",
      "Loss: 0.96838796\n",
      "Batch: 36\n",
      "Loss: 0.96645665\n",
      "Batch: 37\n",
      "Loss: 0.96452475\n",
      "Batch: 38\n",
      "Loss: 0.9625922\n",
      "Batch: 39\n",
      "Loss: 0.960659\n",
      "Batch: 40\n",
      "Loss: 0.9587263\n",
      "Batch: 41\n",
      "Loss: 0.9567944\n",
      "Batch: 42\n",
      "Loss: 0.9548641\n",
      "Batch: 43\n",
      "Loss: 0.9529358\n",
      "Batch: 44\n",
      "Loss: 0.9510101\n",
      "Batch: 45\n",
      "Loss: 0.9490872\n",
      "Batch: 46\n",
      "Loss: 0.9471673\n",
      "Batch: 47\n",
      "Loss: 0.94525045\n",
      "Batch: 48\n",
      "Loss: 0.94333667\n",
      "Batch: 49\n",
      "Loss: 0.94142616\n",
      "Batch: 50\n",
      "Loss: 0.939519\n",
      "Batch: 51\n",
      "Loss: 0.93761516\n",
      "Batch: 52\n",
      "Loss: 0.935715\n",
      "Batch: 53\n",
      "Loss: 0.93381834\n",
      "Batch: 54\n",
      "Loss: 0.93192554\n",
      "Batch: 55\n",
      "Loss: 0.9300364\n",
      "Batch: 56\n",
      "Loss: 0.928151\n",
      "Batch: 57\n",
      "Loss: 0.92626953\n",
      "Batch: 58\n",
      "Loss: 0.9243915\n",
      "Batch: 59\n",
      "Loss: 0.9225175\n",
      "Batch: 60\n",
      "Loss: 0.9206473\n",
      "Batch: 61\n",
      "Loss: 0.918781\n",
      "Batch: 62\n",
      "Loss: 0.9169186\n",
      "Batch: 63\n",
      "Loss: 0.91506016\n",
      "Batch: 64\n",
      "Loss: 0.9132057\n",
      "Batch: 65\n",
      "Loss: 0.911355\n",
      "Batch: 66\n",
      "Loss: 0.90950793\n",
      "Batch: 67\n",
      "Loss: 0.9076646\n",
      "Batch: 68\n",
      "Loss: 0.90582526\n",
      "Batch: 69\n",
      "Loss: 0.90398973\n",
      "Batch: 70\n",
      "Loss: 0.90215814\n",
      "Batch: 71\n",
      "Loss: 0.90033036\n",
      "Batch: 72\n",
      "Loss: 0.89850634\n",
      "Batch: 73\n",
      "Loss: 0.89668614\n",
      "Batch: 74\n",
      "Loss: 0.89486974\n",
      "Batch: 75\n",
      "Loss: 0.893057\n",
      "Batch: 76\n",
      "Loss: 0.89124817\n",
      "Batch: 77\n",
      "Train accuracy: 0.9984975961538461\n",
      "Validation accuracy: 0.6793478260869565\n",
      "Loss: 1.4110727\n",
      "Batch: 0\n",
      "Loss: 1.0909864\n",
      "Batch: 1\n",
      "Loss: 0.9459318\n",
      "Batch: 2\n",
      "Loss: 0.9242387\n",
      "Batch: 3\n",
      "Loss: 0.9006247\n",
      "Batch: 4\n",
      "Loss: 0.890997\n",
      "Batch: 5\n",
      "Loss: 0.88526475\n",
      "Batch: 6\n",
      "Loss: 0.8785959\n",
      "Batch: 7\n",
      "Loss: 0.8727845\n",
      "Batch: 8\n",
      "Loss: 0.86855227\n",
      "Batch: 9\n",
      "Loss: 0.865387\n",
      "Batch: 10\n",
      "Loss: 0.8627663\n",
      "Batch: 11\n",
      "Loss: 0.86040413\n",
      "Batch: 12\n",
      "Loss: 0.8581778\n",
      "Batch: 13\n",
      "Loss: 0.85605776\n",
      "Batch: 14\n",
      "Loss: 0.85403776\n",
      "Batch: 15\n",
      "Loss: 0.8521053\n",
      "Batch: 16\n",
      "Loss: 0.85024947\n",
      "Batch: 17\n",
      "Loss: 0.8484558\n",
      "Batch: 18\n",
      "Loss: 0.84670806\n",
      "Batch: 19\n",
      "Loss: 0.8449929\n",
      "Batch: 20\n",
      "Loss: 0.8433\n",
      "Batch: 21\n",
      "Loss: 0.84162116\n",
      "Batch: 22\n",
      "Loss: 0.8399519\n",
      "Batch: 23\n",
      "Loss: 0.83828837\n",
      "Batch: 24\n",
      "Loss: 0.8366289\n",
      "Batch: 25\n",
      "Loss: 0.8349723\n",
      "Batch: 26\n",
      "Loss: 0.83331776\n",
      "Batch: 27\n",
      "Loss: 0.8316651\n",
      "Batch: 28\n",
      "Loss: 0.8300141\n",
      "Batch: 29\n",
      "Loss: 0.8283647\n",
      "Batch: 30\n",
      "Loss: 0.826717\n",
      "Batch: 31\n",
      "Loss: 0.82507116\n",
      "Batch: 32\n",
      "Loss: 0.8234272\n",
      "Batch: 33\n",
      "Loss: 0.82178503\n",
      "Batch: 34\n",
      "Loss: 0.8201449\n",
      "Batch: 35\n",
      "Loss: 0.8185068\n",
      "Batch: 36\n",
      "Loss: 0.816871\n",
      "Batch: 37\n",
      "Loss: 0.8152374\n",
      "Batch: 38\n",
      "Loss: 0.81360614\n",
      "Batch: 39\n",
      "Loss: 0.81197727\n",
      "Batch: 40\n",
      "Loss: 0.81035066\n",
      "Batch: 41\n",
      "Loss: 0.8087264\n",
      "Batch: 42\n",
      "Loss: 0.80710477\n",
      "Batch: 43\n",
      "Loss: 0.8054857\n",
      "Batch: 44\n",
      "Loss: 0.8038692\n",
      "Batch: 45\n",
      "Loss: 0.8022554\n",
      "Batch: 46\n",
      "Loss: 0.80064434\n",
      "Batch: 47\n",
      "Loss: 0.79903597\n",
      "Batch: 48\n",
      "Loss: 0.79743046\n",
      "Batch: 49\n",
      "Loss: 0.7958277\n",
      "Batch: 50\n",
      "Loss: 0.79422784\n",
      "Batch: 51\n",
      "Loss: 0.7926311\n",
      "Batch: 52\n",
      "Loss: 0.79103696\n",
      "Batch: 53\n",
      "Loss: 0.789446\n",
      "Batch: 54\n",
      "Loss: 0.7878579\n",
      "Batch: 55\n",
      "Loss: 0.786273\n",
      "Batch: 56\n",
      "Loss: 0.784691\n",
      "Batch: 57\n",
      "Loss: 0.783112\n",
      "Batch: 58\n",
      "Loss: 0.7815361\n",
      "Batch: 59\n",
      "Loss: 0.7799631\n",
      "Batch: 60\n",
      "Loss: 0.7783933\n",
      "Batch: 61\n",
      "Loss: 0.7768266\n",
      "Batch: 62\n",
      "Loss: 0.77526295\n",
      "Batch: 63\n",
      "Loss: 0.7737023\n",
      "Batch: 64\n",
      "Loss: 0.77214473\n",
      "Batch: 65\n",
      "Loss: 0.7705903\n",
      "Batch: 66\n",
      "Loss: 0.7690388\n",
      "Batch: 67\n",
      "Loss: 0.7674905\n",
      "Batch: 68\n",
      "Loss: 0.7659451\n",
      "Batch: 69\n",
      "Loss: 0.7644027\n",
      "Batch: 70\n",
      "Loss: 0.7628635\n",
      "Batch: 71\n",
      "Loss: 0.76132727\n",
      "Batch: 72\n",
      "Loss: 0.7597941\n",
      "Batch: 73\n",
      "Loss: 0.7582639\n",
      "Batch: 74\n",
      "Loss: 0.7567368\n",
      "Batch: 75\n",
      "Loss: 0.75521266\n",
      "Batch: 76\n",
      "Loss: 0.7536915\n",
      "Batch: 77\n",
      "Train accuracy: 0.999198717948718\n",
      "Validation accuracy: 0.6820652173913043\n",
      "Loss: 1.3868016\n",
      "Batch: 0\n",
      "Loss: 1.0358479\n",
      "Batch: 1\n",
      "Loss: 0.8219839\n",
      "Batch: 2\n",
      "Loss: 0.8144639\n",
      "Batch: 3\n",
      "Loss: 0.7964127\n",
      "Batch: 4\n",
      "Loss: 0.78090435\n",
      "Batch: 5\n",
      "Loss: 0.7715162\n",
      "Batch: 6\n",
      "Loss: 0.76583576\n",
      "Batch: 7\n",
      "Loss: 0.76113707\n",
      "Batch: 8\n",
      "Loss: 0.7564961\n",
      "Batch: 9\n",
      "Loss: 0.7524457\n",
      "Batch: 10\n",
      "Loss: 0.749241\n",
      "Batch: 11\n",
      "Loss: 0.746696\n",
      "Batch: 12\n",
      "Loss: 0.74451256\n",
      "Batch: 13\n",
      "Loss: 0.74247783\n",
      "Batch: 14\n",
      "Loss: 0.7405241\n",
      "Batch: 15\n",
      "Loss: 0.73869\n",
      "Batch: 16\n",
      "Loss: 0.73703057\n",
      "Batch: 17\n",
      "Loss: 0.7355357\n",
      "Batch: 18\n",
      "Loss: 0.7341207\n",
      "Batch: 19\n",
      "Loss: 0.73270786\n",
      "Batch: 20\n",
      "Loss: 0.73128664\n",
      "Batch: 21\n",
      "Loss: 0.7298837\n",
      "Batch: 22\n",
      "Loss: 0.72850144\n",
      "Batch: 23\n",
      "Loss: 0.72712314\n",
      "Batch: 24\n",
      "Loss: 0.72574264\n",
      "Batch: 25\n",
      "Loss: 0.72436726\n",
      "Batch: 26\n",
      "Loss: 0.7230025\n",
      "Batch: 27\n",
      "Loss: 0.7216453\n",
      "Batch: 28\n",
      "Loss: 0.72029066\n",
      "Batch: 29\n",
      "Loss: 0.71893543\n",
      "Batch: 30\n",
      "Loss: 0.71757877\n",
      "Batch: 31\n",
      "Loss: 0.71622086\n",
      "Batch: 32\n",
      "Loss: 0.714862\n",
      "Batch: 33\n",
      "Loss: 0.7135021\n",
      "Batch: 34\n",
      "Loss: 0.7121406\n",
      "Batch: 35\n",
      "Loss: 0.71077716\n",
      "Batch: 36\n",
      "Loss: 0.7094109\n",
      "Batch: 37\n",
      "Loss: 0.70804167\n",
      "Batch: 38\n",
      "Loss: 0.70666975\n",
      "Batch: 39\n",
      "Loss: 0.70529586\n",
      "Batch: 40\n",
      "Loss: 0.7039206\n",
      "Batch: 41\n",
      "Loss: 0.70254457\n",
      "Batch: 42\n",
      "Loss: 0.7011685\n",
      "Batch: 43\n",
      "Loss: 0.6997928\n",
      "Batch: 44\n",
      "Loss: 0.698418\n",
      "Batch: 45\n",
      "Loss: 0.6970443\n",
      "Batch: 46\n",
      "Loss: 0.6956722\n",
      "Batch: 47\n",
      "Loss: 0.6943015\n",
      "Batch: 48\n",
      "Loss: 0.69293237\n",
      "Batch: 49\n",
      "Loss: 0.69156504\n",
      "Batch: 50\n",
      "Loss: 0.69019955\n",
      "Batch: 51\n",
      "Loss: 0.6888358\n",
      "Batch: 52\n",
      "Loss: 0.68747413\n",
      "Batch: 53\n",
      "Loss: 0.6861147\n",
      "Batch: 54\n",
      "Loss: 0.68475765\n",
      "Batch: 55\n",
      "Loss: 0.68340284\n",
      "Batch: 56\n",
      "Loss: 0.6820505\n",
      "Batch: 57\n",
      "Loss: 0.6807009\n",
      "Batch: 58\n",
      "Loss: 0.67935383\n",
      "Batch: 59\n",
      "Loss: 0.67800945\n",
      "Batch: 60\n",
      "Loss: 0.6766679\n",
      "Batch: 61\n",
      "Loss: 0.67532897\n",
      "Batch: 62\n",
      "Loss: 0.6739928\n",
      "Batch: 63\n",
      "Loss: 0.6726593\n",
      "Batch: 64\n",
      "Loss: 0.67132837\n",
      "Batch: 65\n",
      "Loss: 0.67000026\n",
      "Batch: 66\n",
      "Loss: 0.66867477\n",
      "Batch: 67\n",
      "Loss: 0.667352\n",
      "Batch: 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6660321\n",
      "Batch: 69\n",
      "Loss: 0.66471463\n",
      "Batch: 70\n",
      "Loss: 0.66340005\n",
      "Batch: 71\n",
      "Loss: 0.66208804\n",
      "Batch: 72\n",
      "Loss: 0.6607786\n",
      "Batch: 73\n",
      "Loss: 0.6594721\n",
      "Batch: 74\n",
      "Loss: 0.65816814\n",
      "Batch: 75\n",
      "Loss: 0.6568668\n",
      "Batch: 76\n",
      "Loss: 0.6555682\n",
      "Batch: 77\n",
      "Train accuracy: 0.9983974358974359\n",
      "Validation accuracy: 0.7088994565217391\n",
      "Loss: 1.185165\n",
      "Batch: 0\n",
      "Loss: 0.92216676\n",
      "Batch: 1\n",
      "Loss: 0.74742746\n",
      "Batch: 2\n",
      "Loss: 0.7317742\n",
      "Batch: 3\n",
      "Loss: 0.7031685\n",
      "Batch: 4\n",
      "Loss: 0.68872476\n",
      "Batch: 5\n",
      "Loss: 0.68291694\n",
      "Batch: 6\n",
      "Loss: 0.6806782\n",
      "Batch: 7\n",
      "Loss: 0.6785071\n",
      "Batch: 8\n",
      "Loss: 0.674973\n",
      "Batch: 9\n",
      "Loss: 0.67100304\n",
      "Batch: 10\n",
      "Loss: 0.6676639\n",
      "Batch: 11\n",
      "Loss: 0.665144\n",
      "Batch: 12\n",
      "Loss: 0.66321176\n",
      "Batch: 13\n",
      "Loss: 0.6616208\n",
      "Batch: 14\n",
      "Loss: 0.6601999\n",
      "Batch: 15\n",
      "Loss: 0.6588511\n",
      "Batch: 16\n",
      "Loss: 0.657523\n",
      "Batch: 17\n",
      "Loss: 0.6561891\n",
      "Batch: 18\n",
      "Loss: 0.6548443\n",
      "Batch: 19\n",
      "Loss: 0.6534978\n",
      "Batch: 20\n",
      "Loss: 0.65216553\n",
      "Batch: 21\n",
      "Loss: 0.6508656\n",
      "Batch: 22\n",
      "Loss: 0.6496026\n",
      "Batch: 23\n",
      "Loss: 0.6483559\n",
      "Batch: 24\n",
      "Loss: 0.6471102\n",
      "Batch: 25\n",
      "Loss: 0.6458692\n",
      "Batch: 26\n",
      "Loss: 0.64464\n",
      "Batch: 27\n",
      "Loss: 0.6434247\n",
      "Batch: 28\n",
      "Loss: 0.64222014\n",
      "Batch: 29\n",
      "Loss: 0.64102155\n",
      "Batch: 30\n",
      "Loss: 0.6398255\n",
      "Batch: 31\n",
      "Loss: 0.63862973\n",
      "Batch: 32\n",
      "Loss: 0.6374339\n",
      "Batch: 33\n",
      "Loss: 0.63623816\n",
      "Batch: 34\n",
      "Loss: 0.6350436\n",
      "Batch: 35\n",
      "Loss: 0.6338509\n",
      "Batch: 36\n",
      "Loss: 0.63266057\n",
      "Batch: 37\n",
      "Loss: 0.6314725\n",
      "Batch: 38\n",
      "Loss: 0.63028574\n",
      "Batch: 39\n",
      "Loss: 0.62909985\n",
      "Batch: 40\n",
      "Loss: 0.62791455\n",
      "Batch: 41\n",
      "Loss: 0.62672985\n",
      "Batch: 42\n",
      "Loss: 0.62554675\n",
      "Batch: 43\n",
      "Loss: 0.62436545\n",
      "Batch: 44\n",
      "Loss: 0.62318647\n",
      "Batch: 45\n",
      "Loss: 0.62200963\n",
      "Batch: 46\n",
      "Loss: 0.62083495\n",
      "Batch: 47\n",
      "Loss: 0.6196621\n",
      "Batch: 48\n",
      "Loss: 0.61849123\n",
      "Batch: 49\n",
      "Loss: 0.6173219\n",
      "Batch: 50\n",
      "Loss: 0.61615443\n",
      "Batch: 51\n",
      "Loss: 0.61498904\n",
      "Batch: 52\n",
      "Loss: 0.61382556\n",
      "Batch: 53\n",
      "Loss: 0.6126644\n",
      "Batch: 54\n",
      "Loss: 0.61150557\n",
      "Batch: 55\n",
      "Loss: 0.61034894\n",
      "Batch: 56\n",
      "Loss: 0.60919446\n",
      "Batch: 57\n",
      "Loss: 0.6080422\n",
      "Batch: 58\n",
      "Loss: 0.60689217\n",
      "Batch: 59\n",
      "Loss: 0.6057441\n",
      "Batch: 60\n",
      "Loss: 0.60459834\n",
      "Batch: 61\n",
      "Loss: 0.6034547\n",
      "Batch: 62\n",
      "Loss: 0.60231334\n",
      "Batch: 63\n",
      "Loss: 0.60117424\n",
      "Batch: 64\n",
      "Loss: 0.60003734\n",
      "Batch: 65\n",
      "Loss: 0.5989028\n",
      "Batch: 66\n",
      "Loss: 0.5977704\n",
      "Batch: 67\n",
      "Loss: 0.59664017\n",
      "Batch: 68\n",
      "Loss: 0.5955123\n",
      "Batch: 69\n",
      "Loss: 0.5943864\n",
      "Batch: 70\n",
      "Loss: 0.59326273\n",
      "Batch: 71\n",
      "Loss: 0.59214133\n",
      "Batch: 72\n",
      "Loss: 0.591022\n",
      "Batch: 73\n",
      "Loss: 0.589905\n",
      "Batch: 74\n",
      "Loss: 0.5887903\n",
      "Batch: 75\n",
      "Loss: 0.5876776\n",
      "Batch: 76\n",
      "Loss: 0.5865672\n",
      "Batch: 77\n",
      "Train accuracy: 0.9988982371794872\n",
      "Validation accuracy: 0.7068614130434783\n",
      "Loss: 1.3137329\n",
      "Batch: 0\n",
      "Loss: 0.8412391\n",
      "Batch: 1\n",
      "Loss: 0.68440145\n",
      "Batch: 2\n",
      "Loss: 0.6211965\n",
      "Batch: 3\n",
      "Loss: 0.6081442\n",
      "Batch: 4\n",
      "Loss: 0.60226697\n",
      "Batch: 5\n",
      "Loss: 0.59443396\n",
      "Batch: 6\n",
      "Loss: 0.58770174\n",
      "Batch: 7\n",
      "Loss: 0.5832158\n",
      "Batch: 8\n",
      "Loss: 0.58011526\n",
      "Batch: 9\n",
      "Loss: 0.5774893\n",
      "Batch: 10\n",
      "Loss: 0.5752469\n",
      "Batch: 11\n",
      "Loss: 0.5734676\n",
      "Batch: 12\n",
      "Loss: 0.5719652\n",
      "Batch: 13\n",
      "Loss: 0.5705218\n",
      "Batch: 14\n",
      "Loss: 0.56911635\n",
      "Batch: 15\n",
      "Loss: 0.56782603\n",
      "Batch: 16\n",
      "Loss: 0.56666505\n",
      "Batch: 17\n",
      "Loss: 0.56557924\n",
      "Batch: 18\n",
      "Loss: 0.5645136\n",
      "Batch: 19\n",
      "Loss: 0.56345206\n",
      "Batch: 20\n",
      "Loss: 0.56240505\n",
      "Batch: 21\n",
      "Loss: 0.56138355\n",
      "Batch: 22\n",
      "Loss: 0.5603892\n",
      "Batch: 23\n",
      "Loss: 0.55941635\n",
      "Batch: 24\n",
      "Loss: 0.5584558\n",
      "Batch: 25\n",
      "Loss: 0.55749923\n",
      "Batch: 26\n",
      "Loss: 0.55653954\n",
      "Batch: 27\n",
      "Loss: 0.555572\n",
      "Batch: 28\n",
      "Loss: 0.554595\n",
      "Batch: 29\n",
      "Loss: 0.5536081\n",
      "Batch: 30\n",
      "Loss: 0.552613\n",
      "Batch: 31\n",
      "Loss: 0.5516114\n",
      "Batch: 32\n",
      "Loss: 0.55060554\n",
      "Batch: 33\n",
      "Loss: 0.54959667\n",
      "Batch: 34\n",
      "Loss: 0.548586\n",
      "Batch: 35\n",
      "Loss: 0.54757386\n",
      "Batch: 36\n",
      "Loss: 0.5465605\n",
      "Batch: 37\n",
      "Loss: 0.5455459\n",
      "Batch: 38\n",
      "Loss: 0.5445298\n",
      "Batch: 39\n",
      "Loss: 0.5435124\n",
      "Batch: 40\n",
      "Loss: 0.54249394\n",
      "Batch: 41\n",
      "Loss: 0.5414747\n",
      "Batch: 42\n",
      "Loss: 0.54045504\n",
      "Batch: 43\n",
      "Loss: 0.5394353\n",
      "Batch: 44\n",
      "Loss: 0.5384159\n",
      "Batch: 45\n",
      "Loss: 0.5373973\n",
      "Batch: 46\n",
      "Loss: 0.53637964\n",
      "Batch: 47\n",
      "Loss: 0.53536314\n",
      "Batch: 48\n",
      "Loss: 0.53434795\n",
      "Batch: 49\n",
      "Loss: 0.53333426\n",
      "Batch: 50\n",
      "Loss: 0.53232193\n",
      "Batch: 51\n",
      "Loss: 0.531311\n",
      "Batch: 52\n",
      "Loss: 0.53030163\n",
      "Batch: 53\n",
      "Loss: 0.52929384\n",
      "Batch: 54\n",
      "Loss: 0.5282874\n",
      "Batch: 55\n",
      "Loss: 0.5272827\n",
      "Batch: 56\n",
      "Loss: 0.5262796\n",
      "Batch: 57\n",
      "Loss: 0.5252782\n",
      "Batch: 58\n",
      "Loss: 0.5242784\n",
      "Batch: 59\n",
      "Loss: 0.5232806\n",
      "Batch: 60\n",
      "Loss: 0.5222845\n",
      "Batch: 61\n",
      "Loss: 0.5212903\n",
      "Batch: 62\n",
      "Loss: 0.52029806\n",
      "Batch: 63\n",
      "Loss: 0.5193077\n",
      "Batch: 64\n",
      "Loss: 0.51831925\n",
      "Batch: 65\n",
      "Loss: 0.5173329\n",
      "Batch: 66\n",
      "Loss: 0.5163484\n",
      "Batch: 67\n",
      "Loss: 0.515366\n",
      "Batch: 68\n",
      "Loss: 0.5143854\n",
      "Batch: 69\n",
      "Loss: 0.5134068\n",
      "Batch: 70\n",
      "Loss: 0.51243013\n",
      "Batch: 71\n",
      "Loss: 0.5114554\n",
      "Batch: 72\n",
      "Loss: 0.5104826\n",
      "Batch: 73\n",
      "Loss: 0.50951177\n",
      "Batch: 74\n",
      "Loss: 0.50854284\n",
      "Batch: 75\n",
      "Loss: 0.50757575\n",
      "Batch: 76\n",
      "Loss: 0.5066107\n",
      "Batch: 77\n",
      "Train accuracy: 0.9987980769230769\n",
      "Validation accuracy: 0.7238451086956522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.7238451086956522"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [0.01421418]\n",
      "!\n",
      "training\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9867788461538461\n",
      "Validation accuracy: 0.6691576086956522\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n",
      "aft\n",
      "bef\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=11,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7d02602dd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.7214673913043478,\n",
       "  [0.5209337784341234,\n",
       "   0.00010145672866540617,\n",
       "   0.002745085743403337,\n",
       "   0.01720032028854354,\n",
       "   1.9288787551271169e-07,\n",
       "   0.0018099363819392262]),\n",
       " (-0.7163722826086957,\n",
       "  [0.19566211872939465,\n",
       "   4.612943193520689e-05,\n",
       "   0.0001385245297417862,\n",
       "   2.9933968632131992e-05,\n",
       "   4.5831243835169854e-07,\n",
       "   1.6130460553431962e-08]),\n",
       " (-0.7143342391304348,\n",
       "  [0.915320633137463,\n",
       "   1.7756514691944684e-08,\n",
       "   0.00020724515961583525,\n",
       "   0.0011088453611955683,\n",
       "   0.00102724352699463,\n",
       "   0.00129479916893042]),\n",
       " (-0.712296195652174,\n",
       "  [0.013068606013662519,\n",
       "   4.201693491769488e-08,\n",
       "   0.0244088779431698,\n",
       "   4.674396506517198e-06,\n",
       "   1.7804682060788777e-07,\n",
       "   0.0013964263944603375]),\n",
       " (-0.6878396739130435,\n",
       "  [0.03896589174346648,\n",
       "   1.1360346788939083e-08,\n",
       "   1.1886169633291324e-07,\n",
       "   5.599155379877504e-06,\n",
       "   0.0015369727171973043,\n",
       "   0.0014112606522899652]),\n",
       " (-0.6776494565217391,\n",
       "  [0.007094622341698628,\n",
       "   5.809432102833461e-06,\n",
       "   1.297997647182789e-07,\n",
       "   0.09733600759810308,\n",
       "   0.008836409187350512,\n",
       "   2.3676216240396406e-07]),\n",
       " (-0.6749320652173914,\n",
       "  [0.00010721165504197537,\n",
       "   0.007009773452079454,\n",
       "   0.09486062481920944,\n",
       "   0.0004428663033956968,\n",
       "   0.0002954319268547379,\n",
       "   1.0852267139063392e-05]),\n",
       " (-0.6722146739130435,\n",
       "  [7.867251982441986e-05,\n",
       "   6.920682488638565e-08,\n",
       "   1.0504421974209954e-08,\n",
       "   8.931583281367469e-07,\n",
       "   9.424855854848018e-05,\n",
       "   2.186458026761523e-06]),\n",
       " (-0.6623641304347826,\n",
       "  [0.2860139535703883,\n",
       "   0.00020066057633358258,\n",
       "   3.4231476270162917e-07,\n",
       "   2.3494384595008576e-08,\n",
       "   0.0001867136279391441,\n",
       "   1.244021978347893e-07]),\n",
       " (-0.6616847826086957,\n",
       "  [1.9532210535111048e-05,\n",
       "   4.2754654083880205e-07,\n",
       "   0.0024886332817548604,\n",
       "   6.41901363176667e-06,\n",
       "   0.002125946447702796,\n",
       "   1.7466370212239237e-08]),\n",
       " (-0.6477581521739131, [6e-05, 0.0002, 0.0002, 0.0002, 0.0002, 0.001])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5209337784341234,\n",
       " 0.00010145672866540617,\n",
       " 0.002745085743403337,\n",
       " 0.01720032028854354,\n",
       " 1.9288787551271169e-07,\n",
       " 0.0018099363819392262]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research-paper]",
   "language": "python",
   "name": "conda-env-research-paper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
