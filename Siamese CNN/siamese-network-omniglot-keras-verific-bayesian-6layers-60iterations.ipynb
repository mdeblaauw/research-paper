{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "data_subsets = ['train', 'val']\n",
    "data = {}\n",
    "\n",
    "for name in data_subsets:\n",
    "    with BytesIO() as files:\n",
    "        path = \"omniglot_images/\" +name+ \".pickle\"\n",
    "        s3.Bucket(\"research-paper-omniglot-data\").download_fileobj(path, files)\n",
    "        files.seek(0)    # move back to the beginning after writing\n",
    "        (X,c) = pickle.load(files)\n",
    "        data[name] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = '../../omniglot_images/'\n",
    "data_subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "data = {}\n",
    "categories = {}\n",
    "info = {}\n",
    "        \n",
    "for name in data_subsets:\n",
    "    file_path = os.path.join(path, name + \".pickle\")\n",
    "    print(\"loading data from {}\".format(file_path))\n",
    "    with open(file_path,\"rb\") as f:\n",
    "        (X,c) = pickle.load(f)\n",
    "        data[name] = X\n",
    "        categories[name] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_data(size, s='train'):\n",
    "    #get train data and shape\n",
    "    X=data[s]\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    #initialize 2 empty arrays for the input size in a list\n",
    "    pairs=[np.zeros((size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    #initialize vector for the targets\n",
    "    targets=np.zeros((size,1))\n",
    "    \n",
    "    for x in range(size):\n",
    "        #randomly sample one class (character)\n",
    "        category = rnd.choice(n_classes,1,replace=False)\n",
    "        #randomly sample one example from class (1-20 characters)\n",
    "        idx_1 = rnd.randint(0, n_examples)\n",
    "        pairs[0][x,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        #randomly sample again one example from class and add last class with modulo\n",
    "        # ..to ensure not same class pairs are created\n",
    "        idx_2 = (idx_1 + rnd.randint(0, n_examples)) % n_examples\n",
    "        #pick images of different class for 1st half and same class for 2nd half\n",
    "        if x >= size // 2:\n",
    "            category_2 = category\n",
    "            targets[x] = 1\n",
    "        else: \n",
    "        #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "        # ..different category\n",
    "            idx_2 = rnd.randint(0, n_examples) \n",
    "            category_2 = (category + rnd.randint(1,n_classes)) % n_classes\n",
    "            targets[x] = 0\n",
    "        pairs[1][x,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "        \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, train_labels = create_train_data(10000)\n",
    "val_set, val_labels = create_train_data(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-7, high=0.1, prior='log-uniform', name='learning_rate')\n",
    "dim_reg_layer1 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer1')\n",
    "dim_reg_layer2 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer2')\n",
    "dim_reg_layer3 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer3')\n",
    "dim_reg_layer4 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer4')\n",
    "dim_reg_layer5 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer5')\n",
    "dim_filt_layer1 = Integer(3,12, name='filter_layer1')\n",
    "dim_filt_layer2 = Integer(3,10, name='filter_layer2')\n",
    "dim_filt_layer3 = Integer(3,8, name='filter_layer3')\n",
    "dim_filt_layer4 = Integer(3,6, name='filter_layer4')\n",
    "dim_chan_layer1 = Categorical([16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256], name='channel_layer1')\n",
    "dim_chan_layer2 = Categorical([16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256], name='channel_layer2')\n",
    "dim_chan_layer3 = Categorical([16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256], name='channel_layer3')\n",
    "dim_chan_layer4 = Categorical([16,32,48,64,80,96,112,128,144,160,176,192,208,224,240,256], name='channel_layer4')\n",
    "dim_fc_layer5 = Categorical([256,512,768,1024,1280,1536,1792,2048,2304,2560,2816,3072,3328,3584,3840,4096], name='channel_layer5')\n",
    "beta1 = Real(low=0.00001, high=0.9999, prior = 'uniform', name='beta1')\n",
    "beta2 = Real(low=0.00001, high=0.9999, prior = 'uniform', name='beta2')\n",
    "batch = Categorical([32,48], name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = [dim_learning_rate,\n",
    "             dim_reg_layer1,\n",
    "             dim_reg_layer2,\n",
    "             dim_reg_layer3,\n",
    "             dim_reg_layer4,\n",
    "             dim_reg_layer5,\n",
    "             dim_filt_layer1,\n",
    "             dim_filt_layer2,\n",
    "             dim_filt_layer3,\n",
    "             dim_filt_layer4,\n",
    "             dim_chan_layer1,\n",
    "             dim_chan_layer2,\n",
    "             dim_chan_layer3,\n",
    "             dim_chan_layer4,\n",
    "             dim_fc_layer5,\n",
    "             beta1,\n",
    "             beta2,\n",
    "             batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_parameters = [0.00006,2e-4,2e-4,2e-4,2e-4,1e-3,10,7,4,4,64,128,128,256,2048,0.9,0.999,64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rnd.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rnd.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(**params):\n",
    "    input_shape = (105, 105, 1)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    #build convnet to use in each siamese 'leg'\n",
    "    convnet = Sequential()\n",
    "    convnet.add(Conv2D(params['channel_layer1'],(params['filter_layer1'],params['filter_layer1']),activation='relu',input_shape=input_shape,kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer1']),bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer2'],(params['filter_layer2'],params['filter_layer2']),activation='relu',kernel_regularizer=l2(params['reg_layer2']),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer3'],(params['filter_layer3'],params['filter_layer3']),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer3']),bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer4'],(params['filter_layer4'],params['filter_layer4']),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer4']),bias_initializer=b_init))\n",
    "    convnet.add(Flatten())\n",
    "    convnet.add(Dense(params['channel_layer5'],activation=\"sigmoid\",kernel_regularizer=l2(params['reg_layer5']),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    #call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "    encoded_l = convnet(left_input)\n",
    "    encoded_r = convnet(right_input)\n",
    "    #layer to merge two encoded inputs with the l1 distance between them\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    #call this layer on list of two input tensors.\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    optimizer = Adam(lr=params['learning_rate'], beta_1=params['beta1'], beta_2=params['beta2'])\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "    #siamese_net.count_params()\n",
    "    return(siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, true_val):\n",
    "    acc_bool = np.equal(np.round_(pred), true_val)\n",
    "    acc = np.mean(acc_bool.astype(int))\n",
    "    \n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just as fast as the Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "###################################\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(**params):\n",
    "    #Training loop\n",
    "    siamese_net = create_network(**params)\n",
    "        \n",
    "    print(\"!\")\n",
    "    batch_size = params['batch']\n",
    "    total_batch = int(10000/batch_size)\n",
    "    total_batch_val = int(3000/batch_size)\n",
    "    epoch = 10\n",
    "\n",
    "    print(\"training\")\n",
    "    for i in range(epoch):\n",
    "        batch_x1, batch_x2, batch_y = shuffle(train_set[0],train_set[1], train_labels, n_samples = batch_size)\n",
    "        train_batch_acc = 0\n",
    "        for j in range(total_batch):\n",
    "            loss=siamese_net.train_on_batch([batch_x1, batch_x2],batch_y)\n",
    "            probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "            train_batch_acc += accuracy(probs, batch_y)\n",
    "            #print('Loss:', loss)\n",
    "            #print('Batch:', j)\n",
    "        train_acc = train_batch_acc/total_batch\n",
    "        val_batch_acc = 0\n",
    "        for validation in range(total_batch_val):\n",
    "            batch_x1, batch_x2, batch_y = shuffle(val_set[0],val_set[1], val_labels, n_samples = batch_size)\n",
    "            probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "            val_batch_acc += accuracy(probs, batch_y)\n",
    "        val_acc = val_batch_acc/total_batch_val\n",
    "        print('Epoch:', i)\n",
    "        print('Train accuracy:', train_acc)\n",
    "        print('Validation accuracy:', val_acc)\n",
    "            \n",
    "    return(-val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "Loss: 4.4984665\n",
      "Batch: 0\n",
      "Loss: 4.403412\n",
      "Batch: 1\n",
      "Loss: 4.330426\n",
      "Batch: 2\n",
      "Loss: 4.24168\n",
      "Batch: 3\n",
      "Loss: 4.1520605\n",
      "Batch: 4\n",
      "Loss: 4.066664\n",
      "Batch: 5\n",
      "Loss: 3.988257\n",
      "Batch: 6\n",
      "Loss: 3.9156978\n",
      "Batch: 7\n",
      "Loss: 3.8498886\n",
      "Batch: 8\n",
      "Loss: 3.791628\n",
      "Batch: 9\n",
      "Loss: 3.7408202\n",
      "Batch: 10\n",
      "Loss: 3.6968129\n",
      "Batch: 11\n",
      "Loss: 3.658761\n",
      "Batch: 12\n",
      "Loss: 3.6255662\n",
      "Batch: 13\n",
      "Loss: 3.5960796\n",
      "Batch: 14\n",
      "Loss: 3.5691466\n",
      "Batch: 15\n",
      "Loss: 3.5438907\n",
      "Batch: 16\n",
      "Loss: 3.5197856\n",
      "Batch: 17\n",
      "Loss: 3.496443\n",
      "Batch: 18\n",
      "Loss: 3.4735928\n",
      "Batch: 19\n",
      "Loss: 3.4510489\n",
      "Batch: 20\n",
      "Loss: 3.4286983\n",
      "Batch: 21\n",
      "Loss: 3.4064739\n",
      "Batch: 22\n",
      "Loss: 3.384332\n",
      "Batch: 23\n",
      "Loss: 3.3622463\n",
      "Batch: 24\n",
      "Loss: 3.3402033\n",
      "Batch: 25\n",
      "Loss: 3.318196\n",
      "Batch: 26\n",
      "Loss: 3.2962224\n",
      "Batch: 27\n",
      "Loss: 3.2742836\n",
      "Batch: 28\n",
      "Loss: 3.252379\n",
      "Batch: 29\n",
      "Loss: 3.2305143\n",
      "Batch: 30\n",
      "Loss: 3.2086942\n",
      "Batch: 31\n",
      "Loss: 3.186924\n",
      "Batch: 32\n",
      "Loss: 3.165208\n",
      "Batch: 33\n",
      "Loss: 3.1435518\n",
      "Batch: 34\n",
      "Loss: 3.121961\n",
      "Batch: 35\n",
      "Loss: 3.10044\n",
      "Batch: 36\n",
      "Loss: 3.078994\n",
      "Batch: 37\n",
      "Loss: 3.0576262\n",
      "Batch: 38\n",
      "Loss: 3.0363412\n",
      "Batch: 39\n",
      "Loss: 3.0151424\n",
      "Batch: 40\n",
      "Loss: 2.9940333\n",
      "Batch: 41\n",
      "Loss: 2.9730182\n",
      "Batch: 42\n",
      "Loss: 2.9520986\n",
      "Batch: 43\n",
      "Loss: 2.931277\n",
      "Batch: 44\n",
      "Loss: 2.9105566\n",
      "Batch: 45\n",
      "Loss: 2.8899386\n",
      "Batch: 46\n",
      "Loss: 2.8694258\n",
      "Batch: 47\n",
      "Loss: 2.8490198\n",
      "Batch: 48\n",
      "Loss: 2.828722\n",
      "Batch: 49\n",
      "Loss: 2.808533\n",
      "Batch: 50\n",
      "Loss: 2.7884555\n",
      "Batch: 51\n",
      "Loss: 2.7684894\n",
      "Batch: 52\n",
      "Loss: 2.7486362\n",
      "Batch: 53\n",
      "Loss: 2.7288964\n",
      "Batch: 54\n",
      "Loss: 2.709271\n",
      "Batch: 55\n",
      "Loss: 2.6897604\n",
      "Batch: 56\n",
      "Loss: 2.6703656\n",
      "Batch: 57\n",
      "Loss: 2.651086\n",
      "Batch: 58\n",
      "Loss: 2.631923\n",
      "Batch: 59\n",
      "Loss: 2.6128764\n",
      "Batch: 60\n",
      "Loss: 2.5939467\n",
      "Batch: 61\n",
      "Loss: 2.5751336\n",
      "Batch: 62\n",
      "Loss: 2.556437\n",
      "Batch: 63\n",
      "Loss: 2.537858\n",
      "Batch: 64\n",
      "Loss: 2.519395\n",
      "Batch: 65\n",
      "Loss: 2.5010495\n",
      "Batch: 66\n",
      "Loss: 2.4828203\n",
      "Batch: 67\n",
      "Loss: 2.464708\n",
      "Batch: 68\n",
      "Loss: 2.4467118\n",
      "Batch: 69\n",
      "Loss: 2.428831\n",
      "Batch: 70\n",
      "Loss: 2.4110668\n",
      "Batch: 71\n",
      "Loss: 2.3934178\n",
      "Batch: 72\n",
      "Loss: 2.375884\n",
      "Batch: 73\n",
      "Loss: 2.3584652\n",
      "Batch: 74\n",
      "Loss: 2.341161\n",
      "Batch: 75\n",
      "Loss: 2.3239706\n",
      "Batch: 76\n",
      "Loss: 2.3068943\n",
      "Batch: 77\n",
      "Train accuracy: 0.9938902243589743\n",
      "Validation accuracy: 0.6769701086956522\n",
      "Loss: 3.0459366\n",
      "Batch: 0\n",
      "Loss: 2.739714\n",
      "Batch: 1\n",
      "Loss: 2.5221207\n",
      "Batch: 2\n",
      "Loss: 2.4402697\n",
      "Batch: 3\n",
      "Loss: 2.9450927\n",
      "Batch: 4\n",
      "Loss: 2.3504941\n",
      "Batch: 5\n",
      "Loss: 2.3620136\n",
      "Batch: 6\n",
      "Loss: 2.3611927\n",
      "Batch: 7\n",
      "Loss: 2.3569481\n",
      "Batch: 8\n",
      "Loss: 2.321332\n",
      "Batch: 9\n",
      "Loss: 2.2981665\n",
      "Batch: 10\n",
      "Loss: 2.2569647\n",
      "Batch: 11\n",
      "Loss: 2.2380733\n",
      "Batch: 12\n",
      "Loss: 2.2247086\n",
      "Batch: 13\n",
      "Loss: 2.2093043\n",
      "Batch: 14\n",
      "Loss: 2.1959233\n",
      "Batch: 15\n",
      "Loss: 2.1854534\n",
      "Batch: 16\n",
      "Loss: 2.1763175\n",
      "Batch: 17\n",
      "Loss: 2.1672475\n",
      "Batch: 18\n",
      "Loss: 2.1579008\n",
      "Batch: 19\n",
      "Loss: 2.1485438\n",
      "Batch: 20\n",
      "Loss: 2.1394808\n",
      "Batch: 21\n",
      "Loss: 2.1308556\n",
      "Batch: 22\n",
      "Loss: 2.1226432\n",
      "Batch: 23\n",
      "Loss: 2.114736\n",
      "Batch: 24\n",
      "Loss: 2.107013\n",
      "Batch: 25\n",
      "Loss: 2.0993977\n",
      "Batch: 26\n",
      "Loss: 2.0918615\n",
      "Batch: 27\n",
      "Loss: 2.084403\n",
      "Batch: 28\n",
      "Loss: 2.0770338\n",
      "Batch: 29\n",
      "Loss: 2.0697644\n",
      "Batch: 30\n",
      "Loss: 2.0625944\n",
      "Batch: 31\n",
      "Loss: 2.0555203\n",
      "Batch: 32\n",
      "Loss: 2.048532\n",
      "Batch: 33\n",
      "Loss: 2.0416203\n",
      "Batch: 34\n",
      "Loss: 2.0347724\n",
      "Batch: 35\n",
      "Loss: 2.027978\n",
      "Batch: 36\n",
      "Loss: 2.0212264\n",
      "Batch: 37\n",
      "Loss: 2.0145106\n",
      "Batch: 38\n",
      "Loss: 2.0078266\n",
      "Batch: 39\n",
      "Loss: 2.0011725\n",
      "Batch: 40\n",
      "Loss: 1.9945476\n",
      "Batch: 41\n",
      "Loss: 1.9879524\n",
      "Batch: 42\n",
      "Loss: 1.9813871\n",
      "Batch: 43\n",
      "Loss: 1.974852\n",
      "Batch: 44\n",
      "Loss: 1.9683465\n",
      "Batch: 45\n",
      "Loss: 1.9618701\n",
      "Batch: 46\n",
      "Loss: 1.955422\n",
      "Batch: 47\n",
      "Loss: 1.9490012\n",
      "Batch: 48\n",
      "Loss: 1.9426072\n",
      "Batch: 49\n",
      "Loss: 1.9362389\n",
      "Batch: 50\n",
      "Loss: 1.9298962\n",
      "Batch: 51\n",
      "Loss: 1.9235791\n",
      "Batch: 52\n",
      "Loss: 1.9172871\n",
      "Batch: 53\n",
      "Loss: 1.9110205\n",
      "Batch: 54\n",
      "Loss: 1.9047788\n",
      "Batch: 55\n",
      "Loss: 1.8985623\n",
      "Batch: 56\n",
      "Loss: 1.8923708\n",
      "Batch: 57\n",
      "Loss: 1.886204\n",
      "Batch: 58\n",
      "Loss: 1.8800616\n",
      "Batch: 59\n",
      "Loss: 1.8739433\n",
      "Batch: 60\n",
      "Loss: 1.8678491\n",
      "Batch: 61\n",
      "Loss: 1.8617784\n",
      "Batch: 62\n",
      "Loss: 1.8557315\n",
      "Batch: 63\n",
      "Loss: 1.8497082\n",
      "Batch: 64\n",
      "Loss: 1.8437086\n",
      "Batch: 65\n",
      "Loss: 1.8377331\n",
      "Batch: 66\n",
      "Loss: 1.8317809\n",
      "Batch: 67\n",
      "Loss: 1.8258522\n",
      "Batch: 68\n",
      "Loss: 1.8199469\n",
      "Batch: 69\n",
      "Loss: 1.8140651\n",
      "Batch: 70\n",
      "Loss: 1.8082062\n",
      "Batch: 71\n",
      "Loss: 1.8023704\n",
      "Batch: 72\n",
      "Loss: 1.7965577\n",
      "Batch: 73\n",
      "Loss: 1.7907677\n",
      "Batch: 74\n",
      "Loss: 1.7850004\n",
      "Batch: 75\n",
      "Loss: 1.779256\n",
      "Batch: 76\n",
      "Loss: 1.7735342\n",
      "Batch: 77\n",
      "Train accuracy: 0.9886818910256411\n",
      "Validation accuracy: 0.6885190217391305\n",
      "Loss: 2.4116068\n",
      "Batch: 0\n",
      "Loss: 2.0978696\n",
      "Batch: 1\n",
      "Loss: 1.9236507\n",
      "Batch: 2\n",
      "Loss: 1.8640379\n",
      "Batch: 3\n",
      "Loss: 1.8124716\n",
      "Batch: 4\n",
      "Loss: 1.8012815\n",
      "Batch: 5\n",
      "Loss: 1.782497\n",
      "Batch: 6\n",
      "Loss: 1.7683357\n",
      "Batch: 7\n",
      "Loss: 1.7590804\n",
      "Batch: 8\n",
      "Loss: 1.7491661\n",
      "Batch: 9\n",
      "Loss: 1.73991\n",
      "Batch: 10\n",
      "Loss: 1.7325654\n",
      "Batch: 11\n",
      "Loss: 1.7264568\n",
      "Batch: 12\n",
      "Loss: 1.7206893\n",
      "Batch: 13\n",
      "Loss: 1.7149061\n",
      "Batch: 14\n",
      "Loss: 1.7092019\n",
      "Batch: 15\n",
      "Loss: 1.7037178\n",
      "Batch: 16\n",
      "Loss: 1.6984788\n",
      "Batch: 17\n",
      "Loss: 1.693422\n",
      "Batch: 18\n",
      "Loss: 1.68846\n",
      "Batch: 19\n",
      "Loss: 1.6835389\n",
      "Batch: 20\n",
      "Loss: 1.6786492\n",
      "Batch: 21\n",
      "Loss: 1.6738027\n",
      "Batch: 22\n",
      "Loss: 1.6690087\n",
      "Batch: 23\n",
      "Loss: 1.6642683\n",
      "Batch: 24\n",
      "Loss: 1.6595743\n",
      "Batch: 25\n",
      "Loss: 1.6549157\n",
      "Batch: 26\n",
      "Loss: 1.6502811\n",
      "Batch: 27\n",
      "Loss: 1.6456624\n",
      "Batch: 28\n",
      "Loss: 1.641055\n",
      "Batch: 29\n",
      "Loss: 1.6364573\n",
      "Batch: 30\n",
      "Loss: 1.6318709\n",
      "Batch: 31\n",
      "Loss: 1.6272967\n",
      "Batch: 32\n",
      "Loss: 1.6227374\n",
      "Batch: 33\n",
      "Loss: 1.618194\n",
      "Batch: 34\n",
      "Loss: 1.6136677\n",
      "Batch: 35\n",
      "Loss: 1.6091578\n",
      "Batch: 36\n",
      "Loss: 1.6046641\n",
      "Batch: 37\n",
      "Loss: 1.6001855\n",
      "Batch: 38\n",
      "Loss: 1.5957208\n",
      "Batch: 39\n",
      "Loss: 1.5912693\n",
      "Batch: 40\n",
      "Loss: 1.5868299\n",
      "Batch: 41\n",
      "Loss: 1.5824023\n",
      "Batch: 42\n",
      "Loss: 1.5779872\n",
      "Batch: 43\n",
      "Loss: 1.5735847\n",
      "Batch: 44\n",
      "Loss: 1.5691955\n",
      "Batch: 45\n",
      "Loss: 1.5648202\n",
      "Batch: 46\n",
      "Loss: 1.5604581\n",
      "Batch: 47\n",
      "Loss: 1.5561103\n",
      "Batch: 48\n",
      "Loss: 1.551776\n",
      "Batch: 49\n",
      "Loss: 1.5474557\n",
      "Batch: 50\n",
      "Loss: 1.5431489\n",
      "Batch: 51\n",
      "Loss: 1.5388556\n",
      "Batch: 52\n",
      "Loss: 1.5345757\n",
      "Batch: 53\n",
      "Loss: 1.5303088\n",
      "Batch: 54\n",
      "Loss: 1.5260559\n",
      "Batch: 55\n",
      "Loss: 1.521816\n",
      "Batch: 56\n",
      "Loss: 1.5175892\n",
      "Batch: 57\n",
      "Loss: 1.5133762\n",
      "Batch: 58\n",
      "Loss: 1.5091763\n",
      "Batch: 59\n",
      "Loss: 1.50499\n",
      "Batch: 60\n",
      "Loss: 1.500817\n",
      "Batch: 61\n",
      "Loss: 1.4966577\n",
      "Batch: 62\n",
      "Loss: 1.4925116\n",
      "Batch: 63\n",
      "Loss: 1.4883791\n",
      "Batch: 64\n",
      "Loss: 1.4842596\n",
      "Batch: 65\n",
      "Loss: 1.4801533\n",
      "Batch: 66\n",
      "Loss: 1.4760605\n",
      "Batch: 67\n",
      "Loss: 1.4719807\n",
      "Batch: 68\n",
      "Loss: 1.4679137\n",
      "Batch: 69\n",
      "Loss: 1.46386\n",
      "Batch: 70\n",
      "Loss: 1.4598191\n",
      "Batch: 71\n",
      "Loss: 1.4557912\n",
      "Batch: 72\n",
      "Loss: 1.4517763\n",
      "Batch: 73\n",
      "Loss: 1.4477742\n",
      "Batch: 74\n",
      "Loss: 1.4437846\n",
      "Batch: 75\n",
      "Loss: 1.4398079\n",
      "Batch: 76\n",
      "Loss: 1.4358443\n",
      "Batch: 77\n",
      "Train accuracy: 0.9982972756410257\n",
      "Validation accuracy: 0.6698369565217391\n",
      "Loss: 2.4583118\n",
      "Batch: 0\n",
      "Loss: 1.9351004\n",
      "Batch: 1\n",
      "Loss: 1.6917096\n",
      "Batch: 2\n",
      "Loss: 1.5674249\n",
      "Batch: 3\n",
      "Loss: 1.5247016\n",
      "Batch: 4\n",
      "Loss: 1.4929019\n",
      "Batch: 5\n",
      "Loss: 1.4727852\n",
      "Batch: 6\n",
      "Loss: 1.4599686\n",
      "Batch: 7\n",
      "Loss: 1.4519944\n",
      "Batch: 8\n",
      "Loss: 1.444861\n",
      "Batch: 9\n",
      "Loss: 1.4370844\n",
      "Batch: 10\n",
      "Loss: 1.4295865\n",
      "Batch: 11\n",
      "Loss: 1.4233198\n",
      "Batch: 12\n",
      "Loss: 1.41837\n",
      "Batch: 13\n",
      "Loss: 1.414305\n",
      "Batch: 14\n",
      "Loss: 1.4105344\n",
      "Batch: 15\n",
      "Loss: 1.4066947\n",
      "Batch: 16\n",
      "Loss: 1.4028361\n",
      "Batch: 17\n",
      "Loss: 1.3991036\n",
      "Batch: 18\n",
      "Loss: 1.3955147\n",
      "Batch: 19\n",
      "Loss: 1.3920304\n",
      "Batch: 20\n",
      "Loss: 1.3886316\n",
      "Batch: 21\n",
      "Loss: 1.3853152\n",
      "Batch: 22\n",
      "Loss: 1.382062\n",
      "Batch: 23\n",
      "Loss: 1.3788428\n",
      "Batch: 24\n",
      "Loss: 1.375637\n",
      "Batch: 25\n",
      "Loss: 1.3724406\n",
      "Batch: 26\n",
      "Loss: 1.3692571\n",
      "Batch: 27\n",
      "Loss: 1.36609\n",
      "Batch: 28\n",
      "Loss: 1.3629397\n",
      "Batch: 29\n",
      "Loss: 1.3598043\n",
      "Batch: 30\n",
      "Loss: 1.3566797\n",
      "Batch: 31\n",
      "Loss: 1.3535639\n",
      "Batch: 32\n",
      "Loss: 1.3504541\n",
      "Batch: 33\n",
      "Loss: 1.3473488\n",
      "Batch: 34\n",
      "Loss: 1.3442473\n",
      "Batch: 35\n",
      "Loss: 1.341149\n",
      "Batch: 36\n",
      "Loss: 1.3380538\n",
      "Batch: 37\n",
      "Loss: 1.3349622\n",
      "Batch: 38\n",
      "Loss: 1.3318743\n",
      "Batch: 39\n",
      "Loss: 1.3287907\n",
      "Batch: 40\n",
      "Loss: 1.3257115\n",
      "Batch: 41\n",
      "Loss: 1.3226379\n",
      "Batch: 42\n",
      "Loss: 1.3195701\n",
      "Batch: 43\n",
      "Loss: 1.316508\n",
      "Batch: 44\n",
      "Loss: 1.3134528\n",
      "Batch: 45\n",
      "Loss: 1.3104043\n",
      "Batch: 46\n",
      "Loss: 1.3073628\n",
      "Batch: 47\n",
      "Loss: 1.3043282\n",
      "Batch: 48\n",
      "Loss: 1.3013008\n",
      "Batch: 49\n",
      "Loss: 1.2982804\n",
      "Batch: 50\n",
      "Loss: 1.2952669\n",
      "Batch: 51\n",
      "Loss: 1.2922606\n",
      "Batch: 52\n",
      "Loss: 1.2892611\n",
      "Batch: 53\n",
      "Loss: 1.2862688\n",
      "Batch: 54\n",
      "Loss: 1.2832835\n",
      "Batch: 55\n",
      "Loss: 1.2803055\n",
      "Batch: 56\n",
      "Loss: 1.2773348\n",
      "Batch: 57\n",
      "Loss: 1.2743714\n",
      "Batch: 58\n",
      "Loss: 1.2714154\n",
      "Batch: 59\n",
      "Loss: 1.2684668\n",
      "Batch: 60\n",
      "Loss: 1.2655255\n",
      "Batch: 61\n",
      "Loss: 1.262592\n",
      "Batch: 62\n",
      "Loss: 1.2596656\n",
      "Batch: 63\n",
      "Loss: 1.2567468\n",
      "Batch: 64\n",
      "Loss: 1.2538352\n",
      "Batch: 65\n",
      "Loss: 1.2509313\n",
      "Batch: 66\n",
      "Loss: 1.2480346\n",
      "Batch: 67\n",
      "Loss: 1.2451456\n",
      "Batch: 68\n",
      "Loss: 1.2422636\n",
      "Batch: 69\n",
      "Loss: 1.2393893\n",
      "Batch: 70\n",
      "Loss: 1.2365218\n",
      "Batch: 71\n",
      "Loss: 1.2336622\n",
      "Batch: 72\n",
      "Loss: 1.2308095\n",
      "Batch: 73\n",
      "Loss: 1.2279643\n",
      "Batch: 74\n",
      "Loss: 1.2251261\n",
      "Batch: 75\n",
      "Loss: 1.2222955\n",
      "Batch: 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2194717\n",
      "Batch: 77\n",
      "Train accuracy: 0.9955929487179487\n",
      "Validation accuracy: 0.681046195652174\n",
      "Loss: 2.047232\n",
      "Batch: 0\n",
      "Loss: 1.6528767\n",
      "Batch: 1\n",
      "Loss: 1.3625965\n",
      "Batch: 2\n",
      "Loss: 1.2733976\n",
      "Batch: 3\n",
      "Loss: 1.2483989\n",
      "Batch: 4\n",
      "Loss: 1.2347552\n",
      "Batch: 5\n",
      "Loss: 1.218029\n",
      "Batch: 6\n",
      "Loss: 1.2065902\n",
      "Batch: 7\n",
      "Loss: 1.198956\n",
      "Batch: 8\n",
      "Loss: 1.192144\n",
      "Batch: 9\n",
      "Loss: 1.1861367\n",
      "Batch: 10\n",
      "Loss: 1.1815007\n",
      "Batch: 11\n",
      "Loss: 1.1777263\n",
      "Batch: 12\n",
      "Loss: 1.1742325\n",
      "Batch: 13\n",
      "Loss: 1.1709801\n",
      "Batch: 14\n",
      "Loss: 1.1680155\n",
      "Batch: 15\n",
      "Loss: 1.1651844\n",
      "Batch: 16\n",
      "Loss: 1.1623319\n",
      "Batch: 17\n",
      "Loss: 1.1594843\n",
      "Batch: 18\n",
      "Loss: 1.1567261\n",
      "Batch: 19\n",
      "Loss: 1.154084\n",
      "Batch: 20\n",
      "Loss: 1.1515338\n",
      "Batch: 21\n",
      "Loss: 1.1490407\n",
      "Batch: 22\n",
      "Loss: 1.146579\n",
      "Batch: 23\n",
      "Loss: 1.144134\n",
      "Batch: 24\n",
      "Loss: 1.1416973\n",
      "Batch: 25\n",
      "Loss: 1.1392659\n",
      "Batch: 26\n",
      "Loss: 1.136837\n",
      "Batch: 27\n",
      "Loss: 1.1344097\n",
      "Batch: 28\n",
      "Loss: 1.1319836\n",
      "Batch: 29\n",
      "Loss: 1.1295587\n",
      "Batch: 30\n",
      "Loss: 1.1271353\n",
      "Batch: 31\n",
      "Loss: 1.1247138\n",
      "Batch: 32\n",
      "Loss: 1.1222942\n",
      "Batch: 33\n",
      "Loss: 1.1198772\n",
      "Batch: 34\n",
      "Loss: 1.1174628\n",
      "Batch: 35\n",
      "Loss: 1.1150512\n",
      "Batch: 36\n",
      "Loss: 1.1126425\n",
      "Batch: 37\n",
      "Loss: 1.1102363\n",
      "Batch: 38\n",
      "Loss: 1.1078328\n",
      "Batch: 39\n",
      "Loss: 1.1054322\n",
      "Batch: 40\n",
      "Loss: 1.1030343\n",
      "Batch: 41\n",
      "Loss: 1.1006393\n",
      "Batch: 42\n",
      "Loss: 1.098248\n",
      "Batch: 43\n",
      "Loss: 1.0958604\n",
      "Batch: 44\n",
      "Loss: 1.093477\n",
      "Batch: 45\n",
      "Loss: 1.0910981\n",
      "Batch: 46\n",
      "Loss: 1.0887237\n",
      "Batch: 47\n",
      "Loss: 1.086354\n",
      "Batch: 48\n",
      "Loss: 1.083989\n",
      "Batch: 49\n",
      "Loss: 1.0816288\n",
      "Batch: 50\n",
      "Loss: 1.079273\n",
      "Batch: 51\n",
      "Loss: 1.076922\n",
      "Batch: 52\n",
      "Loss: 1.0745755\n",
      "Batch: 53\n",
      "Loss: 1.0722342\n",
      "Batch: 54\n",
      "Loss: 1.069897\n",
      "Batch: 55\n",
      "Loss: 1.067565\n",
      "Batch: 56\n",
      "Loss: 1.0652382\n",
      "Batch: 57\n",
      "Loss: 1.0629164\n",
      "Batch: 58\n",
      "Loss: 1.0605997\n",
      "Batch: 59\n",
      "Loss: 1.0582882\n",
      "Batch: 60\n",
      "Loss: 1.0559818\n",
      "Batch: 61\n",
      "Loss: 1.0536807\n",
      "Batch: 62\n",
      "Loss: 1.0513848\n",
      "Batch: 63\n",
      "Loss: 1.0490936\n",
      "Batch: 64\n",
      "Loss: 1.0468079\n",
      "Batch: 65\n",
      "Loss: 1.0445275\n",
      "Batch: 66\n",
      "Loss: 1.0422521\n",
      "Batch: 67\n",
      "Loss: 1.0399818\n",
      "Batch: 68\n",
      "Loss: 1.0377167\n",
      "Batch: 69\n",
      "Loss: 1.0354568\n",
      "Batch: 70\n",
      "Loss: 1.0332019\n",
      "Batch: 71\n",
      "Loss: 1.0309523\n",
      "Batch: 72\n",
      "Loss: 1.0287076\n",
      "Batch: 73\n",
      "Loss: 1.0264683\n",
      "Batch: 74\n",
      "Loss: 1.0242338\n",
      "Batch: 75\n",
      "Loss: 1.0220046\n",
      "Batch: 76\n",
      "Loss: 1.0197803\n",
      "Batch: 77\n",
      "Train accuracy: 0.996895032051282\n",
      "Validation accuracy: 0.7027853260869565\n",
      "Loss: 1.6389763\n",
      "Batch: 0\n",
      "Loss: 1.3091729\n",
      "Batch: 1\n",
      "Loss: 1.1467144\n",
      "Batch: 2\n",
      "Loss: 1.0931153\n",
      "Batch: 3\n",
      "Loss: 1.0689673\n",
      "Batch: 4\n",
      "Loss: 1.0627166\n",
      "Batch: 5\n",
      "Loss: 1.04995\n",
      "Batch: 6\n",
      "Loss: 1.0389872\n",
      "Batch: 7\n",
      "Loss: 1.0329069\n",
      "Batch: 8\n",
      "Loss: 1.0289795\n",
      "Batch: 9\n",
      "Loss: 1.0255647\n",
      "Batch: 10\n",
      "Loss: 1.0222299\n",
      "Batch: 11\n",
      "Loss: 1.0190207\n",
      "Batch: 12\n",
      "Loss: 1.0160377\n",
      "Batch: 13\n",
      "Loss: 1.0133082\n",
      "Batch: 14\n",
      "Loss: 1.010803\n",
      "Batch: 15\n",
      "Loss: 1.0084656\n",
      "Batch: 16\n",
      "Loss: 1.0062324\n",
      "Batch: 17\n",
      "Loss: 1.004049\n",
      "Batch: 18\n",
      "Loss: 1.0018888\n",
      "Batch: 19\n",
      "Loss: 0.99975806\n",
      "Batch: 20\n",
      "Loss: 0.99768215\n",
      "Batch: 21\n",
      "Loss: 0.9956751\n",
      "Batch: 22\n",
      "Loss: 0.9937209\n",
      "Batch: 23\n",
      "Loss: 0.9917778\n",
      "Batch: 24\n",
      "Loss: 0.989817\n",
      "Batch: 25\n",
      "Loss: 0.98784417\n",
      "Batch: 26\n",
      "Loss: 0.9858779\n",
      "Batch: 27\n",
      "Loss: 0.9839246\n",
      "Batch: 28\n",
      "Loss: 0.9819781\n",
      "Batch: 29\n",
      "Loss: 0.98003113\n",
      "Batch: 30\n",
      "Loss: 0.9780824\n",
      "Batch: 31\n",
      "Loss: 0.9761351\n",
      "Batch: 32\n",
      "Loss: 0.974192\n",
      "Batch: 33\n",
      "Loss: 0.97225404\n",
      "Batch: 34\n",
      "Loss: 0.97032\n",
      "Batch: 35\n",
      "Loss: 0.96838796\n",
      "Batch: 36\n",
      "Loss: 0.96645665\n",
      "Batch: 37\n",
      "Loss: 0.96452475\n",
      "Batch: 38\n",
      "Loss: 0.9625922\n",
      "Batch: 39\n",
      "Loss: 0.960659\n",
      "Batch: 40\n",
      "Loss: 0.9587263\n",
      "Batch: 41\n",
      "Loss: 0.9567944\n",
      "Batch: 42\n",
      "Loss: 0.9548641\n",
      "Batch: 43\n",
      "Loss: 0.9529358\n",
      "Batch: 44\n",
      "Loss: 0.9510101\n",
      "Batch: 45\n",
      "Loss: 0.9490872\n",
      "Batch: 46\n",
      "Loss: 0.9471673\n",
      "Batch: 47\n",
      "Loss: 0.94525045\n",
      "Batch: 48\n",
      "Loss: 0.94333667\n",
      "Batch: 49\n",
      "Loss: 0.94142616\n",
      "Batch: 50\n",
      "Loss: 0.939519\n",
      "Batch: 51\n",
      "Loss: 0.93761516\n",
      "Batch: 52\n",
      "Loss: 0.935715\n",
      "Batch: 53\n",
      "Loss: 0.93381834\n",
      "Batch: 54\n",
      "Loss: 0.93192554\n",
      "Batch: 55\n",
      "Loss: 0.9300364\n",
      "Batch: 56\n",
      "Loss: 0.928151\n",
      "Batch: 57\n",
      "Loss: 0.92626953\n",
      "Batch: 58\n",
      "Loss: 0.9243915\n",
      "Batch: 59\n",
      "Loss: 0.9225175\n",
      "Batch: 60\n",
      "Loss: 0.9206473\n",
      "Batch: 61\n",
      "Loss: 0.918781\n",
      "Batch: 62\n",
      "Loss: 0.9169186\n",
      "Batch: 63\n",
      "Loss: 0.91506016\n",
      "Batch: 64\n",
      "Loss: 0.9132057\n",
      "Batch: 65\n",
      "Loss: 0.911355\n",
      "Batch: 66\n",
      "Loss: 0.90950793\n",
      "Batch: 67\n",
      "Loss: 0.9076646\n",
      "Batch: 68\n",
      "Loss: 0.90582526\n",
      "Batch: 69\n",
      "Loss: 0.90398973\n",
      "Batch: 70\n",
      "Loss: 0.90215814\n",
      "Batch: 71\n",
      "Loss: 0.90033036\n",
      "Batch: 72\n",
      "Loss: 0.89850634\n",
      "Batch: 73\n",
      "Loss: 0.89668614\n",
      "Batch: 74\n",
      "Loss: 0.89486974\n",
      "Batch: 75\n",
      "Loss: 0.893057\n",
      "Batch: 76\n",
      "Loss: 0.89124817\n",
      "Batch: 77\n",
      "Train accuracy: 0.9984975961538461\n",
      "Validation accuracy: 0.6793478260869565\n",
      "Loss: 1.4110727\n",
      "Batch: 0\n",
      "Loss: 1.0909864\n",
      "Batch: 1\n",
      "Loss: 0.9459318\n",
      "Batch: 2\n",
      "Loss: 0.9242387\n",
      "Batch: 3\n",
      "Loss: 0.9006247\n",
      "Batch: 4\n",
      "Loss: 0.890997\n",
      "Batch: 5\n",
      "Loss: 0.88526475\n",
      "Batch: 6\n",
      "Loss: 0.8785959\n",
      "Batch: 7\n",
      "Loss: 0.8727845\n",
      "Batch: 8\n",
      "Loss: 0.86855227\n",
      "Batch: 9\n",
      "Loss: 0.865387\n",
      "Batch: 10\n",
      "Loss: 0.8627663\n",
      "Batch: 11\n",
      "Loss: 0.86040413\n",
      "Batch: 12\n",
      "Loss: 0.8581778\n",
      "Batch: 13\n",
      "Loss: 0.85605776\n",
      "Batch: 14\n",
      "Loss: 0.85403776\n",
      "Batch: 15\n",
      "Loss: 0.8521053\n",
      "Batch: 16\n",
      "Loss: 0.85024947\n",
      "Batch: 17\n",
      "Loss: 0.8484558\n",
      "Batch: 18\n",
      "Loss: 0.84670806\n",
      "Batch: 19\n",
      "Loss: 0.8449929\n",
      "Batch: 20\n",
      "Loss: 0.8433\n",
      "Batch: 21\n",
      "Loss: 0.84162116\n",
      "Batch: 22\n",
      "Loss: 0.8399519\n",
      "Batch: 23\n",
      "Loss: 0.83828837\n",
      "Batch: 24\n",
      "Loss: 0.8366289\n",
      "Batch: 25\n",
      "Loss: 0.8349723\n",
      "Batch: 26\n",
      "Loss: 0.83331776\n",
      "Batch: 27\n",
      "Loss: 0.8316651\n",
      "Batch: 28\n",
      "Loss: 0.8300141\n",
      "Batch: 29\n",
      "Loss: 0.8283647\n",
      "Batch: 30\n",
      "Loss: 0.826717\n",
      "Batch: 31\n",
      "Loss: 0.82507116\n",
      "Batch: 32\n",
      "Loss: 0.8234272\n",
      "Batch: 33\n",
      "Loss: 0.82178503\n",
      "Batch: 34\n",
      "Loss: 0.8201449\n",
      "Batch: 35\n",
      "Loss: 0.8185068\n",
      "Batch: 36\n",
      "Loss: 0.816871\n",
      "Batch: 37\n",
      "Loss: 0.8152374\n",
      "Batch: 38\n",
      "Loss: 0.81360614\n",
      "Batch: 39\n",
      "Loss: 0.81197727\n",
      "Batch: 40\n",
      "Loss: 0.81035066\n",
      "Batch: 41\n",
      "Loss: 0.8087264\n",
      "Batch: 42\n",
      "Loss: 0.80710477\n",
      "Batch: 43\n",
      "Loss: 0.8054857\n",
      "Batch: 44\n",
      "Loss: 0.8038692\n",
      "Batch: 45\n",
      "Loss: 0.8022554\n",
      "Batch: 46\n",
      "Loss: 0.80064434\n",
      "Batch: 47\n",
      "Loss: 0.79903597\n",
      "Batch: 48\n",
      "Loss: 0.79743046\n",
      "Batch: 49\n",
      "Loss: 0.7958277\n",
      "Batch: 50\n",
      "Loss: 0.79422784\n",
      "Batch: 51\n",
      "Loss: 0.7926311\n",
      "Batch: 52\n",
      "Loss: 0.79103696\n",
      "Batch: 53\n",
      "Loss: 0.789446\n",
      "Batch: 54\n",
      "Loss: 0.7878579\n",
      "Batch: 55\n",
      "Loss: 0.786273\n",
      "Batch: 56\n",
      "Loss: 0.784691\n",
      "Batch: 57\n",
      "Loss: 0.783112\n",
      "Batch: 58\n",
      "Loss: 0.7815361\n",
      "Batch: 59\n",
      "Loss: 0.7799631\n",
      "Batch: 60\n",
      "Loss: 0.7783933\n",
      "Batch: 61\n",
      "Loss: 0.7768266\n",
      "Batch: 62\n",
      "Loss: 0.77526295\n",
      "Batch: 63\n",
      "Loss: 0.7737023\n",
      "Batch: 64\n",
      "Loss: 0.77214473\n",
      "Batch: 65\n",
      "Loss: 0.7705903\n",
      "Batch: 66\n",
      "Loss: 0.7690388\n",
      "Batch: 67\n",
      "Loss: 0.7674905\n",
      "Batch: 68\n",
      "Loss: 0.7659451\n",
      "Batch: 69\n",
      "Loss: 0.7644027\n",
      "Batch: 70\n",
      "Loss: 0.7628635\n",
      "Batch: 71\n",
      "Loss: 0.76132727\n",
      "Batch: 72\n",
      "Loss: 0.7597941\n",
      "Batch: 73\n",
      "Loss: 0.7582639\n",
      "Batch: 74\n",
      "Loss: 0.7567368\n",
      "Batch: 75\n",
      "Loss: 0.75521266\n",
      "Batch: 76\n",
      "Loss: 0.7536915\n",
      "Batch: 77\n",
      "Train accuracy: 0.999198717948718\n",
      "Validation accuracy: 0.6820652173913043\n",
      "Loss: 1.3868016\n",
      "Batch: 0\n",
      "Loss: 1.0358479\n",
      "Batch: 1\n",
      "Loss: 0.8219839\n",
      "Batch: 2\n",
      "Loss: 0.8144639\n",
      "Batch: 3\n",
      "Loss: 0.7964127\n",
      "Batch: 4\n",
      "Loss: 0.78090435\n",
      "Batch: 5\n",
      "Loss: 0.7715162\n",
      "Batch: 6\n",
      "Loss: 0.76583576\n",
      "Batch: 7\n",
      "Loss: 0.76113707\n",
      "Batch: 8\n",
      "Loss: 0.7564961\n",
      "Batch: 9\n",
      "Loss: 0.7524457\n",
      "Batch: 10\n",
      "Loss: 0.749241\n",
      "Batch: 11\n",
      "Loss: 0.746696\n",
      "Batch: 12\n",
      "Loss: 0.74451256\n",
      "Batch: 13\n",
      "Loss: 0.74247783\n",
      "Batch: 14\n",
      "Loss: 0.7405241\n",
      "Batch: 15\n",
      "Loss: 0.73869\n",
      "Batch: 16\n",
      "Loss: 0.73703057\n",
      "Batch: 17\n",
      "Loss: 0.7355357\n",
      "Batch: 18\n",
      "Loss: 0.7341207\n",
      "Batch: 19\n",
      "Loss: 0.73270786\n",
      "Batch: 20\n",
      "Loss: 0.73128664\n",
      "Batch: 21\n",
      "Loss: 0.7298837\n",
      "Batch: 22\n",
      "Loss: 0.72850144\n",
      "Batch: 23\n",
      "Loss: 0.72712314\n",
      "Batch: 24\n",
      "Loss: 0.72574264\n",
      "Batch: 25\n",
      "Loss: 0.72436726\n",
      "Batch: 26\n",
      "Loss: 0.7230025\n",
      "Batch: 27\n",
      "Loss: 0.7216453\n",
      "Batch: 28\n",
      "Loss: 0.72029066\n",
      "Batch: 29\n",
      "Loss: 0.71893543\n",
      "Batch: 30\n",
      "Loss: 0.71757877\n",
      "Batch: 31\n",
      "Loss: 0.71622086\n",
      "Batch: 32\n",
      "Loss: 0.714862\n",
      "Batch: 33\n",
      "Loss: 0.7135021\n",
      "Batch: 34\n",
      "Loss: 0.7121406\n",
      "Batch: 35\n",
      "Loss: 0.71077716\n",
      "Batch: 36\n",
      "Loss: 0.7094109\n",
      "Batch: 37\n",
      "Loss: 0.70804167\n",
      "Batch: 38\n",
      "Loss: 0.70666975\n",
      "Batch: 39\n",
      "Loss: 0.70529586\n",
      "Batch: 40\n",
      "Loss: 0.7039206\n",
      "Batch: 41\n",
      "Loss: 0.70254457\n",
      "Batch: 42\n",
      "Loss: 0.7011685\n",
      "Batch: 43\n",
      "Loss: 0.6997928\n",
      "Batch: 44\n",
      "Loss: 0.698418\n",
      "Batch: 45\n",
      "Loss: 0.6970443\n",
      "Batch: 46\n",
      "Loss: 0.6956722\n",
      "Batch: 47\n",
      "Loss: 0.6943015\n",
      "Batch: 48\n",
      "Loss: 0.69293237\n",
      "Batch: 49\n",
      "Loss: 0.69156504\n",
      "Batch: 50\n",
      "Loss: 0.69019955\n",
      "Batch: 51\n",
      "Loss: 0.6888358\n",
      "Batch: 52\n",
      "Loss: 0.68747413\n",
      "Batch: 53\n",
      "Loss: 0.6861147\n",
      "Batch: 54\n",
      "Loss: 0.68475765\n",
      "Batch: 55\n",
      "Loss: 0.68340284\n",
      "Batch: 56\n",
      "Loss: 0.6820505\n",
      "Batch: 57\n",
      "Loss: 0.6807009\n",
      "Batch: 58\n",
      "Loss: 0.67935383\n",
      "Batch: 59\n",
      "Loss: 0.67800945\n",
      "Batch: 60\n",
      "Loss: 0.6766679\n",
      "Batch: 61\n",
      "Loss: 0.67532897\n",
      "Batch: 62\n",
      "Loss: 0.6739928\n",
      "Batch: 63\n",
      "Loss: 0.6726593\n",
      "Batch: 64\n",
      "Loss: 0.67132837\n",
      "Batch: 65\n",
      "Loss: 0.67000026\n",
      "Batch: 66\n",
      "Loss: 0.66867477\n",
      "Batch: 67\n",
      "Loss: 0.667352\n",
      "Batch: 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6660321\n",
      "Batch: 69\n",
      "Loss: 0.66471463\n",
      "Batch: 70\n",
      "Loss: 0.66340005\n",
      "Batch: 71\n",
      "Loss: 0.66208804\n",
      "Batch: 72\n",
      "Loss: 0.6607786\n",
      "Batch: 73\n",
      "Loss: 0.6594721\n",
      "Batch: 74\n",
      "Loss: 0.65816814\n",
      "Batch: 75\n",
      "Loss: 0.6568668\n",
      "Batch: 76\n",
      "Loss: 0.6555682\n",
      "Batch: 77\n",
      "Train accuracy: 0.9983974358974359\n",
      "Validation accuracy: 0.7088994565217391\n",
      "Loss: 1.185165\n",
      "Batch: 0\n",
      "Loss: 0.92216676\n",
      "Batch: 1\n",
      "Loss: 0.74742746\n",
      "Batch: 2\n",
      "Loss: 0.7317742\n",
      "Batch: 3\n",
      "Loss: 0.7031685\n",
      "Batch: 4\n",
      "Loss: 0.68872476\n",
      "Batch: 5\n",
      "Loss: 0.68291694\n",
      "Batch: 6\n",
      "Loss: 0.6806782\n",
      "Batch: 7\n",
      "Loss: 0.6785071\n",
      "Batch: 8\n",
      "Loss: 0.674973\n",
      "Batch: 9\n",
      "Loss: 0.67100304\n",
      "Batch: 10\n",
      "Loss: 0.6676639\n",
      "Batch: 11\n",
      "Loss: 0.665144\n",
      "Batch: 12\n",
      "Loss: 0.66321176\n",
      "Batch: 13\n",
      "Loss: 0.6616208\n",
      "Batch: 14\n",
      "Loss: 0.6601999\n",
      "Batch: 15\n",
      "Loss: 0.6588511\n",
      "Batch: 16\n",
      "Loss: 0.657523\n",
      "Batch: 17\n",
      "Loss: 0.6561891\n",
      "Batch: 18\n",
      "Loss: 0.6548443\n",
      "Batch: 19\n",
      "Loss: 0.6534978\n",
      "Batch: 20\n",
      "Loss: 0.65216553\n",
      "Batch: 21\n",
      "Loss: 0.6508656\n",
      "Batch: 22\n",
      "Loss: 0.6496026\n",
      "Batch: 23\n",
      "Loss: 0.6483559\n",
      "Batch: 24\n",
      "Loss: 0.6471102\n",
      "Batch: 25\n",
      "Loss: 0.6458692\n",
      "Batch: 26\n",
      "Loss: 0.64464\n",
      "Batch: 27\n",
      "Loss: 0.6434247\n",
      "Batch: 28\n",
      "Loss: 0.64222014\n",
      "Batch: 29\n",
      "Loss: 0.64102155\n",
      "Batch: 30\n",
      "Loss: 0.6398255\n",
      "Batch: 31\n",
      "Loss: 0.63862973\n",
      "Batch: 32\n",
      "Loss: 0.6374339\n",
      "Batch: 33\n",
      "Loss: 0.63623816\n",
      "Batch: 34\n",
      "Loss: 0.6350436\n",
      "Batch: 35\n",
      "Loss: 0.6338509\n",
      "Batch: 36\n",
      "Loss: 0.63266057\n",
      "Batch: 37\n",
      "Loss: 0.6314725\n",
      "Batch: 38\n",
      "Loss: 0.63028574\n",
      "Batch: 39\n",
      "Loss: 0.62909985\n",
      "Batch: 40\n",
      "Loss: 0.62791455\n",
      "Batch: 41\n",
      "Loss: 0.62672985\n",
      "Batch: 42\n",
      "Loss: 0.62554675\n",
      "Batch: 43\n",
      "Loss: 0.62436545\n",
      "Batch: 44\n",
      "Loss: 0.62318647\n",
      "Batch: 45\n",
      "Loss: 0.62200963\n",
      "Batch: 46\n",
      "Loss: 0.62083495\n",
      "Batch: 47\n",
      "Loss: 0.6196621\n",
      "Batch: 48\n",
      "Loss: 0.61849123\n",
      "Batch: 49\n",
      "Loss: 0.6173219\n",
      "Batch: 50\n",
      "Loss: 0.61615443\n",
      "Batch: 51\n",
      "Loss: 0.61498904\n",
      "Batch: 52\n",
      "Loss: 0.61382556\n",
      "Batch: 53\n",
      "Loss: 0.6126644\n",
      "Batch: 54\n",
      "Loss: 0.61150557\n",
      "Batch: 55\n",
      "Loss: 0.61034894\n",
      "Batch: 56\n",
      "Loss: 0.60919446\n",
      "Batch: 57\n",
      "Loss: 0.6080422\n",
      "Batch: 58\n",
      "Loss: 0.60689217\n",
      "Batch: 59\n",
      "Loss: 0.6057441\n",
      "Batch: 60\n",
      "Loss: 0.60459834\n",
      "Batch: 61\n",
      "Loss: 0.6034547\n",
      "Batch: 62\n",
      "Loss: 0.60231334\n",
      "Batch: 63\n",
      "Loss: 0.60117424\n",
      "Batch: 64\n",
      "Loss: 0.60003734\n",
      "Batch: 65\n",
      "Loss: 0.5989028\n",
      "Batch: 66\n",
      "Loss: 0.5977704\n",
      "Batch: 67\n",
      "Loss: 0.59664017\n",
      "Batch: 68\n",
      "Loss: 0.5955123\n",
      "Batch: 69\n",
      "Loss: 0.5943864\n",
      "Batch: 70\n",
      "Loss: 0.59326273\n",
      "Batch: 71\n",
      "Loss: 0.59214133\n",
      "Batch: 72\n",
      "Loss: 0.591022\n",
      "Batch: 73\n",
      "Loss: 0.589905\n",
      "Batch: 74\n",
      "Loss: 0.5887903\n",
      "Batch: 75\n",
      "Loss: 0.5876776\n",
      "Batch: 76\n",
      "Loss: 0.5865672\n",
      "Batch: 77\n",
      "Train accuracy: 0.9988982371794872\n",
      "Validation accuracy: 0.7068614130434783\n",
      "Loss: 1.3137329\n",
      "Batch: 0\n",
      "Loss: 0.8412391\n",
      "Batch: 1\n",
      "Loss: 0.68440145\n",
      "Batch: 2\n",
      "Loss: 0.6211965\n",
      "Batch: 3\n",
      "Loss: 0.6081442\n",
      "Batch: 4\n",
      "Loss: 0.60226697\n",
      "Batch: 5\n",
      "Loss: 0.59443396\n",
      "Batch: 6\n",
      "Loss: 0.58770174\n",
      "Batch: 7\n",
      "Loss: 0.5832158\n",
      "Batch: 8\n",
      "Loss: 0.58011526\n",
      "Batch: 9\n",
      "Loss: 0.5774893\n",
      "Batch: 10\n",
      "Loss: 0.5752469\n",
      "Batch: 11\n",
      "Loss: 0.5734676\n",
      "Batch: 12\n",
      "Loss: 0.5719652\n",
      "Batch: 13\n",
      "Loss: 0.5705218\n",
      "Batch: 14\n",
      "Loss: 0.56911635\n",
      "Batch: 15\n",
      "Loss: 0.56782603\n",
      "Batch: 16\n",
      "Loss: 0.56666505\n",
      "Batch: 17\n",
      "Loss: 0.56557924\n",
      "Batch: 18\n",
      "Loss: 0.5645136\n",
      "Batch: 19\n",
      "Loss: 0.56345206\n",
      "Batch: 20\n",
      "Loss: 0.56240505\n",
      "Batch: 21\n",
      "Loss: 0.56138355\n",
      "Batch: 22\n",
      "Loss: 0.5603892\n",
      "Batch: 23\n",
      "Loss: 0.55941635\n",
      "Batch: 24\n",
      "Loss: 0.5584558\n",
      "Batch: 25\n",
      "Loss: 0.55749923\n",
      "Batch: 26\n",
      "Loss: 0.55653954\n",
      "Batch: 27\n",
      "Loss: 0.555572\n",
      "Batch: 28\n",
      "Loss: 0.554595\n",
      "Batch: 29\n",
      "Loss: 0.5536081\n",
      "Batch: 30\n",
      "Loss: 0.552613\n",
      "Batch: 31\n",
      "Loss: 0.5516114\n",
      "Batch: 32\n",
      "Loss: 0.55060554\n",
      "Batch: 33\n",
      "Loss: 0.54959667\n",
      "Batch: 34\n",
      "Loss: 0.548586\n",
      "Batch: 35\n",
      "Loss: 0.54757386\n",
      "Batch: 36\n",
      "Loss: 0.5465605\n",
      "Batch: 37\n",
      "Loss: 0.5455459\n",
      "Batch: 38\n",
      "Loss: 0.5445298\n",
      "Batch: 39\n",
      "Loss: 0.5435124\n",
      "Batch: 40\n",
      "Loss: 0.54249394\n",
      "Batch: 41\n",
      "Loss: 0.5414747\n",
      "Batch: 42\n",
      "Loss: 0.54045504\n",
      "Batch: 43\n",
      "Loss: 0.5394353\n",
      "Batch: 44\n",
      "Loss: 0.5384159\n",
      "Batch: 45\n",
      "Loss: 0.5373973\n",
      "Batch: 46\n",
      "Loss: 0.53637964\n",
      "Batch: 47\n",
      "Loss: 0.53536314\n",
      "Batch: 48\n",
      "Loss: 0.53434795\n",
      "Batch: 49\n",
      "Loss: 0.53333426\n",
      "Batch: 50\n",
      "Loss: 0.53232193\n",
      "Batch: 51\n",
      "Loss: 0.531311\n",
      "Batch: 52\n",
      "Loss: 0.53030163\n",
      "Batch: 53\n",
      "Loss: 0.52929384\n",
      "Batch: 54\n",
      "Loss: 0.5282874\n",
      "Batch: 55\n",
      "Loss: 0.5272827\n",
      "Batch: 56\n",
      "Loss: 0.5262796\n",
      "Batch: 57\n",
      "Loss: 0.5252782\n",
      "Batch: 58\n",
      "Loss: 0.5242784\n",
      "Batch: 59\n",
      "Loss: 0.5232806\n",
      "Batch: 60\n",
      "Loss: 0.5222845\n",
      "Batch: 61\n",
      "Loss: 0.5212903\n",
      "Batch: 62\n",
      "Loss: 0.52029806\n",
      "Batch: 63\n",
      "Loss: 0.5193077\n",
      "Batch: 64\n",
      "Loss: 0.51831925\n",
      "Batch: 65\n",
      "Loss: 0.5173329\n",
      "Batch: 66\n",
      "Loss: 0.5163484\n",
      "Batch: 67\n",
      "Loss: 0.515366\n",
      "Batch: 68\n",
      "Loss: 0.5143854\n",
      "Batch: 69\n",
      "Loss: 0.5134068\n",
      "Batch: 70\n",
      "Loss: 0.51243013\n",
      "Batch: 71\n",
      "Loss: 0.5114554\n",
      "Batch: 72\n",
      "Loss: 0.5104826\n",
      "Batch: 73\n",
      "Loss: 0.50951177\n",
      "Batch: 74\n",
      "Loss: 0.50854284\n",
      "Batch: 75\n",
      "Loss: 0.50757575\n",
      "Batch: 76\n",
      "Loss: 0.5066107\n",
      "Batch: 77\n",
      "Train accuracy: 0.9987980769230769\n",
      "Validation accuracy: 0.7238451086956522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.7238451086956522"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9978966346153846\n",
      "Validation accuracy: 0.6525135869565217\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9941907051282052\n",
      "Validation accuracy: 0.6443614130434783\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9982972756410257\n",
      "Validation accuracy: 0.6436820652173914\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9995993589743589\n",
      "Validation accuracy: 0.6161684782608695\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9987980769230769\n",
      "Validation accuracy: 0.618546195652174\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9981971153846154\n",
      "Validation accuracy: 0.6063179347826086\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9989983974358975\n",
      "Validation accuracy: 0.592391304347826\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9990985576923077\n",
      "Validation accuracy: 0.5764266304347826\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9990985576923077\n",
      "Validation accuracy: 0.6569293478260869\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9995993589743589\n",
      "Validation accuracy: 0.6348505434782609\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5863381410256411\n",
      "Validation accuracy: 0.5678763440860215\n",
      "Epoch: 1\n",
      "Train accuracy: 0.7123397435897436\n",
      "Validation accuracy: 0.5799731182795699\n",
      "Epoch: 2\n",
      "Train accuracy: 0.6151842948717948\n",
      "Validation accuracy: 0.6236559139784946\n",
      "Epoch: 3\n",
      "Train accuracy: 0.5986578525641025\n",
      "Validation accuracy: 0.5362903225806451\n",
      "Epoch: 4\n",
      "Train accuracy: 0.5846354166666666\n",
      "Validation accuracy: 0.5591397849462365\n",
      "Epoch: 5\n",
      "Train accuracy: 0.6080729166666666\n",
      "Validation accuracy: 0.5735887096774194\n",
      "Epoch: 6\n",
      "Train accuracy: 0.6676682692307693\n",
      "Validation accuracy: 0.5944220430107527\n",
      "Epoch: 7\n",
      "Train accuracy: 0.5791266025641025\n",
      "Validation accuracy: 0.5934139784946236\n",
      "Epoch: 8\n",
      "Train accuracy: 0.5738181089743589\n",
      "Validation accuracy: 0.5618279569892473\n",
      "Epoch: 9\n",
      "Train accuracy: 0.6372195512820513\n",
      "Validation accuracy: 0.5181451612903226\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9870793269230769\n",
      "Validation accuracy: 0.5903532608695652\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9853766025641025\n",
      "Validation accuracy: 0.6114130434782609\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9921875\n",
      "Validation accuracy: 0.6202445652173914\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9926883012820513\n",
      "Validation accuracy: 0.6069972826086957\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9914863782051282\n",
      "Validation accuracy: 0.6443614130434783\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9889823717948718\n",
      "Validation accuracy: 0.6239809782608695\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9885817307692307\n",
      "Validation accuracy: 0.6086956521739131\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9867788461538461\n",
      "Validation accuracy: 0.6005434782608695\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9910857371794872\n",
      "Validation accuracy: 0.5760869565217391\n",
      "Epoch: 9\n",
      "Train accuracy: 0.991386217948718\n",
      "Validation accuracy: 0.5940896739130435\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9870793269230769\n",
      "Validation accuracy: 0.5621639784946236\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9954927884615384\n",
      "Validation accuracy: 0.5826612903225806\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9946915064102564\n",
      "Validation accuracy: 0.5631720430107527\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9954927884615384\n",
      "Validation accuracy: 0.5577956989247311\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9945913461538461\n",
      "Validation accuracy: 0.5131048387096774\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9905849358974359\n",
      "Validation accuracy: 0.5661962365591398\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9922876602564102\n",
      "Validation accuracy: 0.5433467741935484\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9985977564102564\n",
      "Validation accuracy: 0.5389784946236559\n",
      "Epoch: 8\n",
      "Train accuracy: 0.992988782051282\n",
      "Validation accuracy: 0.5510752688172043\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9961939102564102\n",
      "Validation accuracy: 0.5665322580645161\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9393028846153846\n",
      "Validation accuracy: 0.6141304347826086\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9785657051282052\n",
      "Validation accuracy: 0.5838994565217391\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9733573717948718\n",
      "Validation accuracy: 0.5737092391304348\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9655448717948718\n",
      "Validation accuracy: 0.5625\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9700520833333334\n",
      "Validation accuracy: 0.5237771739130435\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9696514423076923\n",
      "Validation accuracy: 0.5295516304347826\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9708533653846154\n",
      "Validation accuracy: 0.5411005434782609\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9504206730769231\n",
      "Validation accuracy: 0.5502717391304348\n",
      "Epoch: 8\n",
      "Train accuracy: 0.952323717948718\n",
      "Validation accuracy: 0.5254755434782609\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9672475961538461\n",
      "Validation accuracy: 0.5641983695652174\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5471754807692307\n",
      "Validation accuracy: 0.5458559782608695\n",
      "Epoch: 1\n",
      "Train accuracy: 0.5471754807692307\n",
      "Validation accuracy: 0.5373641304347826\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5360576923076923\n",
      "Validation accuracy: 0.5944293478260869\n",
      "Epoch: 3\n",
      "Train accuracy: 0.5428685897435898\n",
      "Validation accuracy: 0.5771059782608695\n",
      "Epoch: 4\n",
      "Train accuracy: 0.5724158653846154\n",
      "Validation accuracy: 0.5020380434782609\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5609975961538461\n",
      "Validation accuracy: 0.5489130434782609\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5323517628205128\n",
      "Validation accuracy: 0.563858695652174\n",
      "Epoch: 7\n",
      "Train accuracy: 0.5392628205128205\n",
      "Validation accuracy: 0.5040760869565217\n",
      "Epoch: 8\n",
      "Train accuracy: 0.5340544871794872\n",
      "Validation accuracy: 0.5587635869565217\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5307491987179487\n",
      "Validation accuracy: 0.48029891304347827\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9391025641025641\n",
      "Validation accuracy: 0.5740489130434783\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9741586538461539\n",
      "Validation accuracy: 0.5974864130434783\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9536258012820513\n",
      "Validation accuracy: 0.5557065217391305\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9563301282051282\n",
      "Validation accuracy: 0.5604619565217391\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9185697115384616\n",
      "Validation accuracy: 0.5618206521739131\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9185697115384616\n",
      "Validation accuracy: 0.5451766304347826\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9239783653846154\n",
      "Validation accuracy: 0.5499320652173914\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9274839743589743\n",
      "Validation accuracy: 0.5747282608695652\n",
      "Epoch: 8\n",
      "Train accuracy: 0.8977363782051282\n",
      "Validation accuracy: 0.576766304347826\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9219751602564102\n",
      "Validation accuracy: 0.5985054347826086\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.453125\n",
      "Validation accuracy: 0.4986413043478261\n",
      "Epoch: 1\n",
      "Train accuracy: 0.6650641025641025\n",
      "Validation accuracy: 0.48641304347826086\n",
      "Epoch: 2\n",
      "Train accuracy: 0.6834935897435898\n",
      "Validation accuracy: 0.6620244565217391\n",
      "Epoch: 3\n",
      "Train accuracy: 0.8709935897435898\n",
      "Validation accuracy: 0.6779891304347826\n",
      "Epoch: 4\n",
      "Train accuracy: 0.8349358974358975\n",
      "Validation accuracy: 0.6735733695652174\n",
      "Epoch: 5\n",
      "Train accuracy: 0.8560697115384616\n",
      "Validation accuracy: 0.7092391304347826\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9166666666666666\n",
      "Validation accuracy: 0.7068614130434783\n",
      "Epoch: 7\n",
      "Train accuracy: 0.8345352564102564\n",
      "Validation accuracy: 0.7238451086956522\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9306891025641025\n",
      "Validation accuracy: 0.7190896739130435\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9259815705128205\n",
      "Validation accuracy: 0.7007472826086957\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5134214743589743\n",
      "Validation accuracy: 0.4891304347826087\n",
      "Epoch: 1\n",
      "Train accuracy: 0.5199318910256411\n",
      "Validation accuracy: 0.477241847826087\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5236378205128205\n",
      "Validation accuracy: 0.49660326086956524\n",
      "Epoch: 3\n",
      "Train accuracy: 0.5244391025641025\n",
      "Validation accuracy: 0.5207201086956522\n",
      "Epoch: 4\n",
      "Train accuracy: 0.5495793269230769\n",
      "Validation accuracy: 0.5230978260869565\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5876402243589743\n",
      "Validation accuracy: 0.5329483695652174\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5581931089743589\n",
      "Validation accuracy: 0.5550271739130435\n",
      "Epoch: 7\n",
      "Train accuracy: 0.5174278846153846\n",
      "Validation accuracy: 0.47927989130434784\n",
      "Epoch: 8\n",
      "Train accuracy: 0.5625\n",
      "Validation accuracy: 0.5146059782608695\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5220352564102564\n",
      "Validation accuracy: 0.5067934782608695\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.8360376602564102\n",
      "Validation accuracy: 0.5567876344086021\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9791666666666666\n",
      "Validation accuracy: 0.5057123655913979\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9772636217948718\n",
      "Validation accuracy: 0.5493951612903226\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9857772435897436\n",
      "Validation accuracy: 0.5352822580645161\n",
      "Epoch: 4\n",
      "Train accuracy: 0.8619791666666666\n",
      "Validation accuracy: 0.5628360215053764\n",
      "Epoch: 5\n",
      "Train accuracy: 0.6946113782051282\n",
      "Validation accuracy: 0.5047043010752689\n",
      "Epoch: 6\n",
      "Train accuracy: 0.6763822115384616\n",
      "Validation accuracy: 0.5070564516129032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Train accuracy: 0.6274038461538461\n",
      "Validation accuracy: 0.5013440860215054\n",
      "Epoch: 8\n",
      "Train accuracy: 0.5728165064102564\n",
      "Validation accuracy: 0.5184811827956989\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5007011217948718\n",
      "Validation accuracy: 0.4885752688172043\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.7862580128205128\n",
      "Validation accuracy: 0.49422554347826086\n",
      "Epoch: 1\n",
      "Train accuracy: 0.40625\n",
      "Validation accuracy: 0.5061141304347826\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5625\n",
      "Validation accuracy: 0.49660326086956524\n",
      "Epoch: 3\n",
      "Train accuracy: 0.484375\n",
      "Validation accuracy: 0.5047554347826086\n",
      "Epoch: 4\n",
      "Train accuracy: 0.46895032051282054\n",
      "Validation accuracy: 0.5183423913043478\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5005008012820513\n",
      "Validation accuracy: 0.5054347826086957\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5783253205128205\n",
      "Validation accuracy: 0.4969429347826087\n",
      "Epoch: 7\n",
      "Train accuracy: 0.4065504807692308\n",
      "Validation accuracy: 0.4945652173913043\n",
      "Epoch: 8\n",
      "Train accuracy: 0.5327524038461539\n",
      "Validation accuracy: 0.5050951086956522\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5627003205128205\n",
      "Validation accuracy: 0.49898097826086957\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.4375\n",
      "Validation accuracy: 0.49798387096774194\n",
      "Epoch: 1\n",
      "Train accuracy: 0.59375\n",
      "Validation accuracy: 0.49227150537634407\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5077123397435898\n",
      "Validation accuracy: 0.5053763440860215\n",
      "Epoch: 3\n",
      "Train accuracy: 0.3740985576923077\n",
      "Validation accuracy: 0.5352822580645161\n",
      "Epoch: 4\n",
      "Train accuracy: 0.8121995192307693\n",
      "Validation accuracy: 0.6518817204301075\n",
      "Epoch: 5\n",
      "Train accuracy: 0.8455528846153846\n",
      "Validation accuracy: 0.6458333333333334\n",
      "Epoch: 6\n",
      "Train accuracy: 0.7091346153846154\n",
      "Validation accuracy: 0.5974462365591398\n",
      "Epoch: 7\n",
      "Train accuracy: 0.7203525641025641\n",
      "Validation accuracy: 0.6001344086021505\n",
      "Epoch: 8\n",
      "Train accuracy: 0.7302684294871795\n",
      "Validation accuracy: 0.6663306451612904\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9116586538461539\n",
      "Validation accuracy: 0.6895161290322581\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9196714743589743\n",
      "Validation accuracy: 0.6297554347826086\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9939903846153846\n",
      "Validation accuracy: 0.6402853260869565\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9983974358974359\n",
      "Validation accuracy: 0.6175271739130435\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9959935897435898\n",
      "Validation accuracy: 0.631453804347826\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9970953525641025\n",
      "Validation accuracy: 0.6480978260869565\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9993990384615384\n",
      "Validation accuracy: 0.5889945652173914\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9957932692307693\n",
      "Validation accuracy: 0.5502717391304348\n",
      "Epoch: 7\n",
      "Train accuracy: 0.99609375\n",
      "Validation accuracy: 0.6362092391304348\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9963942307692307\n",
      "Validation accuracy: 0.6141304347826086\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9961939102564102\n",
      "Validation accuracy: 0.6073369565217391\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.48647836538461536\n",
      "Validation accuracy: 0.4874320652173913\n",
      "Epoch: 1\n",
      "Train accuracy: 0.453125\n",
      "Validation accuracy: 0.4935461956521739\n",
      "Epoch: 2\n",
      "Train accuracy: 0.578125\n",
      "Validation accuracy: 0.49014945652173914\n",
      "Epoch: 3\n",
      "Train accuracy: 0.453125\n",
      "Validation accuracy: 0.46908967391304346\n",
      "Epoch: 4\n",
      "Train accuracy: 0.625\n",
      "Validation accuracy: 0.4782608695652174\n",
      "Epoch: 5\n",
      "Train accuracy: 0.4385016025641026\n",
      "Validation accuracy: 0.5023777173913043\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5007011217948718\n",
      "Validation accuracy: 0.5193614130434783\n",
      "Epoch: 7\n",
      "Train accuracy: 0.5015024038461539\n",
      "Validation accuracy: 0.4891304347826087\n",
      "Epoch: 8\n",
      "Train accuracy: 0.5054086538461539\n",
      "Validation accuracy: 0.49558423913043476\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5079126602564102\n",
      "Validation accuracy: 0.5186820652173914\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5173277243589743\n",
      "Validation accuracy: 0.48641304347826086\n",
      "Epoch: 1\n",
      "Train accuracy: 0.45002003205128205\n",
      "Validation accuracy: 0.5444972826086957\n",
      "Epoch: 2\n",
      "Train accuracy: 0.4973958333333333\n",
      "Validation accuracy: 0.47214673913043476\n",
      "Epoch: 3\n",
      "Train accuracy: 0.4735576923076923\n",
      "Validation accuracy: 0.4622961956521739\n",
      "Epoch: 4\n",
      "Train accuracy: 0.430088141025641\n",
      "Validation accuracy: 0.45414402173913043\n",
      "Epoch: 5\n",
      "Train accuracy: 0.4573317307692308\n",
      "Validation accuracy: 0.4639945652173913\n",
      "Epoch: 6\n",
      "Train accuracy: 0.49949919871794873\n",
      "Validation accuracy: 0.4782608695652174\n",
      "Epoch: 7\n",
      "Train accuracy: 0.44711538461538464\n",
      "Validation accuracy: 0.4752038043478261\n",
      "Epoch: 8\n",
      "Train accuracy: 0.47325721153846156\n",
      "Validation accuracy: 0.453804347826087\n",
      "Epoch: 9\n",
      "Train accuracy: 0.3965344551282051\n",
      "Validation accuracy: 0.438179347826087\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9964943910256411\n",
      "Validation accuracy: 0.5672043010752689\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9958934294871795\n",
      "Validation accuracy: 0.5389784946236559\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9933894230769231\n",
      "Validation accuracy: 0.5403225806451613\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9803685897435898\n",
      "Validation accuracy: 0.5504032258064516\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9777644230769231\n",
      "Validation accuracy: 0.550739247311828\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9849759615384616\n",
      "Validation accuracy: 0.5305779569892473\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9916866987179487\n",
      "Validation accuracy: 0.5423387096774194\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9860777243589743\n",
      "Validation accuracy: 0.5860215053763441\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9900841346153846\n",
      "Validation accuracy: 0.5157930107526881\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9791666666666666\n",
      "Validation accuracy: 0.5547715053763441\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5625\n",
      "Validation accuracy: 0.5154569892473119\n",
      "Epoch: 1\n",
      "Train accuracy: 0.375\n",
      "Validation accuracy: 0.5178091397849462\n",
      "Epoch: 2\n",
      "Train accuracy: 0.59375\n",
      "Validation accuracy: 0.509744623655914\n",
      "Epoch: 3\n",
      "Train accuracy: 0.4375\n",
      "Validation accuracy: 0.5164650537634409\n",
      "Epoch: 4\n",
      "Train accuracy: 0.6875\n",
      "Validation accuracy: 0.49193548387096775\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5625\n",
      "Validation accuracy: 0.5006720430107527\n",
      "Epoch: 6\n",
      "Train accuracy: 0.78125\n",
      "Validation accuracy: 0.5073924731182796\n",
      "Epoch: 7\n",
      "Train accuracy: 0.46875\n",
      "Validation accuracy: 0.5047043010752689\n",
      "Epoch: 8\n",
      "Train accuracy: 0.375\n",
      "Validation accuracy: 0.4946236559139785\n",
      "Epoch: 9\n",
      "Train accuracy: 0.46875\n",
      "Validation accuracy: 0.490255376344086\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.9476161858974359\n",
      "Validation accuracy: 0.670516304347826\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9893830128205128\n",
      "Validation accuracy: 0.6436820652173914\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9969951923076923\n",
      "Validation accuracy: 0.6531929347826086\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9967948717948718\n",
      "Validation accuracy: 0.6684782608695652\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9969951923076923\n",
      "Validation accuracy: 0.6803668478260869\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9950921474358975\n",
      "Validation accuracy: 0.6864809782608695\n",
      "Epoch: 6\n",
      "Train accuracy: 0.996895032051282\n",
      "Validation accuracy: 0.704483695652174\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9973958333333334\n",
      "Validation accuracy: 0.7092391304347826\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9971955128205128\n",
      "Validation accuracy: 0.6776494565217391\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9962940705128205\n",
      "Validation accuracy: 0.6691576086956522\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.46875\n",
      "Validation accuracy: 0.5101902173913043\n",
      "Epoch: 1\n",
      "Train accuracy: 0.484375\n",
      "Validation accuracy: 0.49626358695652173\n",
      "Epoch: 2\n",
      "Train accuracy: 0.421875\n",
      "Validation accuracy: 0.49660326086956524\n",
      "Epoch: 3\n",
      "Train accuracy: 0.421875\n",
      "Validation accuracy: 0.48539402173913043\n",
      "Epoch: 4\n",
      "Train accuracy: 0.453125\n",
      "Validation accuracy: 0.4908288043478261\n",
      "Epoch: 5\n",
      "Train accuracy: 0.515625\n",
      "Validation accuracy: 0.5\n",
      "Epoch: 6\n",
      "Train accuracy: 0.453125\n",
      "Validation accuracy: 0.49728260869565216\n",
      "Epoch: 7\n",
      "Train accuracy: 0.46875\n",
      "Validation accuracy: 0.4969429347826087\n",
      "Epoch: 8\n",
      "Train accuracy: 0.515625\n",
      "Validation accuracy: 0.5132472826086957\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5\n",
      "Validation accuracy: 0.49626358695652173\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5480769230769231\n",
      "Validation accuracy: 0.5026881720430108\n",
      "Epoch: 1\n",
      "Train accuracy: 0.6802884615384616\n",
      "Validation accuracy: 0.5403225806451613\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5746193910256411\n",
      "Validation accuracy: 0.5883736559139785\n",
      "Epoch: 3\n",
      "Train accuracy: 0.7244591346153846\n",
      "Validation accuracy: 0.5611559139784946\n",
      "Epoch: 4\n",
      "Train accuracy: 0.7842548076923077\n",
      "Validation accuracy: 0.5292338709677419\n",
      "Epoch: 5\n",
      "Train accuracy: 0.6114783653846154\n",
      "Validation accuracy: 0.5719086021505376\n",
      "Epoch: 6\n",
      "Train accuracy: 0.6614583333333334\n",
      "Validation accuracy: 0.5450268817204301\n",
      "Epoch: 7\n",
      "Train accuracy: 0.5095152243589743\n",
      "Validation accuracy: 0.5745967741935484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Train accuracy: 0.7169471153846154\n",
      "Validation accuracy: 0.5870295698924731\n",
      "Epoch: 9\n",
      "Train accuracy: 0.6824919871794872\n",
      "Validation accuracy: 0.5981182795698925\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.48357371794871795\n",
      "Validation accuracy: 0.49184782608695654\n",
      "Epoch: 1\n",
      "Train accuracy: 0.7950721153846154\n",
      "Validation accuracy: 0.6942934782608695\n",
      "Epoch: 2\n",
      "Train accuracy: 0.8519631410256411\n",
      "Validation accuracy: 0.6864809782608695\n",
      "Epoch: 3\n",
      "Train accuracy: 0.8700921474358975\n",
      "Validation accuracy: 0.6613451086956522\n",
      "Epoch: 4\n",
      "Train accuracy: 0.8606770833333334\n",
      "Validation accuracy: 0.6786684782608695\n",
      "Epoch: 5\n",
      "Train accuracy: 0.8511618589743589\n",
      "Validation accuracy: 0.6691576086956522\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9081530448717948\n",
      "Validation accuracy: 0.665421195652174\n",
      "Epoch: 7\n",
      "Train accuracy: 0.8916266025641025\n",
      "Validation accuracy: 0.6355298913043478\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9068509615384616\n",
      "Validation accuracy: 0.634171195652174\n",
      "Epoch: 9\n",
      "Train accuracy: 0.8418469551282052\n",
      "Validation accuracy: 0.6603260869565217\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5600961538461539\n",
      "Validation accuracy: 0.5104166666666666\n",
      "Epoch: 1\n",
      "Train accuracy: 0.6676682692307693\n",
      "Validation accuracy: 0.5057123655913979\n",
      "Epoch: 2\n",
      "Train accuracy: 0.6811899038461539\n",
      "Validation accuracy: 0.5171370967741935\n",
      "Epoch: 3\n",
      "Train accuracy: 0.5626001602564102\n",
      "Validation accuracy: 0.49361559139784944\n",
      "Epoch: 4\n",
      "Train accuracy: 0.48778044871794873\n",
      "Validation accuracy: 0.5094086021505376\n",
      "Epoch: 5\n",
      "Train accuracy: 0.5087139423076923\n",
      "Validation accuracy: 0.5030241935483871\n",
      "Epoch: 6\n",
      "Train accuracy: 0.5054086538461539\n",
      "Validation accuracy: 0.5178091397849462\n",
      "Epoch: 7\n",
      "Train accuracy: 0.4830729166666667\n",
      "Validation accuracy: 0.5601478494623656\n",
      "Epoch: 8\n",
      "Train accuracy: 0.4360977564102564\n",
      "Validation accuracy: 0.4825268817204301\n",
      "Epoch: 9\n",
      "Train accuracy: 0.4947916666666667\n",
      "Validation accuracy: 0.5504032258064516\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.53125\n",
      "Validation accuracy: 0.49660326086956524\n",
      "Epoch: 1\n",
      "Train accuracy: 0.609375\n",
      "Validation accuracy: 0.49626358695652173\n",
      "Epoch: 2\n",
      "Train accuracy: 0.5625\n",
      "Validation accuracy: 0.5125679347826086\n",
      "Epoch: 3\n",
      "Train accuracy: 0.421875\n",
      "Validation accuracy: 0.492866847826087\n",
      "Epoch: 4\n",
      "Train accuracy: 0.5\n",
      "Validation accuracy: 0.4952445652173913\n",
      "Epoch: 5\n",
      "Train accuracy: 0.53125\n",
      "Validation accuracy: 0.501358695652174\n",
      "Epoch: 6\n",
      "Train accuracy: 0.4375\n",
      "Validation accuracy: 0.484375\n",
      "Epoch: 7\n",
      "Train accuracy: 0.484375\n",
      "Validation accuracy: 0.4976222826086957\n",
      "Epoch: 8\n",
      "Train accuracy: 0.453125\n",
      "Validation accuracy: 0.5030570652173914\n",
      "Epoch: 9\n",
      "Train accuracy: 0.59375\n",
      "Validation accuracy: 0.5027173913043478\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5\n",
      "Validation accuracy: 0.4898097826086957\n",
      "Epoch: 1\n",
      "Train accuracy: 0.46875\n",
      "Validation accuracy: 0.499320652173913\n",
      "Epoch: 2\n",
      "Train accuracy: 0.46875\n",
      "Validation accuracy: 0.5061141304347826\n",
      "Epoch: 3\n",
      "Train accuracy: 0.375\n",
      "Validation accuracy: 0.5003396739130435\n",
      "Epoch: 4\n",
      "Train accuracy: 0.515625\n",
      "Validation accuracy: 0.49014945652173914\n",
      "Epoch: 5\n",
      "Train accuracy: 0.6875\n",
      "Validation accuracy: 0.5016983695652174\n",
      "Epoch: 6\n",
      "Train accuracy: 0.515625\n",
      "Validation accuracy: 0.5054347826086957\n",
      "Epoch: 7\n",
      "Train accuracy: 0.484375\n",
      "Validation accuracy: 0.5054347826086957\n",
      "Epoch: 8\n",
      "Train accuracy: 0.4375\n",
      "Validation accuracy: 0.48709239130434784\n",
      "Epoch: 9\n",
      "Train accuracy: 0.5\n",
      "Validation accuracy: 0.5122282608695652\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5689102564102564\n",
      "Validation accuracy: 0.6212635869565217\n",
      "Epoch: 1\n",
      "Train accuracy: 0.8932291666666666\n",
      "Validation accuracy: 0.6837635869565217\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9185697115384616\n",
      "Validation accuracy: 0.6817255434782609\n",
      "Epoch: 3\n",
      "Train accuracy: 0.8997395833333334\n",
      "Validation accuracy: 0.6491168478260869\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9193709935897436\n",
      "Validation accuracy: 0.6409646739130435\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9340945512820513\n",
      "Validation accuracy: 0.6379076086956522\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9279847756410257\n",
      "Validation accuracy: 0.6817255434782609\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9710536858974359\n",
      "Validation accuracy: 0.6820652173913043\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9216746794871795\n",
      "Validation accuracy: 0.6956521739130435\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9365985576923077\n",
      "Validation accuracy: 0.71875\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.578125\n",
      "Validation accuracy: 0.4908288043478261\n",
      "Epoch: 1\n",
      "Train accuracy: 0.6011618589743589\n",
      "Validation accuracy: 0.5743885869565217\n",
      "Epoch: 2\n",
      "Train accuracy: 0.8667868589743589\n",
      "Validation accuracy: 0.7170516304347826\n",
      "Epoch: 3\n",
      "Train accuracy: 0.8735977564102564\n",
      "Validation accuracy: 0.6372282608695652\n",
      "Epoch: 4\n",
      "Train accuracy: 0.867988782051282\n",
      "Validation accuracy: 0.6827445652173914\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9036458333333334\n",
      "Validation accuracy: 0.6555706521739131\n",
      "Epoch: 6\n",
      "Train accuracy: 0.8954326923076923\n",
      "Validation accuracy: 0.6915760869565217\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9328926282051282\n",
      "Validation accuracy: 0.7143342391304348\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9181690705128205\n",
      "Validation accuracy: 0.7160326086956522\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9625400641025641\n",
      "Validation accuracy: 0.7245244565217391\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.7955729166666666\n",
      "Validation accuracy: 0.6965725806451613\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9633413461538461\n",
      "Validation accuracy: 0.655241935483871\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9706530448717948\n",
      "Validation accuracy: 0.6784274193548387\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9811698717948718\n",
      "Validation accuracy: 0.738239247311828\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9852764423076923\n",
      "Validation accuracy: 0.6878360215053764\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9859775641025641\n",
      "Validation accuracy: 0.6915322580645161\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9719551282051282\n",
      "Validation accuracy: 0.7096774193548387\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9899839743589743\n",
      "Validation accuracy: 0.6831317204301075\n",
      "Epoch: 8\n",
      "Train accuracy: 0.979667467948718\n",
      "Validation accuracy: 0.6989247311827957\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9922876602564102\n",
      "Validation accuracy: 0.677755376344086\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.8724959935897436\n",
      "Validation accuracy: 0.6664402173913043\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9909855769230769\n",
      "Validation accuracy: 0.6858016304347826\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9941907051282052\n",
      "Validation accuracy: 0.6993885869565217\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9917868589743589\n",
      "Validation accuracy: 0.6807065217391305\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9986979166666666\n",
      "Validation accuracy: 0.7289402173913043\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9978966346153846\n",
      "Validation accuracy: 0.6987092391304348\n",
      "Epoch: 6\n",
      "Train accuracy: 0.996895032051282\n",
      "Validation accuracy: 0.7099184782608695\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9962940705128205\n",
      "Validation accuracy: 0.7102581521739131\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9981971153846154\n",
      "Validation accuracy: 0.7228260869565217\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9980969551282052\n",
      "Validation accuracy: 0.7445652173913043\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.5228365384615384\n",
      "Validation accuracy: 0.5040760869565217\n",
      "Epoch: 1\n",
      "Train accuracy: 0.8650841346153846\n",
      "Validation accuracy: 0.7163722826086957\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9682491987179487\n",
      "Validation accuracy: 0.7038043478260869\n",
      "Epoch: 3\n",
      "Train accuracy: 0.9690504807692307\n",
      "Validation accuracy: 0.7000679347826086\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9697516025641025\n",
      "Validation accuracy: 0.6813858695652174\n",
      "Epoch: 5\n",
      "Train accuracy: 0.9769631410256411\n",
      "Validation accuracy: 0.6817255434782609\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9880809294871795\n",
      "Validation accuracy: 0.6881793478260869\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9910857371794872\n",
      "Validation accuracy: 0.6942934782608695\n",
      "Epoch: 8\n",
      "Train accuracy: 0.9932892628205128\n",
      "Validation accuracy: 0.6946331521739131\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9921875\n",
      "Validation accuracy: 0.6871603260869565\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Train accuracy: 0.6278044871794872\n",
      "Validation accuracy: 0.6205842391304348\n",
      "Epoch: 1\n",
      "Train accuracy: 0.9715544871794872\n",
      "Validation accuracy: 0.6898777173913043\n",
      "Epoch: 2\n",
      "Train accuracy: 0.9558293269230769\n",
      "Validation accuracy: 0.6511548913043478\n",
      "Epoch: 3\n",
      "Train accuracy: 0.971854967948718\n",
      "Validation accuracy: 0.6616847826086957\n",
      "Epoch: 4\n",
      "Train accuracy: 0.9815705128205128\n",
      "Validation accuracy: 0.6878396739130435\n",
      "Epoch: 5\n",
      "Train accuracy: 0.984375\n",
      "Validation accuracy: 0.71875\n",
      "Epoch: 6\n",
      "Train accuracy: 0.9910857371794872\n",
      "Validation accuracy: 0.6973505434782609\n",
      "Epoch: 7\n",
      "Train accuracy: 0.9923878205128205\n",
      "Validation accuracy: 0.7058423913043478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Train accuracy: 0.9926883012820513\n",
      "Validation accuracy: 0.6905570652173914\n",
      "Epoch: 9\n",
      "Train accuracy: 0.9937900641025641\n",
      "Validation accuracy: 0.6905570652173914\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=60,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f17dd3bd630>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plot_objective(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.7445652173913043,\n",
       "  [1.5178328855285662e-06,\n",
       "   0.0004895539356438342,\n",
       "   0.0020969879904624285,\n",
       "   6.214158600010628e-06,\n",
       "   9.135323619965988e-05,\n",
       "   4.5481429094701505e-07,\n",
       "   8,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   48,\n",
       "   208,\n",
       "   256,\n",
       "   192,\n",
       "   2560,\n",
       "   1e-05,\n",
       "   0.9999,\n",
       "   64]),\n",
       " (-0.7245244565217391,\n",
       "  [1.3033188318149699e-06,\n",
       "   0.010520064725614906,\n",
       "   2.499583831538053e-07,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   0.0014668749087331124,\n",
       "   5,\n",
       "   3,\n",
       "   3,\n",
       "   6,\n",
       "   96,\n",
       "   80,\n",
       "   48,\n",
       "   192,\n",
       "   2560,\n",
       "   0.013097073829578391,\n",
       "   0.5585870123208373,\n",
       "   64]),\n",
       " (-0.71875,\n",
       "  [1.8176001939748006e-06,\n",
       "   0.00518751877267737,\n",
       "   0.1,\n",
       "   1.3916145310640059e-07,\n",
       "   0.004910254336438996,\n",
       "   4.715725769220243e-07,\n",
       "   8,\n",
       "   10,\n",
       "   3,\n",
       "   5,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   0.1142465464862375,\n",
       "   0.529921974682424,\n",
       "   64]),\n",
       " (-0.7007472826086957,\n",
       "  [1.3501981392404177e-06,\n",
       "   0.006593009505125237,\n",
       "   0.0009600887444387682,\n",
       "   1.0811711460849992e-08,\n",
       "   0.0035589595657425904,\n",
       "   8.723934974406631e-07,\n",
       "   6,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   0.1906948287763293,\n",
       "   0.014700459585290586,\n",
       "   64]),\n",
       " (-0.6905570652173914,\n",
       "  [1.5271087834413554e-06,\n",
       "   0.007888906971196964,\n",
       "   5.395714195501132e-08,\n",
       "   1.0956650929129386e-06,\n",
       "   8.024533253365652e-05,\n",
       "   1.0938355591900463e-08,\n",
       "   8,\n",
       "   5,\n",
       "   7,\n",
       "   4,\n",
       "   224,\n",
       "   176,\n",
       "   256,\n",
       "   16,\n",
       "   2560,\n",
       "   1e-05,\n",
       "   0.9999,\n",
       "   64]),\n",
       " (-0.6895161290322581,\n",
       "  [1.0555940878921794e-07,\n",
       "   0.09317373040508546,\n",
       "   0.0005202358529301968,\n",
       "   1.653231861582997e-08,\n",
       "   0.003380047187571669,\n",
       "   3.599907586274443e-07,\n",
       "   8,\n",
       "   8,\n",
       "   7,\n",
       "   5,\n",
       "   64,\n",
       "   112,\n",
       "   128,\n",
       "   256,\n",
       "   2048,\n",
       "   0.012709323330334494,\n",
       "   0.006521082839045292,\n",
       "   32]),\n",
       " (-0.6871603260869565,\n",
       "  [1.536613113991343e-06,\n",
       "   2.1945902026804268e-07,\n",
       "   2.013005567194683e-08,\n",
       "   0.0061658296743864445,\n",
       "   0.021683252838781465,\n",
       "   1.8405247250166618e-06,\n",
       "   5,\n",
       "   4,\n",
       "   7,\n",
       "   4,\n",
       "   64,\n",
       "   112,\n",
       "   64,\n",
       "   144,\n",
       "   2560,\n",
       "   1e-05,\n",
       "   0.9999,\n",
       "   64]),\n",
       " (-0.677755376344086,\n",
       "  [1.3697546212118618e-06,\n",
       "   0.00019993448872016327,\n",
       "   0.03198496007230006,\n",
       "   1.3036334237082671e-08,\n",
       "   9.489981319771628e-08,\n",
       "   1.2311389714024988e-05,\n",
       "   5,\n",
       "   6,\n",
       "   3,\n",
       "   6,\n",
       "   48,\n",
       "   144,\n",
       "   112,\n",
       "   176,\n",
       "   2560,\n",
       "   0.8972696493803541,\n",
       "   0.902409288920951,\n",
       "   32]),\n",
       " (-0.6691576086956522,\n",
       "  [6.74166144683711e-06,\n",
       "   0.00019115498036790615,\n",
       "   0.0003209438029110774,\n",
       "   2.1248941737008284e-06,\n",
       "   0.1,\n",
       "   1.7996620800780862e-08,\n",
       "   4,\n",
       "   7,\n",
       "   7,\n",
       "   3,\n",
       "   64,\n",
       "   80,\n",
       "   128,\n",
       "   256,\n",
       "   2048,\n",
       "   0.4685215381722805,\n",
       "   0.9534106502346201,\n",
       "   64]),\n",
       " (-0.6603260869565217,\n",
       "  [1.4701272990387835e-06,\n",
       "   3.3078620735208596e-08,\n",
       "   0.005802631773594926,\n",
       "   0.001526156308703228,\n",
       "   0.09896520070339151,\n",
       "   1e-08,\n",
       "   10,\n",
       "   5,\n",
       "   7,\n",
       "   4,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   0.8790867410306067,\n",
       "   1e-05,\n",
       "   64]),\n",
       " (-0.6348505434782609,\n",
       "  [6e-05,\n",
       "   0.0002,\n",
       "   0.0002,\n",
       "   0.0002,\n",
       "   0.0002,\n",
       "   0.001,\n",
       "   10,\n",
       "   7,\n",
       "   4,\n",
       "   4,\n",
       "   64,\n",
       "   128,\n",
       "   128,\n",
       "   256,\n",
       "   2048,\n",
       "   0.9,\n",
       "   0.999,\n",
       "   64]),\n",
       " (-0.6073369565217391,\n",
       "  [1.2122721305455692e-05,\n",
       "   0.01175790323881513,\n",
       "   0.0037113504783813874,\n",
       "   2.8440008886486996e-07,\n",
       "   1.3318471316006523e-06,\n",
       "   0.01074515048486014,\n",
       "   11,\n",
       "   8,\n",
       "   3,\n",
       "   4,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   1e-05,\n",
       "   0.3377741507095172,\n",
       "   64]),\n",
       " (-0.5985054347826086,\n",
       "  [1.7424131400864444e-05,\n",
       "   6.007211029488985e-07,\n",
       "   7.025826647330092e-08,\n",
       "   3.2221806955520504e-08,\n",
       "   1.9969767055635207e-05,\n",
       "   3.030716461229088e-08,\n",
       "   8,\n",
       "   9,\n",
       "   7,\n",
       "   4,\n",
       "   48,\n",
       "   144,\n",
       "   112,\n",
       "   160,\n",
       "   2560,\n",
       "   0.9167463853152296,\n",
       "   0.48502131446374575,\n",
       "   64]),\n",
       " (-0.5981182795698925,\n",
       "  [0.0054127287908113456,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   2.2241158908618434e-08,\n",
       "   1e-08,\n",
       "   6,\n",
       "   6,\n",
       "   8,\n",
       "   3,\n",
       "   48,\n",
       "   144,\n",
       "   112,\n",
       "   160,\n",
       "   2560,\n",
       "   0.7481337994242019,\n",
       "   0.3359416908325563,\n",
       "   32]),\n",
       " (-0.5940896739130435,\n",
       "  [0.00013364624916903647,\n",
       "   1.0416618992098408e-07,\n",
       "   1.4833579164775348e-06,\n",
       "   0.0013636673304319711,\n",
       "   0.007563209768960482,\n",
       "   1.6192011872360527e-06,\n",
       "   11,\n",
       "   9,\n",
       "   6,\n",
       "   4,\n",
       "   80,\n",
       "   48,\n",
       "   16,\n",
       "   112,\n",
       "   2304,\n",
       "   0.8405727463155327,\n",
       "   0.5642471524070105,\n",
       "   64]),\n",
       " (-0.5665322580645161,\n",
       "  [7.745479653549314e-05,\n",
       "   0.060902900439212294,\n",
       "   0.00021505678541084607,\n",
       "   0.0057636450782077835,\n",
       "   0.00043803173785116495,\n",
       "   0.04889117953096646,\n",
       "   6,\n",
       "   9,\n",
       "   4,\n",
       "   5,\n",
       "   160,\n",
       "   32,\n",
       "   176,\n",
       "   32,\n",
       "   3072,\n",
       "   0.030556747654708317,\n",
       "   0.4098555713877424,\n",
       "   32]),\n",
       " (-0.5641983695652174,\n",
       "  [1.9287831836458686e-05,\n",
       "   0.0377846894599227,\n",
       "   1.2449882347712405e-07,\n",
       "   0.00015955844941531686,\n",
       "   0.0012025902163508362,\n",
       "   0.004429230280166121,\n",
       "   9,\n",
       "   7,\n",
       "   6,\n",
       "   4,\n",
       "   144,\n",
       "   112,\n",
       "   160,\n",
       "   32,\n",
       "   2560,\n",
       "   0.9177878217050816,\n",
       "   0.5339299069637019,\n",
       "   64]),\n",
       " (-0.5547715053763441,\n",
       "  [3.567149331545089e-05,\n",
       "   0.0008885694224615459,\n",
       "   0.006744263085432023,\n",
       "   3.606498234980932e-05,\n",
       "   0.1,\n",
       "   2.7415988727220302e-06,\n",
       "   6,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   208,\n",
       "   64,\n",
       "   128,\n",
       "   256,\n",
       "   2048,\n",
       "   0.13036487604963587,\n",
       "   1e-05,\n",
       "   32]),\n",
       " (-0.5504032258064516,\n",
       "  [0.0003905456133842435,\n",
       "   6.419655671468e-06,\n",
       "   0.00010677167211585777,\n",
       "   1e-08,\n",
       "   4.3940056650352265e-05,\n",
       "   5.691218566431342e-07,\n",
       "   11,\n",
       "   3,\n",
       "   7,\n",
       "   6,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   0.2968535997888272,\n",
       "   1e-05,\n",
       "   32]),\n",
       " (-0.5186820652173914,\n",
       "  [0.0008460674278485597,\n",
       "   5.334077536622421e-08,\n",
       "   0.0010124931868582979,\n",
       "   8.663499284358695e-07,\n",
       "   0.1,\n",
       "   1.6520082939254864e-07,\n",
       "   10,\n",
       "   5,\n",
       "   4,\n",
       "   5,\n",
       "   240,\n",
       "   160,\n",
       "   128,\n",
       "   256,\n",
       "   2048,\n",
       "   1e-05,\n",
       "   0.6841666334139775,\n",
       "   64]),\n",
       " (-0.5181451612903226,\n",
       "  [0.04000510344086536,\n",
       "   0.005835666092930939,\n",
       "   0.006002676494331097,\n",
       "   0.002255756482601523,\n",
       "   2.624387408217529e-07,\n",
       "   2.5560157933955434e-06,\n",
       "   6,\n",
       "   8,\n",
       "   7,\n",
       "   6,\n",
       "   64,\n",
       "   256,\n",
       "   176,\n",
       "   16,\n",
       "   2816,\n",
       "   0.5021592073815234,\n",
       "   0.42847170334106854,\n",
       "   32]),\n",
       " (-0.5122282608695652,\n",
       "  [1e-07,\n",
       "   1e-08,\n",
       "   0.003947642474542367,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   3,\n",
       "   3,\n",
       "   8,\n",
       "   6,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   2560,\n",
       "   0.9999,\n",
       "   1e-05,\n",
       "   64]),\n",
       " (-0.5067934782608695,\n",
       "  [0.07815886159344815,\n",
       "   1.1123807020692621e-07,\n",
       "   1.1309675800102237e-08,\n",
       "   1.8548778155491597e-08,\n",
       "   1.7958569767220705e-05,\n",
       "   3.7299163971015324e-05,\n",
       "   5,\n",
       "   10,\n",
       "   6,\n",
       "   5,\n",
       "   160,\n",
       "   128,\n",
       "   160,\n",
       "   176,\n",
       "   3840,\n",
       "   0.8718364920330032,\n",
       "   0.6492092039313555,\n",
       "   64]),\n",
       " (-0.5027173913043478,\n",
       "  [1e-07,\n",
       "   1e-08,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   3,\n",
       "   3,\n",
       "   8,\n",
       "   3,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   160,\n",
       "   2560,\n",
       "   0.9999,\n",
       "   1e-05,\n",
       "   64]),\n",
       " (-0.49898097826086957,\n",
       "  [0.0005454801910240061,\n",
       "   8.034983947365333e-08,\n",
       "   0.08502519186966044,\n",
       "   0.049621437355391554,\n",
       "   0.00013923961041148886,\n",
       "   1.745240655511307e-06,\n",
       "   7,\n",
       "   4,\n",
       "   7,\n",
       "   4,\n",
       "   64,\n",
       "   112,\n",
       "   144,\n",
       "   176,\n",
       "   768,\n",
       "   0.7620668426783267,\n",
       "   0.977514031234138,\n",
       "   64]),\n",
       " (-0.49626358695652173,\n",
       "  [1e-07,\n",
       "   4.407273256559331e-08,\n",
       "   1.2277982028615515e-07,\n",
       "   2.063145545117272e-05,\n",
       "   0.0942499826713531,\n",
       "   2.2578129442394752e-07,\n",
       "   5,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   16,\n",
       "   96,\n",
       "   48,\n",
       "   208,\n",
       "   1024,\n",
       "   0.5747104167729952,\n",
       "   0.6788394165866407,\n",
       "   64]),\n",
       " (-0.490255376344086,\n",
       "  [1e-07,\n",
       "   0.0002707178456494346,\n",
       "   1.1322080026803123e-08,\n",
       "   0.013282683033298567,\n",
       "   0.1,\n",
       "   5.730527884860132e-07,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   6,\n",
       "   48,\n",
       "   144,\n",
       "   224,\n",
       "   160,\n",
       "   512,\n",
       "   0.10253784802269071,\n",
       "   0.7231794730931498,\n",
       "   32]),\n",
       " (-0.4885752688172043,\n",
       "  [0.0014199467314752108,\n",
       "   0.0002985203808851363,\n",
       "   0.009088012885372188,\n",
       "   2.5950300380283493e-07,\n",
       "   2.2975222468857063e-05,\n",
       "   0.033036347942765054,\n",
       "   5,\n",
       "   6,\n",
       "   6,\n",
       "   3,\n",
       "   64,\n",
       "   208,\n",
       "   80,\n",
       "   176,\n",
       "   768,\n",
       "   0.39173489454061877,\n",
       "   0.5807755813295862,\n",
       "   32]),\n",
       " (-0.48029891304347827,\n",
       "  [0.06899900263532611,\n",
       "   2.781930180888892e-06,\n",
       "   4.483831288432357e-05,\n",
       "   0.0009325436794948076,\n",
       "   0.03579306568176825,\n",
       "   1.213977966175106e-06,\n",
       "   7,\n",
       "   5,\n",
       "   4,\n",
       "   6,\n",
       "   208,\n",
       "   176,\n",
       "   64,\n",
       "   192,\n",
       "   3072,\n",
       "   0.7173247575040793,\n",
       "   0.4287773957596433,\n",
       "   64]),\n",
       " (-0.438179347826087,\n",
       "  [0.005008857808128422,\n",
       "   0.002487788471814195,\n",
       "   1.5715422195054192e-08,\n",
       "   0.00044348638492298047,\n",
       "   1e-08,\n",
       "   9.514840735337916e-08,\n",
       "   6,\n",
       "   4,\n",
       "   5,\n",
       "   4,\n",
       "   176,\n",
       "   176,\n",
       "   112,\n",
       "   128,\n",
       "   2560,\n",
       "   0.9671124636939593,\n",
       "   0.49061608698669285,\n",
       "   64])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5178328855285662e-06,\n",
       " 0.0004895539356438342,\n",
       " 0.0020969879904624285,\n",
       " 6.214158600010628e-06,\n",
       " 9.135323619965988e-05,\n",
       " 4.5481429094701505e-07,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 48,\n",
       " 208,\n",
       " 256,\n",
       " 192,\n",
       " 2560,\n",
       " 1e-05,\n",
       " 0.9999,\n",
       " 64]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
