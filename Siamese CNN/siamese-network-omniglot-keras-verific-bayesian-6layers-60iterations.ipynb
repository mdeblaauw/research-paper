{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "data_subsets = ['train', 'val']\n",
    "data = {}\n",
    "\n",
    "for name in data_subsets:\n",
    "    with BytesIO() as files:\n",
    "        path = \"omniglot_images/\" +name+ \".pickle\"\n",
    "        s3.Bucket(\"research-paper-omniglot-data\").download_fileobj(path, files)\n",
    "        files.seek(0)    # move back to the beginning after writing\n",
    "        (X,c) = pickle.load(files)\n",
    "        data[name] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = '../../omniglot_images/'\n",
    "data_subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "data = {}\n",
    "categories = {}\n",
    "info = {}\n",
    "        \n",
    "for name in data_subsets:\n",
    "    file_path = os.path.join(path, name + \".pickle\")\n",
    "    print(\"loading data from {}\".format(file_path))\n",
    "    with open(file_path,\"rb\") as f:\n",
    "        (X,c) = pickle.load(f)\n",
    "        data[name] = X\n",
    "        categories[name] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_data(size, s='train'):\n",
    "    #get train data and shape\n",
    "    X=data[s]\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    #initialize 2 empty arrays for the input size in a list\n",
    "    pairs=[np.zeros((size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    #initialize vector for the targets\n",
    "    targets=np.zeros((size,1))\n",
    "    \n",
    "    for x in range(size):\n",
    "        #randomly sample one class (character)\n",
    "        category = rnd.choice(n_classes,1,replace=False)\n",
    "        #randomly sample one example from class (1-20 characters)\n",
    "        idx_1 = rnd.randint(0, n_examples)\n",
    "        pairs[0][x,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        #randomly sample again one example from class and add last class with modulo\n",
    "        # ..to ensure not same class pairs are created\n",
    "        idx_2 = (idx_1 + rnd.randint(0, n_examples)) % n_examples\n",
    "        #pick images of different class for 1st half and same class for 2nd half\n",
    "        if x >= size // 2:\n",
    "            category_2 = category\n",
    "            targets[x] = 1\n",
    "        else: \n",
    "        #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "        # ..different category\n",
    "            idx_2 = rnd.randint(0, n_examples) \n",
    "            category_2 = (category + rnd.randint(1,n_classes)) % n_classes\n",
    "            targets[x] = 0\n",
    "        pairs[1][x,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "        \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, train_labels = create_train_data(10000)\n",
    "val_set, val_labels = create_train_data(3000, s='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-7, high=0.1, prior='log-uniform', name='learning_rate')\n",
    "dim_reg_layer1 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer1')\n",
    "dim_reg_layer2 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer2')\n",
    "dim_reg_layer3 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer3')\n",
    "dim_reg_layer4 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer4')\n",
    "dim_reg_layer5 = Real(low=0.00000001, high=0.1, prior='log-uniform', name='reg_layer5')\n",
    "dim_filt_layer1 = Integer(4,12, name='filter_layer1')\n",
    "dim_filt_layer2 = Integer(8,12, name='filter_layer2')\n",
    "dim_filt_layer3 = Integer(5,8, name='filter_layer3')\n",
    "dim_filt_layer4 = Integer(2,5, name='filter_layer4')\n",
    "dim_chan_layer1 = Categorical([16,32,48,64,80,96], name='channel_layer1')\n",
    "dim_chan_layer2 = Categorical([16,32,48,64,80,96,112,128], name='channel_layer2')\n",
    "dim_chan_layer3 = Categorical([16,32,48,64,80,96,112,128,144,160], name='channel_layer3')\n",
    "dim_chan_layer4 = Categorical([16,32,48,64,80,96,112,128,144,160,176,192], name='channel_layer4')\n",
    "dim_fc_layer5 = Categorical([256,512,768,1024,1280,1536,1792,2048,2304,2560,2816,3072], name='channel_layer5')\n",
    "beta1 = Real(low=0.00001, high=0.9999, prior = 'uniform', name='beta1')\n",
    "beta2 = Real(low=0.00001, high=0.9999, prior = 'uniform', name='beta2')\n",
    "batch = Categorical([32,48], name='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = [dim_learning_rate,\n",
    "             dim_reg_layer1,\n",
    "             dim_reg_layer2,\n",
    "             dim_reg_layer3,\n",
    "             dim_reg_layer4,\n",
    "             dim_reg_layer5,\n",
    "             dim_filt_layer1,\n",
    "             dim_filt_layer2,\n",
    "             dim_filt_layer3,\n",
    "             dim_filt_layer4,\n",
    "             dim_chan_layer1,\n",
    "             dim_chan_layer2,\n",
    "             dim_chan_layer3,\n",
    "             dim_chan_layer4,\n",
    "             dim_fc_layer5,\n",
    "             beta1,\n",
    "             beta2,\n",
    "             batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_parameters = [8.824e-05,1e-08,1.327e-08,1e-08,5.0997e-05,1e-08,6,10,8,3,64,32,96,160,2560,0.739,0.9999,32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rnd.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rnd.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(**params):\n",
    "    input_shape = (105, 105, 1)\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    #build convnet to use in each siamese 'leg'\n",
    "    convnet = Sequential()\n",
    "    convnet.add(Conv2D(params['channel_layer1'],(params['filter_layer1'],params['filter_layer1']),activation='relu',input_shape=input_shape,kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer1']),bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer2'],(params['filter_layer2'],params['filter_layer2']),activation='relu',kernel_regularizer=l2(params['reg_layer2']),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer3'],(params['filter_layer3'],params['filter_layer3']),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer3']),bias_initializer=b_init))\n",
    "    convnet.add(MaxPooling2D())\n",
    "    convnet.add(Conv2D(params['channel_layer4'],(params['filter_layer4'],params['filter_layer4']),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(params['reg_layer4']),bias_initializer=b_init))\n",
    "    convnet.add(Flatten())\n",
    "    convnet.add(Dense(params['channel_layer5'],activation=\"sigmoid\",kernel_regularizer=l2(params['reg_layer5']),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "    #call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "    encoded_l = convnet(left_input)\n",
    "    encoded_r = convnet(right_input)\n",
    "    #layer to merge two encoded inputs with the l1 distance between them\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    #call this layer on list of two input tensors.\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    optimizer = Adam(lr=params['learning_rate'], beta_1=params['beta1'], beta_2=params['beta2'])\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "    #siamese_net.count_params()\n",
    "    return(siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, true_val):\n",
    "    acc_bool = np.equal(np.round_(pred), true_val)\n",
    "    acc = np.mean(acc_bool.astype(int))\n",
    "    \n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just as fast as the Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "###################################\n",
    "\n",
    "\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(**params):\n",
    "    #Training loop\n",
    "    siamese_net = create_network(**params)\n",
    "        \n",
    "    print(\"!\")\n",
    "    batch_size = params['batch']\n",
    "    total_batch = int(10000/batch_size)\n",
    "    total_batch_val = int(3000/batch_size)\n",
    "    epoch = 10\n",
    "    print(\"training\")\n",
    "    val_batch_acc = 0\n",
    "    val_acc = 0\n",
    "    for i in range(epoch):\n",
    "        #train_batch_acc = 0\n",
    "        for j in range(total_batch):\n",
    "            batch_x1, batch_x2, batch_y = shuffle(train_set[0],train_set[1], train_labels, n_samples = batch_size)\n",
    "            loss=siamese_net.train_on_batch([batch_x1, batch_x2],batch_y)\n",
    "            #probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "            #train_batch_acc += accuracy(probs, batch_y)\n",
    "            #print('Loss:', loss)\n",
    "            #print('Batch:', j)\n",
    "        #train_acc = train_batch_acc/total_batch\n",
    "        if i == epoch - 1:\n",
    "            for validation in range(total_batch_val):\n",
    "                batch_x1, batch_x2, batch_y = shuffle(val_set[0],val_set[1], val_labels, n_samples = batch_size)\n",
    "                probs = siamese_net.predict([batch_x1, batch_x2])\n",
    "                val_batch_acc += accuracy(probs, batch_y)\n",
    "            val_acc = val_batch_acc/total_batch_val\n",
    "            print('Validation accuracy:', val_acc)\n",
    "        print('Epoch:', i)\n",
    "        #print('Train accuracy:', train_acc)\n",
    "    return(-val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "Loss: 4.4984665\n",
      "Batch: 0\n",
      "Loss: 4.403412\n",
      "Batch: 1\n",
      "Loss: 4.330426\n",
      "Batch: 2\n",
      "Loss: 4.24168\n",
      "Batch: 3\n",
      "Loss: 4.1520605\n",
      "Batch: 4\n",
      "Loss: 4.066664\n",
      "Batch: 5\n",
      "Loss: 3.988257\n",
      "Batch: 6\n",
      "Loss: 3.9156978\n",
      "Batch: 7\n",
      "Loss: 3.8498886\n",
      "Batch: 8\n",
      "Loss: 3.791628\n",
      "Batch: 9\n",
      "Loss: 3.7408202\n",
      "Batch: 10\n",
      "Loss: 3.6968129\n",
      "Batch: 11\n",
      "Loss: 3.658761\n",
      "Batch: 12\n",
      "Loss: 3.6255662\n",
      "Batch: 13\n",
      "Loss: 3.5960796\n",
      "Batch: 14\n",
      "Loss: 3.5691466\n",
      "Batch: 15\n",
      "Loss: 3.5438907\n",
      "Batch: 16\n",
      "Loss: 3.5197856\n",
      "Batch: 17\n",
      "Loss: 3.496443\n",
      "Batch: 18\n",
      "Loss: 3.4735928\n",
      "Batch: 19\n",
      "Loss: 3.4510489\n",
      "Batch: 20\n",
      "Loss: 3.4286983\n",
      "Batch: 21\n",
      "Loss: 3.4064739\n",
      "Batch: 22\n",
      "Loss: 3.384332\n",
      "Batch: 23\n",
      "Loss: 3.3622463\n",
      "Batch: 24\n",
      "Loss: 3.3402033\n",
      "Batch: 25\n",
      "Loss: 3.318196\n",
      "Batch: 26\n",
      "Loss: 3.2962224\n",
      "Batch: 27\n",
      "Loss: 3.2742836\n",
      "Batch: 28\n",
      "Loss: 3.252379\n",
      "Batch: 29\n",
      "Loss: 3.2305143\n",
      "Batch: 30\n",
      "Loss: 3.2086942\n",
      "Batch: 31\n",
      "Loss: 3.186924\n",
      "Batch: 32\n",
      "Loss: 3.165208\n",
      "Batch: 33\n",
      "Loss: 3.1435518\n",
      "Batch: 34\n",
      "Loss: 3.121961\n",
      "Batch: 35\n",
      "Loss: 3.10044\n",
      "Batch: 36\n",
      "Loss: 3.078994\n",
      "Batch: 37\n",
      "Loss: 3.0576262\n",
      "Batch: 38\n",
      "Loss: 3.0363412\n",
      "Batch: 39\n",
      "Loss: 3.0151424\n",
      "Batch: 40\n",
      "Loss: 2.9940333\n",
      "Batch: 41\n",
      "Loss: 2.9730182\n",
      "Batch: 42\n",
      "Loss: 2.9520986\n",
      "Batch: 43\n",
      "Loss: 2.931277\n",
      "Batch: 44\n",
      "Loss: 2.9105566\n",
      "Batch: 45\n",
      "Loss: 2.8899386\n",
      "Batch: 46\n",
      "Loss: 2.8694258\n",
      "Batch: 47\n",
      "Loss: 2.8490198\n",
      "Batch: 48\n",
      "Loss: 2.828722\n",
      "Batch: 49\n",
      "Loss: 2.808533\n",
      "Batch: 50\n",
      "Loss: 2.7884555\n",
      "Batch: 51\n",
      "Loss: 2.7684894\n",
      "Batch: 52\n",
      "Loss: 2.7486362\n",
      "Batch: 53\n",
      "Loss: 2.7288964\n",
      "Batch: 54\n",
      "Loss: 2.709271\n",
      "Batch: 55\n",
      "Loss: 2.6897604\n",
      "Batch: 56\n",
      "Loss: 2.6703656\n",
      "Batch: 57\n",
      "Loss: 2.651086\n",
      "Batch: 58\n",
      "Loss: 2.631923\n",
      "Batch: 59\n",
      "Loss: 2.6128764\n",
      "Batch: 60\n",
      "Loss: 2.5939467\n",
      "Batch: 61\n",
      "Loss: 2.5751336\n",
      "Batch: 62\n",
      "Loss: 2.556437\n",
      "Batch: 63\n",
      "Loss: 2.537858\n",
      "Batch: 64\n",
      "Loss: 2.519395\n",
      "Batch: 65\n",
      "Loss: 2.5010495\n",
      "Batch: 66\n",
      "Loss: 2.4828203\n",
      "Batch: 67\n",
      "Loss: 2.464708\n",
      "Batch: 68\n",
      "Loss: 2.4467118\n",
      "Batch: 69\n",
      "Loss: 2.428831\n",
      "Batch: 70\n",
      "Loss: 2.4110668\n",
      "Batch: 71\n",
      "Loss: 2.3934178\n",
      "Batch: 72\n",
      "Loss: 2.375884\n",
      "Batch: 73\n",
      "Loss: 2.3584652\n",
      "Batch: 74\n",
      "Loss: 2.341161\n",
      "Batch: 75\n",
      "Loss: 2.3239706\n",
      "Batch: 76\n",
      "Loss: 2.3068943\n",
      "Batch: 77\n",
      "Train accuracy: 0.9938902243589743\n",
      "Validation accuracy: 0.6769701086956522\n",
      "Loss: 3.0459366\n",
      "Batch: 0\n",
      "Loss: 2.739714\n",
      "Batch: 1\n",
      "Loss: 2.5221207\n",
      "Batch: 2\n",
      "Loss: 2.4402697\n",
      "Batch: 3\n",
      "Loss: 2.9450927\n",
      "Batch: 4\n",
      "Loss: 2.3504941\n",
      "Batch: 5\n",
      "Loss: 2.3620136\n",
      "Batch: 6\n",
      "Loss: 2.3611927\n",
      "Batch: 7\n",
      "Loss: 2.3569481\n",
      "Batch: 8\n",
      "Loss: 2.321332\n",
      "Batch: 9\n",
      "Loss: 2.2981665\n",
      "Batch: 10\n",
      "Loss: 2.2569647\n",
      "Batch: 11\n",
      "Loss: 2.2380733\n",
      "Batch: 12\n",
      "Loss: 2.2247086\n",
      "Batch: 13\n",
      "Loss: 2.2093043\n",
      "Batch: 14\n",
      "Loss: 2.1959233\n",
      "Batch: 15\n",
      "Loss: 2.1854534\n",
      "Batch: 16\n",
      "Loss: 2.1763175\n",
      "Batch: 17\n",
      "Loss: 2.1672475\n",
      "Batch: 18\n",
      "Loss: 2.1579008\n",
      "Batch: 19\n",
      "Loss: 2.1485438\n",
      "Batch: 20\n",
      "Loss: 2.1394808\n",
      "Batch: 21\n",
      "Loss: 2.1308556\n",
      "Batch: 22\n",
      "Loss: 2.1226432\n",
      "Batch: 23\n",
      "Loss: 2.114736\n",
      "Batch: 24\n",
      "Loss: 2.107013\n",
      "Batch: 25\n",
      "Loss: 2.0993977\n",
      "Batch: 26\n",
      "Loss: 2.0918615\n",
      "Batch: 27\n",
      "Loss: 2.084403\n",
      "Batch: 28\n",
      "Loss: 2.0770338\n",
      "Batch: 29\n",
      "Loss: 2.0697644\n",
      "Batch: 30\n",
      "Loss: 2.0625944\n",
      "Batch: 31\n",
      "Loss: 2.0555203\n",
      "Batch: 32\n",
      "Loss: 2.048532\n",
      "Batch: 33\n",
      "Loss: 2.0416203\n",
      "Batch: 34\n",
      "Loss: 2.0347724\n",
      "Batch: 35\n",
      "Loss: 2.027978\n",
      "Batch: 36\n",
      "Loss: 2.0212264\n",
      "Batch: 37\n",
      "Loss: 2.0145106\n",
      "Batch: 38\n",
      "Loss: 2.0078266\n",
      "Batch: 39\n",
      "Loss: 2.0011725\n",
      "Batch: 40\n",
      "Loss: 1.9945476\n",
      "Batch: 41\n",
      "Loss: 1.9879524\n",
      "Batch: 42\n",
      "Loss: 1.9813871\n",
      "Batch: 43\n",
      "Loss: 1.974852\n",
      "Batch: 44\n",
      "Loss: 1.9683465\n",
      "Batch: 45\n",
      "Loss: 1.9618701\n",
      "Batch: 46\n",
      "Loss: 1.955422\n",
      "Batch: 47\n",
      "Loss: 1.9490012\n",
      "Batch: 48\n",
      "Loss: 1.9426072\n",
      "Batch: 49\n",
      "Loss: 1.9362389\n",
      "Batch: 50\n",
      "Loss: 1.9298962\n",
      "Batch: 51\n",
      "Loss: 1.9235791\n",
      "Batch: 52\n",
      "Loss: 1.9172871\n",
      "Batch: 53\n",
      "Loss: 1.9110205\n",
      "Batch: 54\n",
      "Loss: 1.9047788\n",
      "Batch: 55\n",
      "Loss: 1.8985623\n",
      "Batch: 56\n",
      "Loss: 1.8923708\n",
      "Batch: 57\n",
      "Loss: 1.886204\n",
      "Batch: 58\n",
      "Loss: 1.8800616\n",
      "Batch: 59\n",
      "Loss: 1.8739433\n",
      "Batch: 60\n",
      "Loss: 1.8678491\n",
      "Batch: 61\n",
      "Loss: 1.8617784\n",
      "Batch: 62\n",
      "Loss: 1.8557315\n",
      "Batch: 63\n",
      "Loss: 1.8497082\n",
      "Batch: 64\n",
      "Loss: 1.8437086\n",
      "Batch: 65\n",
      "Loss: 1.8377331\n",
      "Batch: 66\n",
      "Loss: 1.8317809\n",
      "Batch: 67\n",
      "Loss: 1.8258522\n",
      "Batch: 68\n",
      "Loss: 1.8199469\n",
      "Batch: 69\n",
      "Loss: 1.8140651\n",
      "Batch: 70\n",
      "Loss: 1.8082062\n",
      "Batch: 71\n",
      "Loss: 1.8023704\n",
      "Batch: 72\n",
      "Loss: 1.7965577\n",
      "Batch: 73\n",
      "Loss: 1.7907677\n",
      "Batch: 74\n",
      "Loss: 1.7850004\n",
      "Batch: 75\n",
      "Loss: 1.779256\n",
      "Batch: 76\n",
      "Loss: 1.7735342\n",
      "Batch: 77\n",
      "Train accuracy: 0.9886818910256411\n",
      "Validation accuracy: 0.6885190217391305\n",
      "Loss: 2.4116068\n",
      "Batch: 0\n",
      "Loss: 2.0978696\n",
      "Batch: 1\n",
      "Loss: 1.9236507\n",
      "Batch: 2\n",
      "Loss: 1.8640379\n",
      "Batch: 3\n",
      "Loss: 1.8124716\n",
      "Batch: 4\n",
      "Loss: 1.8012815\n",
      "Batch: 5\n",
      "Loss: 1.782497\n",
      "Batch: 6\n",
      "Loss: 1.7683357\n",
      "Batch: 7\n",
      "Loss: 1.7590804\n",
      "Batch: 8\n",
      "Loss: 1.7491661\n",
      "Batch: 9\n",
      "Loss: 1.73991\n",
      "Batch: 10\n",
      "Loss: 1.7325654\n",
      "Batch: 11\n",
      "Loss: 1.7264568\n",
      "Batch: 12\n",
      "Loss: 1.7206893\n",
      "Batch: 13\n",
      "Loss: 1.7149061\n",
      "Batch: 14\n",
      "Loss: 1.7092019\n",
      "Batch: 15\n",
      "Loss: 1.7037178\n",
      "Batch: 16\n",
      "Loss: 1.6984788\n",
      "Batch: 17\n",
      "Loss: 1.693422\n",
      "Batch: 18\n",
      "Loss: 1.68846\n",
      "Batch: 19\n",
      "Loss: 1.6835389\n",
      "Batch: 20\n",
      "Loss: 1.6786492\n",
      "Batch: 21\n",
      "Loss: 1.6738027\n",
      "Batch: 22\n",
      "Loss: 1.6690087\n",
      "Batch: 23\n",
      "Loss: 1.6642683\n",
      "Batch: 24\n",
      "Loss: 1.6595743\n",
      "Batch: 25\n",
      "Loss: 1.6549157\n",
      "Batch: 26\n",
      "Loss: 1.6502811\n",
      "Batch: 27\n",
      "Loss: 1.6456624\n",
      "Batch: 28\n",
      "Loss: 1.641055\n",
      "Batch: 29\n",
      "Loss: 1.6364573\n",
      "Batch: 30\n",
      "Loss: 1.6318709\n",
      "Batch: 31\n",
      "Loss: 1.6272967\n",
      "Batch: 32\n",
      "Loss: 1.6227374\n",
      "Batch: 33\n",
      "Loss: 1.618194\n",
      "Batch: 34\n",
      "Loss: 1.6136677\n",
      "Batch: 35\n",
      "Loss: 1.6091578\n",
      "Batch: 36\n",
      "Loss: 1.6046641\n",
      "Batch: 37\n",
      "Loss: 1.6001855\n",
      "Batch: 38\n",
      "Loss: 1.5957208\n",
      "Batch: 39\n",
      "Loss: 1.5912693\n",
      "Batch: 40\n",
      "Loss: 1.5868299\n",
      "Batch: 41\n",
      "Loss: 1.5824023\n",
      "Batch: 42\n",
      "Loss: 1.5779872\n",
      "Batch: 43\n",
      "Loss: 1.5735847\n",
      "Batch: 44\n",
      "Loss: 1.5691955\n",
      "Batch: 45\n",
      "Loss: 1.5648202\n",
      "Batch: 46\n",
      "Loss: 1.5604581\n",
      "Batch: 47\n",
      "Loss: 1.5561103\n",
      "Batch: 48\n",
      "Loss: 1.551776\n",
      "Batch: 49\n",
      "Loss: 1.5474557\n",
      "Batch: 50\n",
      "Loss: 1.5431489\n",
      "Batch: 51\n",
      "Loss: 1.5388556\n",
      "Batch: 52\n",
      "Loss: 1.5345757\n",
      "Batch: 53\n",
      "Loss: 1.5303088\n",
      "Batch: 54\n",
      "Loss: 1.5260559\n",
      "Batch: 55\n",
      "Loss: 1.521816\n",
      "Batch: 56\n",
      "Loss: 1.5175892\n",
      "Batch: 57\n",
      "Loss: 1.5133762\n",
      "Batch: 58\n",
      "Loss: 1.5091763\n",
      "Batch: 59\n",
      "Loss: 1.50499\n",
      "Batch: 60\n",
      "Loss: 1.500817\n",
      "Batch: 61\n",
      "Loss: 1.4966577\n",
      "Batch: 62\n",
      "Loss: 1.4925116\n",
      "Batch: 63\n",
      "Loss: 1.4883791\n",
      "Batch: 64\n",
      "Loss: 1.4842596\n",
      "Batch: 65\n",
      "Loss: 1.4801533\n",
      "Batch: 66\n",
      "Loss: 1.4760605\n",
      "Batch: 67\n",
      "Loss: 1.4719807\n",
      "Batch: 68\n",
      "Loss: 1.4679137\n",
      "Batch: 69\n",
      "Loss: 1.46386\n",
      "Batch: 70\n",
      "Loss: 1.4598191\n",
      "Batch: 71\n",
      "Loss: 1.4557912\n",
      "Batch: 72\n",
      "Loss: 1.4517763\n",
      "Batch: 73\n",
      "Loss: 1.4477742\n",
      "Batch: 74\n",
      "Loss: 1.4437846\n",
      "Batch: 75\n",
      "Loss: 1.4398079\n",
      "Batch: 76\n",
      "Loss: 1.4358443\n",
      "Batch: 77\n",
      "Train accuracy: 0.9982972756410257\n",
      "Validation accuracy: 0.6698369565217391\n",
      "Loss: 2.4583118\n",
      "Batch: 0\n",
      "Loss: 1.9351004\n",
      "Batch: 1\n",
      "Loss: 1.6917096\n",
      "Batch: 2\n",
      "Loss: 1.5674249\n",
      "Batch: 3\n",
      "Loss: 1.5247016\n",
      "Batch: 4\n",
      "Loss: 1.4929019\n",
      "Batch: 5\n",
      "Loss: 1.4727852\n",
      "Batch: 6\n",
      "Loss: 1.4599686\n",
      "Batch: 7\n",
      "Loss: 1.4519944\n",
      "Batch: 8\n",
      "Loss: 1.444861\n",
      "Batch: 9\n",
      "Loss: 1.4370844\n",
      "Batch: 10\n",
      "Loss: 1.4295865\n",
      "Batch: 11\n",
      "Loss: 1.4233198\n",
      "Batch: 12\n",
      "Loss: 1.41837\n",
      "Batch: 13\n",
      "Loss: 1.414305\n",
      "Batch: 14\n",
      "Loss: 1.4105344\n",
      "Batch: 15\n",
      "Loss: 1.4066947\n",
      "Batch: 16\n",
      "Loss: 1.4028361\n",
      "Batch: 17\n",
      "Loss: 1.3991036\n",
      "Batch: 18\n",
      "Loss: 1.3955147\n",
      "Batch: 19\n",
      "Loss: 1.3920304\n",
      "Batch: 20\n",
      "Loss: 1.3886316\n",
      "Batch: 21\n",
      "Loss: 1.3853152\n",
      "Batch: 22\n",
      "Loss: 1.382062\n",
      "Batch: 23\n",
      "Loss: 1.3788428\n",
      "Batch: 24\n",
      "Loss: 1.375637\n",
      "Batch: 25\n",
      "Loss: 1.3724406\n",
      "Batch: 26\n",
      "Loss: 1.3692571\n",
      "Batch: 27\n",
      "Loss: 1.36609\n",
      "Batch: 28\n",
      "Loss: 1.3629397\n",
      "Batch: 29\n",
      "Loss: 1.3598043\n",
      "Batch: 30\n",
      "Loss: 1.3566797\n",
      "Batch: 31\n",
      "Loss: 1.3535639\n",
      "Batch: 32\n",
      "Loss: 1.3504541\n",
      "Batch: 33\n",
      "Loss: 1.3473488\n",
      "Batch: 34\n",
      "Loss: 1.3442473\n",
      "Batch: 35\n",
      "Loss: 1.341149\n",
      "Batch: 36\n",
      "Loss: 1.3380538\n",
      "Batch: 37\n",
      "Loss: 1.3349622\n",
      "Batch: 38\n",
      "Loss: 1.3318743\n",
      "Batch: 39\n",
      "Loss: 1.3287907\n",
      "Batch: 40\n",
      "Loss: 1.3257115\n",
      "Batch: 41\n",
      "Loss: 1.3226379\n",
      "Batch: 42\n",
      "Loss: 1.3195701\n",
      "Batch: 43\n",
      "Loss: 1.316508\n",
      "Batch: 44\n",
      "Loss: 1.3134528\n",
      "Batch: 45\n",
      "Loss: 1.3104043\n",
      "Batch: 46\n",
      "Loss: 1.3073628\n",
      "Batch: 47\n",
      "Loss: 1.3043282\n",
      "Batch: 48\n",
      "Loss: 1.3013008\n",
      "Batch: 49\n",
      "Loss: 1.2982804\n",
      "Batch: 50\n",
      "Loss: 1.2952669\n",
      "Batch: 51\n",
      "Loss: 1.2922606\n",
      "Batch: 52\n",
      "Loss: 1.2892611\n",
      "Batch: 53\n",
      "Loss: 1.2862688\n",
      "Batch: 54\n",
      "Loss: 1.2832835\n",
      "Batch: 55\n",
      "Loss: 1.2803055\n",
      "Batch: 56\n",
      "Loss: 1.2773348\n",
      "Batch: 57\n",
      "Loss: 1.2743714\n",
      "Batch: 58\n",
      "Loss: 1.2714154\n",
      "Batch: 59\n",
      "Loss: 1.2684668\n",
      "Batch: 60\n",
      "Loss: 1.2655255\n",
      "Batch: 61\n",
      "Loss: 1.262592\n",
      "Batch: 62\n",
      "Loss: 1.2596656\n",
      "Batch: 63\n",
      "Loss: 1.2567468\n",
      "Batch: 64\n",
      "Loss: 1.2538352\n",
      "Batch: 65\n",
      "Loss: 1.2509313\n",
      "Batch: 66\n",
      "Loss: 1.2480346\n",
      "Batch: 67\n",
      "Loss: 1.2451456\n",
      "Batch: 68\n",
      "Loss: 1.2422636\n",
      "Batch: 69\n",
      "Loss: 1.2393893\n",
      "Batch: 70\n",
      "Loss: 1.2365218\n",
      "Batch: 71\n",
      "Loss: 1.2336622\n",
      "Batch: 72\n",
      "Loss: 1.2308095\n",
      "Batch: 73\n",
      "Loss: 1.2279643\n",
      "Batch: 74\n",
      "Loss: 1.2251261\n",
      "Batch: 75\n",
      "Loss: 1.2222955\n",
      "Batch: 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2194717\n",
      "Batch: 77\n",
      "Train accuracy: 0.9955929487179487\n",
      "Validation accuracy: 0.681046195652174\n",
      "Loss: 2.047232\n",
      "Batch: 0\n",
      "Loss: 1.6528767\n",
      "Batch: 1\n",
      "Loss: 1.3625965\n",
      "Batch: 2\n",
      "Loss: 1.2733976\n",
      "Batch: 3\n",
      "Loss: 1.2483989\n",
      "Batch: 4\n",
      "Loss: 1.2347552\n",
      "Batch: 5\n",
      "Loss: 1.218029\n",
      "Batch: 6\n",
      "Loss: 1.2065902\n",
      "Batch: 7\n",
      "Loss: 1.198956\n",
      "Batch: 8\n",
      "Loss: 1.192144\n",
      "Batch: 9\n",
      "Loss: 1.1861367\n",
      "Batch: 10\n",
      "Loss: 1.1815007\n",
      "Batch: 11\n",
      "Loss: 1.1777263\n",
      "Batch: 12\n",
      "Loss: 1.1742325\n",
      "Batch: 13\n",
      "Loss: 1.1709801\n",
      "Batch: 14\n",
      "Loss: 1.1680155\n",
      "Batch: 15\n",
      "Loss: 1.1651844\n",
      "Batch: 16\n",
      "Loss: 1.1623319\n",
      "Batch: 17\n",
      "Loss: 1.1594843\n",
      "Batch: 18\n",
      "Loss: 1.1567261\n",
      "Batch: 19\n",
      "Loss: 1.154084\n",
      "Batch: 20\n",
      "Loss: 1.1515338\n",
      "Batch: 21\n",
      "Loss: 1.1490407\n",
      "Batch: 22\n",
      "Loss: 1.146579\n",
      "Batch: 23\n",
      "Loss: 1.144134\n",
      "Batch: 24\n",
      "Loss: 1.1416973\n",
      "Batch: 25\n",
      "Loss: 1.1392659\n",
      "Batch: 26\n",
      "Loss: 1.136837\n",
      "Batch: 27\n",
      "Loss: 1.1344097\n",
      "Batch: 28\n",
      "Loss: 1.1319836\n",
      "Batch: 29\n",
      "Loss: 1.1295587\n",
      "Batch: 30\n",
      "Loss: 1.1271353\n",
      "Batch: 31\n",
      "Loss: 1.1247138\n",
      "Batch: 32\n",
      "Loss: 1.1222942\n",
      "Batch: 33\n",
      "Loss: 1.1198772\n",
      "Batch: 34\n",
      "Loss: 1.1174628\n",
      "Batch: 35\n",
      "Loss: 1.1150512\n",
      "Batch: 36\n",
      "Loss: 1.1126425\n",
      "Batch: 37\n",
      "Loss: 1.1102363\n",
      "Batch: 38\n",
      "Loss: 1.1078328\n",
      "Batch: 39\n",
      "Loss: 1.1054322\n",
      "Batch: 40\n",
      "Loss: 1.1030343\n",
      "Batch: 41\n",
      "Loss: 1.1006393\n",
      "Batch: 42\n",
      "Loss: 1.098248\n",
      "Batch: 43\n",
      "Loss: 1.0958604\n",
      "Batch: 44\n",
      "Loss: 1.093477\n",
      "Batch: 45\n",
      "Loss: 1.0910981\n",
      "Batch: 46\n",
      "Loss: 1.0887237\n",
      "Batch: 47\n",
      "Loss: 1.086354\n",
      "Batch: 48\n",
      "Loss: 1.083989\n",
      "Batch: 49\n",
      "Loss: 1.0816288\n",
      "Batch: 50\n",
      "Loss: 1.079273\n",
      "Batch: 51\n",
      "Loss: 1.076922\n",
      "Batch: 52\n",
      "Loss: 1.0745755\n",
      "Batch: 53\n",
      "Loss: 1.0722342\n",
      "Batch: 54\n",
      "Loss: 1.069897\n",
      "Batch: 55\n",
      "Loss: 1.067565\n",
      "Batch: 56\n",
      "Loss: 1.0652382\n",
      "Batch: 57\n",
      "Loss: 1.0629164\n",
      "Batch: 58\n",
      "Loss: 1.0605997\n",
      "Batch: 59\n",
      "Loss: 1.0582882\n",
      "Batch: 60\n",
      "Loss: 1.0559818\n",
      "Batch: 61\n",
      "Loss: 1.0536807\n",
      "Batch: 62\n",
      "Loss: 1.0513848\n",
      "Batch: 63\n",
      "Loss: 1.0490936\n",
      "Batch: 64\n",
      "Loss: 1.0468079\n",
      "Batch: 65\n",
      "Loss: 1.0445275\n",
      "Batch: 66\n",
      "Loss: 1.0422521\n",
      "Batch: 67\n",
      "Loss: 1.0399818\n",
      "Batch: 68\n",
      "Loss: 1.0377167\n",
      "Batch: 69\n",
      "Loss: 1.0354568\n",
      "Batch: 70\n",
      "Loss: 1.0332019\n",
      "Batch: 71\n",
      "Loss: 1.0309523\n",
      "Batch: 72\n",
      "Loss: 1.0287076\n",
      "Batch: 73\n",
      "Loss: 1.0264683\n",
      "Batch: 74\n",
      "Loss: 1.0242338\n",
      "Batch: 75\n",
      "Loss: 1.0220046\n",
      "Batch: 76\n",
      "Loss: 1.0197803\n",
      "Batch: 77\n",
      "Train accuracy: 0.996895032051282\n",
      "Validation accuracy: 0.7027853260869565\n",
      "Loss: 1.6389763\n",
      "Batch: 0\n",
      "Loss: 1.3091729\n",
      "Batch: 1\n",
      "Loss: 1.1467144\n",
      "Batch: 2\n",
      "Loss: 1.0931153\n",
      "Batch: 3\n",
      "Loss: 1.0689673\n",
      "Batch: 4\n",
      "Loss: 1.0627166\n",
      "Batch: 5\n",
      "Loss: 1.04995\n",
      "Batch: 6\n",
      "Loss: 1.0389872\n",
      "Batch: 7\n",
      "Loss: 1.0329069\n",
      "Batch: 8\n",
      "Loss: 1.0289795\n",
      "Batch: 9\n",
      "Loss: 1.0255647\n",
      "Batch: 10\n",
      "Loss: 1.0222299\n",
      "Batch: 11\n",
      "Loss: 1.0190207\n",
      "Batch: 12\n",
      "Loss: 1.0160377\n",
      "Batch: 13\n",
      "Loss: 1.0133082\n",
      "Batch: 14\n",
      "Loss: 1.010803\n",
      "Batch: 15\n",
      "Loss: 1.0084656\n",
      "Batch: 16\n",
      "Loss: 1.0062324\n",
      "Batch: 17\n",
      "Loss: 1.004049\n",
      "Batch: 18\n",
      "Loss: 1.0018888\n",
      "Batch: 19\n",
      "Loss: 0.99975806\n",
      "Batch: 20\n",
      "Loss: 0.99768215\n",
      "Batch: 21\n",
      "Loss: 0.9956751\n",
      "Batch: 22\n",
      "Loss: 0.9937209\n",
      "Batch: 23\n",
      "Loss: 0.9917778\n",
      "Batch: 24\n",
      "Loss: 0.989817\n",
      "Batch: 25\n",
      "Loss: 0.98784417\n",
      "Batch: 26\n",
      "Loss: 0.9858779\n",
      "Batch: 27\n",
      "Loss: 0.9839246\n",
      "Batch: 28\n",
      "Loss: 0.9819781\n",
      "Batch: 29\n",
      "Loss: 0.98003113\n",
      "Batch: 30\n",
      "Loss: 0.9780824\n",
      "Batch: 31\n",
      "Loss: 0.9761351\n",
      "Batch: 32\n",
      "Loss: 0.974192\n",
      "Batch: 33\n",
      "Loss: 0.97225404\n",
      "Batch: 34\n",
      "Loss: 0.97032\n",
      "Batch: 35\n",
      "Loss: 0.96838796\n",
      "Batch: 36\n",
      "Loss: 0.96645665\n",
      "Batch: 37\n",
      "Loss: 0.96452475\n",
      "Batch: 38\n",
      "Loss: 0.9625922\n",
      "Batch: 39\n",
      "Loss: 0.960659\n",
      "Batch: 40\n",
      "Loss: 0.9587263\n",
      "Batch: 41\n",
      "Loss: 0.9567944\n",
      "Batch: 42\n",
      "Loss: 0.9548641\n",
      "Batch: 43\n",
      "Loss: 0.9529358\n",
      "Batch: 44\n",
      "Loss: 0.9510101\n",
      "Batch: 45\n",
      "Loss: 0.9490872\n",
      "Batch: 46\n",
      "Loss: 0.9471673\n",
      "Batch: 47\n",
      "Loss: 0.94525045\n",
      "Batch: 48\n",
      "Loss: 0.94333667\n",
      "Batch: 49\n",
      "Loss: 0.94142616\n",
      "Batch: 50\n",
      "Loss: 0.939519\n",
      "Batch: 51\n",
      "Loss: 0.93761516\n",
      "Batch: 52\n",
      "Loss: 0.935715\n",
      "Batch: 53\n",
      "Loss: 0.93381834\n",
      "Batch: 54\n",
      "Loss: 0.93192554\n",
      "Batch: 55\n",
      "Loss: 0.9300364\n",
      "Batch: 56\n",
      "Loss: 0.928151\n",
      "Batch: 57\n",
      "Loss: 0.92626953\n",
      "Batch: 58\n",
      "Loss: 0.9243915\n",
      "Batch: 59\n",
      "Loss: 0.9225175\n",
      "Batch: 60\n",
      "Loss: 0.9206473\n",
      "Batch: 61\n",
      "Loss: 0.918781\n",
      "Batch: 62\n",
      "Loss: 0.9169186\n",
      "Batch: 63\n",
      "Loss: 0.91506016\n",
      "Batch: 64\n",
      "Loss: 0.9132057\n",
      "Batch: 65\n",
      "Loss: 0.911355\n",
      "Batch: 66\n",
      "Loss: 0.90950793\n",
      "Batch: 67\n",
      "Loss: 0.9076646\n",
      "Batch: 68\n",
      "Loss: 0.90582526\n",
      "Batch: 69\n",
      "Loss: 0.90398973\n",
      "Batch: 70\n",
      "Loss: 0.90215814\n",
      "Batch: 71\n",
      "Loss: 0.90033036\n",
      "Batch: 72\n",
      "Loss: 0.89850634\n",
      "Batch: 73\n",
      "Loss: 0.89668614\n",
      "Batch: 74\n",
      "Loss: 0.89486974\n",
      "Batch: 75\n",
      "Loss: 0.893057\n",
      "Batch: 76\n",
      "Loss: 0.89124817\n",
      "Batch: 77\n",
      "Train accuracy: 0.9984975961538461\n",
      "Validation accuracy: 0.6793478260869565\n",
      "Loss: 1.4110727\n",
      "Batch: 0\n",
      "Loss: 1.0909864\n",
      "Batch: 1\n",
      "Loss: 0.9459318\n",
      "Batch: 2\n",
      "Loss: 0.9242387\n",
      "Batch: 3\n",
      "Loss: 0.9006247\n",
      "Batch: 4\n",
      "Loss: 0.890997\n",
      "Batch: 5\n",
      "Loss: 0.88526475\n",
      "Batch: 6\n",
      "Loss: 0.8785959\n",
      "Batch: 7\n",
      "Loss: 0.8727845\n",
      "Batch: 8\n",
      "Loss: 0.86855227\n",
      "Batch: 9\n",
      "Loss: 0.865387\n",
      "Batch: 10\n",
      "Loss: 0.8627663\n",
      "Batch: 11\n",
      "Loss: 0.86040413\n",
      "Batch: 12\n",
      "Loss: 0.8581778\n",
      "Batch: 13\n",
      "Loss: 0.85605776\n",
      "Batch: 14\n",
      "Loss: 0.85403776\n",
      "Batch: 15\n",
      "Loss: 0.8521053\n",
      "Batch: 16\n",
      "Loss: 0.85024947\n",
      "Batch: 17\n",
      "Loss: 0.8484558\n",
      "Batch: 18\n",
      "Loss: 0.84670806\n",
      "Batch: 19\n",
      "Loss: 0.8449929\n",
      "Batch: 20\n",
      "Loss: 0.8433\n",
      "Batch: 21\n",
      "Loss: 0.84162116\n",
      "Batch: 22\n",
      "Loss: 0.8399519\n",
      "Batch: 23\n",
      "Loss: 0.83828837\n",
      "Batch: 24\n",
      "Loss: 0.8366289\n",
      "Batch: 25\n",
      "Loss: 0.8349723\n",
      "Batch: 26\n",
      "Loss: 0.83331776\n",
      "Batch: 27\n",
      "Loss: 0.8316651\n",
      "Batch: 28\n",
      "Loss: 0.8300141\n",
      "Batch: 29\n",
      "Loss: 0.8283647\n",
      "Batch: 30\n",
      "Loss: 0.826717\n",
      "Batch: 31\n",
      "Loss: 0.82507116\n",
      "Batch: 32\n",
      "Loss: 0.8234272\n",
      "Batch: 33\n",
      "Loss: 0.82178503\n",
      "Batch: 34\n",
      "Loss: 0.8201449\n",
      "Batch: 35\n",
      "Loss: 0.8185068\n",
      "Batch: 36\n",
      "Loss: 0.816871\n",
      "Batch: 37\n",
      "Loss: 0.8152374\n",
      "Batch: 38\n",
      "Loss: 0.81360614\n",
      "Batch: 39\n",
      "Loss: 0.81197727\n",
      "Batch: 40\n",
      "Loss: 0.81035066\n",
      "Batch: 41\n",
      "Loss: 0.8087264\n",
      "Batch: 42\n",
      "Loss: 0.80710477\n",
      "Batch: 43\n",
      "Loss: 0.8054857\n",
      "Batch: 44\n",
      "Loss: 0.8038692\n",
      "Batch: 45\n",
      "Loss: 0.8022554\n",
      "Batch: 46\n",
      "Loss: 0.80064434\n",
      "Batch: 47\n",
      "Loss: 0.79903597\n",
      "Batch: 48\n",
      "Loss: 0.79743046\n",
      "Batch: 49\n",
      "Loss: 0.7958277\n",
      "Batch: 50\n",
      "Loss: 0.79422784\n",
      "Batch: 51\n",
      "Loss: 0.7926311\n",
      "Batch: 52\n",
      "Loss: 0.79103696\n",
      "Batch: 53\n",
      "Loss: 0.789446\n",
      "Batch: 54\n",
      "Loss: 0.7878579\n",
      "Batch: 55\n",
      "Loss: 0.786273\n",
      "Batch: 56\n",
      "Loss: 0.784691\n",
      "Batch: 57\n",
      "Loss: 0.783112\n",
      "Batch: 58\n",
      "Loss: 0.7815361\n",
      "Batch: 59\n",
      "Loss: 0.7799631\n",
      "Batch: 60\n",
      "Loss: 0.7783933\n",
      "Batch: 61\n",
      "Loss: 0.7768266\n",
      "Batch: 62\n",
      "Loss: 0.77526295\n",
      "Batch: 63\n",
      "Loss: 0.7737023\n",
      "Batch: 64\n",
      "Loss: 0.77214473\n",
      "Batch: 65\n",
      "Loss: 0.7705903\n",
      "Batch: 66\n",
      "Loss: 0.7690388\n",
      "Batch: 67\n",
      "Loss: 0.7674905\n",
      "Batch: 68\n",
      "Loss: 0.7659451\n",
      "Batch: 69\n",
      "Loss: 0.7644027\n",
      "Batch: 70\n",
      "Loss: 0.7628635\n",
      "Batch: 71\n",
      "Loss: 0.76132727\n",
      "Batch: 72\n",
      "Loss: 0.7597941\n",
      "Batch: 73\n",
      "Loss: 0.7582639\n",
      "Batch: 74\n",
      "Loss: 0.7567368\n",
      "Batch: 75\n",
      "Loss: 0.75521266\n",
      "Batch: 76\n",
      "Loss: 0.7536915\n",
      "Batch: 77\n",
      "Train accuracy: 0.999198717948718\n",
      "Validation accuracy: 0.6820652173913043\n",
      "Loss: 1.3868016\n",
      "Batch: 0\n",
      "Loss: 1.0358479\n",
      "Batch: 1\n",
      "Loss: 0.8219839\n",
      "Batch: 2\n",
      "Loss: 0.8144639\n",
      "Batch: 3\n",
      "Loss: 0.7964127\n",
      "Batch: 4\n",
      "Loss: 0.78090435\n",
      "Batch: 5\n",
      "Loss: 0.7715162\n",
      "Batch: 6\n",
      "Loss: 0.76583576\n",
      "Batch: 7\n",
      "Loss: 0.76113707\n",
      "Batch: 8\n",
      "Loss: 0.7564961\n",
      "Batch: 9\n",
      "Loss: 0.7524457\n",
      "Batch: 10\n",
      "Loss: 0.749241\n",
      "Batch: 11\n",
      "Loss: 0.746696\n",
      "Batch: 12\n",
      "Loss: 0.74451256\n",
      "Batch: 13\n",
      "Loss: 0.74247783\n",
      "Batch: 14\n",
      "Loss: 0.7405241\n",
      "Batch: 15\n",
      "Loss: 0.73869\n",
      "Batch: 16\n",
      "Loss: 0.73703057\n",
      "Batch: 17\n",
      "Loss: 0.7355357\n",
      "Batch: 18\n",
      "Loss: 0.7341207\n",
      "Batch: 19\n",
      "Loss: 0.73270786\n",
      "Batch: 20\n",
      "Loss: 0.73128664\n",
      "Batch: 21\n",
      "Loss: 0.7298837\n",
      "Batch: 22\n",
      "Loss: 0.72850144\n",
      "Batch: 23\n",
      "Loss: 0.72712314\n",
      "Batch: 24\n",
      "Loss: 0.72574264\n",
      "Batch: 25\n",
      "Loss: 0.72436726\n",
      "Batch: 26\n",
      "Loss: 0.7230025\n",
      "Batch: 27\n",
      "Loss: 0.7216453\n",
      "Batch: 28\n",
      "Loss: 0.72029066\n",
      "Batch: 29\n",
      "Loss: 0.71893543\n",
      "Batch: 30\n",
      "Loss: 0.71757877\n",
      "Batch: 31\n",
      "Loss: 0.71622086\n",
      "Batch: 32\n",
      "Loss: 0.714862\n",
      "Batch: 33\n",
      "Loss: 0.7135021\n",
      "Batch: 34\n",
      "Loss: 0.7121406\n",
      "Batch: 35\n",
      "Loss: 0.71077716\n",
      "Batch: 36\n",
      "Loss: 0.7094109\n",
      "Batch: 37\n",
      "Loss: 0.70804167\n",
      "Batch: 38\n",
      "Loss: 0.70666975\n",
      "Batch: 39\n",
      "Loss: 0.70529586\n",
      "Batch: 40\n",
      "Loss: 0.7039206\n",
      "Batch: 41\n",
      "Loss: 0.70254457\n",
      "Batch: 42\n",
      "Loss: 0.7011685\n",
      "Batch: 43\n",
      "Loss: 0.6997928\n",
      "Batch: 44\n",
      "Loss: 0.698418\n",
      "Batch: 45\n",
      "Loss: 0.6970443\n",
      "Batch: 46\n",
      "Loss: 0.6956722\n",
      "Batch: 47\n",
      "Loss: 0.6943015\n",
      "Batch: 48\n",
      "Loss: 0.69293237\n",
      "Batch: 49\n",
      "Loss: 0.69156504\n",
      "Batch: 50\n",
      "Loss: 0.69019955\n",
      "Batch: 51\n",
      "Loss: 0.6888358\n",
      "Batch: 52\n",
      "Loss: 0.68747413\n",
      "Batch: 53\n",
      "Loss: 0.6861147\n",
      "Batch: 54\n",
      "Loss: 0.68475765\n",
      "Batch: 55\n",
      "Loss: 0.68340284\n",
      "Batch: 56\n",
      "Loss: 0.6820505\n",
      "Batch: 57\n",
      "Loss: 0.6807009\n",
      "Batch: 58\n",
      "Loss: 0.67935383\n",
      "Batch: 59\n",
      "Loss: 0.67800945\n",
      "Batch: 60\n",
      "Loss: 0.6766679\n",
      "Batch: 61\n",
      "Loss: 0.67532897\n",
      "Batch: 62\n",
      "Loss: 0.6739928\n",
      "Batch: 63\n",
      "Loss: 0.6726593\n",
      "Batch: 64\n",
      "Loss: 0.67132837\n",
      "Batch: 65\n",
      "Loss: 0.67000026\n",
      "Batch: 66\n",
      "Loss: 0.66867477\n",
      "Batch: 67\n",
      "Loss: 0.667352\n",
      "Batch: 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6660321\n",
      "Batch: 69\n",
      "Loss: 0.66471463\n",
      "Batch: 70\n",
      "Loss: 0.66340005\n",
      "Batch: 71\n",
      "Loss: 0.66208804\n",
      "Batch: 72\n",
      "Loss: 0.6607786\n",
      "Batch: 73\n",
      "Loss: 0.6594721\n",
      "Batch: 74\n",
      "Loss: 0.65816814\n",
      "Batch: 75\n",
      "Loss: 0.6568668\n",
      "Batch: 76\n",
      "Loss: 0.6555682\n",
      "Batch: 77\n",
      "Train accuracy: 0.9983974358974359\n",
      "Validation accuracy: 0.7088994565217391\n",
      "Loss: 1.185165\n",
      "Batch: 0\n",
      "Loss: 0.92216676\n",
      "Batch: 1\n",
      "Loss: 0.74742746\n",
      "Batch: 2\n",
      "Loss: 0.7317742\n",
      "Batch: 3\n",
      "Loss: 0.7031685\n",
      "Batch: 4\n",
      "Loss: 0.68872476\n",
      "Batch: 5\n",
      "Loss: 0.68291694\n",
      "Batch: 6\n",
      "Loss: 0.6806782\n",
      "Batch: 7\n",
      "Loss: 0.6785071\n",
      "Batch: 8\n",
      "Loss: 0.674973\n",
      "Batch: 9\n",
      "Loss: 0.67100304\n",
      "Batch: 10\n",
      "Loss: 0.6676639\n",
      "Batch: 11\n",
      "Loss: 0.665144\n",
      "Batch: 12\n",
      "Loss: 0.66321176\n",
      "Batch: 13\n",
      "Loss: 0.6616208\n",
      "Batch: 14\n",
      "Loss: 0.6601999\n",
      "Batch: 15\n",
      "Loss: 0.6588511\n",
      "Batch: 16\n",
      "Loss: 0.657523\n",
      "Batch: 17\n",
      "Loss: 0.6561891\n",
      "Batch: 18\n",
      "Loss: 0.6548443\n",
      "Batch: 19\n",
      "Loss: 0.6534978\n",
      "Batch: 20\n",
      "Loss: 0.65216553\n",
      "Batch: 21\n",
      "Loss: 0.6508656\n",
      "Batch: 22\n",
      "Loss: 0.6496026\n",
      "Batch: 23\n",
      "Loss: 0.6483559\n",
      "Batch: 24\n",
      "Loss: 0.6471102\n",
      "Batch: 25\n",
      "Loss: 0.6458692\n",
      "Batch: 26\n",
      "Loss: 0.64464\n",
      "Batch: 27\n",
      "Loss: 0.6434247\n",
      "Batch: 28\n",
      "Loss: 0.64222014\n",
      "Batch: 29\n",
      "Loss: 0.64102155\n",
      "Batch: 30\n",
      "Loss: 0.6398255\n",
      "Batch: 31\n",
      "Loss: 0.63862973\n",
      "Batch: 32\n",
      "Loss: 0.6374339\n",
      "Batch: 33\n",
      "Loss: 0.63623816\n",
      "Batch: 34\n",
      "Loss: 0.6350436\n",
      "Batch: 35\n",
      "Loss: 0.6338509\n",
      "Batch: 36\n",
      "Loss: 0.63266057\n",
      "Batch: 37\n",
      "Loss: 0.6314725\n",
      "Batch: 38\n",
      "Loss: 0.63028574\n",
      "Batch: 39\n",
      "Loss: 0.62909985\n",
      "Batch: 40\n",
      "Loss: 0.62791455\n",
      "Batch: 41\n",
      "Loss: 0.62672985\n",
      "Batch: 42\n",
      "Loss: 0.62554675\n",
      "Batch: 43\n",
      "Loss: 0.62436545\n",
      "Batch: 44\n",
      "Loss: 0.62318647\n",
      "Batch: 45\n",
      "Loss: 0.62200963\n",
      "Batch: 46\n",
      "Loss: 0.62083495\n",
      "Batch: 47\n",
      "Loss: 0.6196621\n",
      "Batch: 48\n",
      "Loss: 0.61849123\n",
      "Batch: 49\n",
      "Loss: 0.6173219\n",
      "Batch: 50\n",
      "Loss: 0.61615443\n",
      "Batch: 51\n",
      "Loss: 0.61498904\n",
      "Batch: 52\n",
      "Loss: 0.61382556\n",
      "Batch: 53\n",
      "Loss: 0.6126644\n",
      "Batch: 54\n",
      "Loss: 0.61150557\n",
      "Batch: 55\n",
      "Loss: 0.61034894\n",
      "Batch: 56\n",
      "Loss: 0.60919446\n",
      "Batch: 57\n",
      "Loss: 0.6080422\n",
      "Batch: 58\n",
      "Loss: 0.60689217\n",
      "Batch: 59\n",
      "Loss: 0.6057441\n",
      "Batch: 60\n",
      "Loss: 0.60459834\n",
      "Batch: 61\n",
      "Loss: 0.6034547\n",
      "Batch: 62\n",
      "Loss: 0.60231334\n",
      "Batch: 63\n",
      "Loss: 0.60117424\n",
      "Batch: 64\n",
      "Loss: 0.60003734\n",
      "Batch: 65\n",
      "Loss: 0.5989028\n",
      "Batch: 66\n",
      "Loss: 0.5977704\n",
      "Batch: 67\n",
      "Loss: 0.59664017\n",
      "Batch: 68\n",
      "Loss: 0.5955123\n",
      "Batch: 69\n",
      "Loss: 0.5943864\n",
      "Batch: 70\n",
      "Loss: 0.59326273\n",
      "Batch: 71\n",
      "Loss: 0.59214133\n",
      "Batch: 72\n",
      "Loss: 0.591022\n",
      "Batch: 73\n",
      "Loss: 0.589905\n",
      "Batch: 74\n",
      "Loss: 0.5887903\n",
      "Batch: 75\n",
      "Loss: 0.5876776\n",
      "Batch: 76\n",
      "Loss: 0.5865672\n",
      "Batch: 77\n",
      "Train accuracy: 0.9988982371794872\n",
      "Validation accuracy: 0.7068614130434783\n",
      "Loss: 1.3137329\n",
      "Batch: 0\n",
      "Loss: 0.8412391\n",
      "Batch: 1\n",
      "Loss: 0.68440145\n",
      "Batch: 2\n",
      "Loss: 0.6211965\n",
      "Batch: 3\n",
      "Loss: 0.6081442\n",
      "Batch: 4\n",
      "Loss: 0.60226697\n",
      "Batch: 5\n",
      "Loss: 0.59443396\n",
      "Batch: 6\n",
      "Loss: 0.58770174\n",
      "Batch: 7\n",
      "Loss: 0.5832158\n",
      "Batch: 8\n",
      "Loss: 0.58011526\n",
      "Batch: 9\n",
      "Loss: 0.5774893\n",
      "Batch: 10\n",
      "Loss: 0.5752469\n",
      "Batch: 11\n",
      "Loss: 0.5734676\n",
      "Batch: 12\n",
      "Loss: 0.5719652\n",
      "Batch: 13\n",
      "Loss: 0.5705218\n",
      "Batch: 14\n",
      "Loss: 0.56911635\n",
      "Batch: 15\n",
      "Loss: 0.56782603\n",
      "Batch: 16\n",
      "Loss: 0.56666505\n",
      "Batch: 17\n",
      "Loss: 0.56557924\n",
      "Batch: 18\n",
      "Loss: 0.5645136\n",
      "Batch: 19\n",
      "Loss: 0.56345206\n",
      "Batch: 20\n",
      "Loss: 0.56240505\n",
      "Batch: 21\n",
      "Loss: 0.56138355\n",
      "Batch: 22\n",
      "Loss: 0.5603892\n",
      "Batch: 23\n",
      "Loss: 0.55941635\n",
      "Batch: 24\n",
      "Loss: 0.5584558\n",
      "Batch: 25\n",
      "Loss: 0.55749923\n",
      "Batch: 26\n",
      "Loss: 0.55653954\n",
      "Batch: 27\n",
      "Loss: 0.555572\n",
      "Batch: 28\n",
      "Loss: 0.554595\n",
      "Batch: 29\n",
      "Loss: 0.5536081\n",
      "Batch: 30\n",
      "Loss: 0.552613\n",
      "Batch: 31\n",
      "Loss: 0.5516114\n",
      "Batch: 32\n",
      "Loss: 0.55060554\n",
      "Batch: 33\n",
      "Loss: 0.54959667\n",
      "Batch: 34\n",
      "Loss: 0.548586\n",
      "Batch: 35\n",
      "Loss: 0.54757386\n",
      "Batch: 36\n",
      "Loss: 0.5465605\n",
      "Batch: 37\n",
      "Loss: 0.5455459\n",
      "Batch: 38\n",
      "Loss: 0.5445298\n",
      "Batch: 39\n",
      "Loss: 0.5435124\n",
      "Batch: 40\n",
      "Loss: 0.54249394\n",
      "Batch: 41\n",
      "Loss: 0.5414747\n",
      "Batch: 42\n",
      "Loss: 0.54045504\n",
      "Batch: 43\n",
      "Loss: 0.5394353\n",
      "Batch: 44\n",
      "Loss: 0.5384159\n",
      "Batch: 45\n",
      "Loss: 0.5373973\n",
      "Batch: 46\n",
      "Loss: 0.53637964\n",
      "Batch: 47\n",
      "Loss: 0.53536314\n",
      "Batch: 48\n",
      "Loss: 0.53434795\n",
      "Batch: 49\n",
      "Loss: 0.53333426\n",
      "Batch: 50\n",
      "Loss: 0.53232193\n",
      "Batch: 51\n",
      "Loss: 0.531311\n",
      "Batch: 52\n",
      "Loss: 0.53030163\n",
      "Batch: 53\n",
      "Loss: 0.52929384\n",
      "Batch: 54\n",
      "Loss: 0.5282874\n",
      "Batch: 55\n",
      "Loss: 0.5272827\n",
      "Batch: 56\n",
      "Loss: 0.5262796\n",
      "Batch: 57\n",
      "Loss: 0.5252782\n",
      "Batch: 58\n",
      "Loss: 0.5242784\n",
      "Batch: 59\n",
      "Loss: 0.5232806\n",
      "Batch: 60\n",
      "Loss: 0.5222845\n",
      "Batch: 61\n",
      "Loss: 0.5212903\n",
      "Batch: 62\n",
      "Loss: 0.52029806\n",
      "Batch: 63\n",
      "Loss: 0.5193077\n",
      "Batch: 64\n",
      "Loss: 0.51831925\n",
      "Batch: 65\n",
      "Loss: 0.5173329\n",
      "Batch: 66\n",
      "Loss: 0.5163484\n",
      "Batch: 67\n",
      "Loss: 0.515366\n",
      "Batch: 68\n",
      "Loss: 0.5143854\n",
      "Batch: 69\n",
      "Loss: 0.5134068\n",
      "Batch: 70\n",
      "Loss: 0.51243013\n",
      "Batch: 71\n",
      "Loss: 0.5114554\n",
      "Batch: 72\n",
      "Loss: 0.5104826\n",
      "Batch: 73\n",
      "Loss: 0.50951177\n",
      "Batch: 74\n",
      "Loss: 0.50854284\n",
      "Batch: 75\n",
      "Loss: 0.50757575\n",
      "Batch: 76\n",
      "Loss: 0.5066107\n",
      "Batch: 77\n",
      "Train accuracy: 0.9987980769230769\n",
      "Validation accuracy: 0.7238451086956522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.7238451086956522"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at provided point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8356854838709677\n",
      "Epoch: 9\n",
      "Iteration No: 1 ended. Evaluation done at provided point.\n",
      "Time taken: 67.5356\n",
      "Function value obtained: -0.8357\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.5047043010752688\n",
      "Epoch: 9\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 99.9424\n",
      "Function value obtained: -0.5047\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7207661290322581\n",
      "Epoch: 9\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 94.2459\n",
      "Function value obtained: -0.7208\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7318548387096774\n",
      "Epoch: 9\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 90.3353\n",
      "Function value obtained: -0.7319\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7711693548387096\n",
      "Epoch: 9\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 92.4866\n",
      "Function value obtained: -0.7712\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.5618279569892473\n",
      "Epoch: 9\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 116.6074\n",
      "Function value obtained: -0.5618\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7570564516129032\n",
      "Epoch: 9\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 77.7473\n",
      "Function value obtained: -0.7571\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7795698924731181\n",
      "Epoch: 9\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 52.2262\n",
      "Function value obtained: -0.7796\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.6750672043010753\n",
      "Epoch: 9\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 116.9493\n",
      "Function value obtained: -0.6751\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8252688172043011\n",
      "Epoch: 9\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 65.1643\n",
      "Function value obtained: -0.8253\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8141801075268817\n",
      "Epoch: 9\n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 117.8329\n",
      "Function value obtained: -0.8142\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.5997983870967742\n",
      "Epoch: 9\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 83.7734\n",
      "Function value obtained: -0.5998\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.6599462365591398\n",
      "Epoch: 9\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 125.5010\n",
      "Function value obtained: -0.6599\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.835013440860215\n",
      "Epoch: 9\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 73.4676\n",
      "Function value obtained: -0.8350\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8178763440860215\n",
      "Epoch: 9\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 65.6189\n",
      "Function value obtained: -0.8179\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8172043010752689\n",
      "Epoch: 9\n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 81.3381\n",
      "Function value obtained: -0.8172\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7113575268817205\n",
      "Epoch: 9\n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 86.3110\n",
      "Function value obtained: -0.7114\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.4946236559139785\n",
      "Epoch: 9\n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 115.4719\n",
      "Function value obtained: -0.4946\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.5709005376344086\n",
      "Epoch: 9\n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 91.3372\n",
      "Function value obtained: -0.5709\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7110215053763441\n",
      "Epoch: 9\n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 82.3851\n",
      "Function value obtained: -0.7110\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8252688172043011\n",
      "Epoch: 9\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 63.4345\n",
      "Function value obtained: -0.8253\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7012768817204301\n",
      "Epoch: 9\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 82.8363\n",
      "Function value obtained: -0.7013\n",
      "Current minimum: -0.8357\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.842741935483871\n",
      "Epoch: 9\n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 84.8025\n",
      "Function value obtained: -0.8427\n",
      "Current minimum: -0.8427\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8343413978494624\n",
      "Epoch: 9\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 86.5389\n",
      "Function value obtained: -0.8343\n",
      "Current minimum: -0.8427\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.4946236559139785\n",
      "Epoch: 9\n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 65.8027\n",
      "Function value obtained: -0.4946\n",
      "Current minimum: -0.8427\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.5903897849462365\n",
      "Epoch: 9\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 83.6021\n",
      "Function value obtained: -0.5904\n",
      "Current minimum: -0.8427\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.49731182795698925\n",
      "Epoch: 9\n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 121.1559\n",
      "Function value obtained: -0.4973\n",
      "Current minimum: -0.8427\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8413978494623656\n",
      "Epoch: 9\n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 78.4113\n",
      "Function value obtained: -0.8414\n",
      "Current minimum: -0.8427\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8377016129032258\n",
      "Epoch: 9\n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 82.4795\n",
      "Function value obtained: -0.8377\n",
      "Current minimum: -0.8427\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8272849462365591\n",
      "Epoch: 9\n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 80.5682\n",
      "Function value obtained: -0.8273\n",
      "Current minimum: -0.8427\n",
      "Iteration No: 31 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.84375\n",
      "Epoch: 9\n",
      "Iteration No: 31 ended. Search finished for the next optimal point.\n",
      "Time taken: 75.3616\n",
      "Function value obtained: -0.8438\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 32 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7960349462365591\n",
      "Epoch: 9\n",
      "Iteration No: 32 ended. Search finished for the next optimal point.\n",
      "Time taken: 76.9660\n",
      "Function value obtained: -0.7960\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 33 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7557123655913979\n",
      "Epoch: 9\n",
      "Iteration No: 33 ended. Search finished for the next optimal point.\n",
      "Time taken: 81.5258\n",
      "Function value obtained: -0.7557\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 34 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.5782930107526881\n",
      "Epoch: 9\n",
      "Iteration No: 34 ended. Search finished for the next optimal point.\n",
      "Time taken: 69.0291\n",
      "Function value obtained: -0.5783\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 35 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8430779569892473\n",
      "Epoch: 9\n",
      "Iteration No: 35 ended. Search finished for the next optimal point.\n",
      "Time taken: 77.5157\n",
      "Function value obtained: -0.8431\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 36 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7422715053763441\n",
      "Epoch: 9\n",
      "Iteration No: 36 ended. Search finished for the next optimal point.\n",
      "Time taken: 72.0428\n",
      "Function value obtained: -0.7423\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 37 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.813508064516129\n",
      "Epoch: 9\n",
      "Iteration No: 37 ended. Search finished for the next optimal point.\n",
      "Time taken: 73.5227\n",
      "Function value obtained: -0.8135\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 38 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8333333333333334\n",
      "Epoch: 9\n",
      "Iteration No: 38 ended. Search finished for the next optimal point.\n",
      "Time taken: 105.9108\n",
      "Function value obtained: -0.8333\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 39 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.6542338709677419\n",
      "Epoch: 9\n",
      "Iteration No: 39 ended. Search finished for the next optimal point.\n",
      "Time taken: 53.3030\n",
      "Function value obtained: -0.6542\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 40 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.6350806451612905\n",
      "Epoch: 9\n",
      "Iteration No: 40 ended. Search finished for the next optimal point.\n",
      "Time taken: 58.3189\n",
      "Function value obtained: -0.6351\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 41 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7856182795698924\n",
      "Epoch: 9\n",
      "Iteration No: 41 ended. Search finished for the next optimal point.\n",
      "Time taken: 72.6056\n",
      "Function value obtained: -0.7856\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 42 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.6058467741935486\n",
      "Epoch: 9\n",
      "Iteration No: 42 ended. Search finished for the next optimal point.\n",
      "Time taken: 94.8096\n",
      "Function value obtained: -0.6058\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 43 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7160618279569892\n",
      "Epoch: 9\n",
      "Iteration No: 43 ended. Search finished for the next optimal point.\n",
      "Time taken: 116.1366\n",
      "Function value obtained: -0.7161\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 44 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8172043010752689\n",
      "Epoch: 9\n",
      "Iteration No: 44 ended. Search finished for the next optimal point.\n",
      "Time taken: 81.3199\n",
      "Function value obtained: -0.8172\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 45 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7889784946236557\n",
      "Epoch: 9\n",
      "Iteration No: 45 ended. Search finished for the next optimal point.\n",
      "Time taken: 98.3075\n",
      "Function value obtained: -0.7890\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 46 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.801747311827957\n",
      "Epoch: 9\n",
      "Iteration No: 46 ended. Search finished for the next optimal point.\n",
      "Time taken: 88.6838\n",
      "Function value obtained: -0.8017\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 47 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8286290322580645\n",
      "Epoch: 9\n",
      "Iteration No: 47 ended. Search finished for the next optimal point.\n",
      "Time taken: 79.4859\n",
      "Function value obtained: -0.8286\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 48 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.5191532258064516\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 48 ended. Search finished for the next optimal point.\n",
      "Time taken: 83.2670\n",
      "Function value obtained: -0.5192\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 49 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7799059139784946\n",
      "Epoch: 9\n",
      "Iteration No: 49 ended. Search finished for the next optimal point.\n",
      "Time taken: 93.5671\n",
      "Function value obtained: -0.7799\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 50 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8145161290322581\n",
      "Epoch: 9\n",
      "Iteration No: 50 ended. Search finished for the next optimal point.\n",
      "Time taken: 80.1725\n",
      "Function value obtained: -0.8145\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 51 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8427419354838711\n",
      "Epoch: 9\n",
      "Iteration No: 51 ended. Search finished for the next optimal point.\n",
      "Time taken: 81.3922\n",
      "Function value obtained: -0.8427\n",
      "Current minimum: -0.8438\n",
      "Iteration No: 52 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8457661290322579\n",
      "Epoch: 9\n",
      "Iteration No: 52 ended. Search finished for the next optimal point.\n",
      "Time taken: 95.0335\n",
      "Function value obtained: -0.8458\n",
      "Current minimum: -0.8458\n",
      "Iteration No: 53 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.802755376344086\n",
      "Epoch: 9\n",
      "Iteration No: 53 ended. Search finished for the next optimal point.\n",
      "Time taken: 105.6121\n",
      "Function value obtained: -0.8028\n",
      "Current minimum: -0.8458\n",
      "Iteration No: 54 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8360215053763441\n",
      "Epoch: 9\n",
      "Iteration No: 54 ended. Search finished for the next optimal point.\n",
      "Time taken: 103.0624\n",
      "Function value obtained: -0.8360\n",
      "Current minimum: -0.8458\n",
      "Iteration No: 55 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.511760752688172\n",
      "Epoch: 9\n",
      "Iteration No: 55 ended. Search finished for the next optimal point.\n",
      "Time taken: 94.2705\n",
      "Function value obtained: -0.5118\n",
      "Current minimum: -0.8458\n",
      "Iteration No: 56 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7782258064516129\n",
      "Epoch: 9\n",
      "Iteration No: 56 ended. Search finished for the next optimal point.\n",
      "Time taken: 89.1583\n",
      "Function value obtained: -0.7782\n",
      "Current minimum: -0.8458\n",
      "Iteration No: 57 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.8336693548387093\n",
      "Epoch: 9\n",
      "Iteration No: 57 ended. Search finished for the next optimal point.\n",
      "Time taken: 97.3726\n",
      "Function value obtained: -0.8337\n",
      "Current minimum: -0.8458\n",
      "Iteration No: 58 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7946908602150539\n",
      "Epoch: 9\n",
      "Iteration No: 58 ended. Search finished for the next optimal point.\n",
      "Time taken: 70.7095\n",
      "Function value obtained: -0.7947\n",
      "Current minimum: -0.8458\n",
      "Iteration No: 59 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.7983870967741935\n",
      "Epoch: 9\n",
      "Iteration No: 59 ended. Search finished for the next optimal point.\n",
      "Time taken: 110.6060\n",
      "Function value obtained: -0.7984\n",
      "Current minimum: -0.8458\n",
      "Iteration No: 60 started. Searching for the next optimal point.\n",
      "!\n",
      "training\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Validation accuracy: 0.831989247311828\n",
      "Epoch: 9\n",
      "Iteration No: 60 ended. Search finished for the next optimal point.\n",
      "Time taken: 75.3690\n",
      "Function value obtained: -0.8320\n",
      "Current minimum: -0.8458\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=60,\n",
    "                            x0=default_parameters,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f267da90630>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 60 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.8457661290322579,\n",
       "  [5.490651011229577e-05,\n",
       "   1.271441508724827e-06,\n",
       "   1e-08,\n",
       "   2.7342798595232468e-05,\n",
       "   0.005638904645963502,\n",
       "   0.00048774652028289105,\n",
       "   5,\n",
       "   12,\n",
       "   7,\n",
       "   2,\n",
       "   64,\n",
       "   112,\n",
       "   96,\n",
       "   144,\n",
       "   1536,\n",
       "   0.5744831333017718,\n",
       "   0.9999,\n",
       "   48]),\n",
       " (-0.84375,\n",
       "  [0.00010504113129066549,\n",
       "   0.00022022031107525204,\n",
       "   1.3524799530744914e-07,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   7.796148534881568e-06,\n",
       "   11,\n",
       "   12,\n",
       "   5,\n",
       "   4,\n",
       "   64,\n",
       "   48,\n",
       "   112,\n",
       "   192,\n",
       "   1024,\n",
       "   0.27896433738705284,\n",
       "   0.6468398543538967,\n",
       "   32]),\n",
       " (-0.8430779569892473,\n",
       "  [9.458575571312315e-05,\n",
       "   2.9154611653801043e-06,\n",
       "   1.7328553111633675e-07,\n",
       "   6.967137793206209e-08,\n",
       "   4.6208665422528896e-08,\n",
       "   3.2619665303795646e-06,\n",
       "   12,\n",
       "   12,\n",
       "   8,\n",
       "   2,\n",
       "   48,\n",
       "   48,\n",
       "   112,\n",
       "   192,\n",
       "   2560,\n",
       "   0.05234801798230036,\n",
       "   0.9422340814482131,\n",
       "   32]),\n",
       " (-0.8427419354838711,\n",
       "  [0.00011021598610503274,\n",
       "   1.486602508822446e-05,\n",
       "   4.969388186479319e-06,\n",
       "   0.0011043009424972741,\n",
       "   4.400839707187273e-06,\n",
       "   1.6573127874582863e-05,\n",
       "   12,\n",
       "   11,\n",
       "   7,\n",
       "   3,\n",
       "   64,\n",
       "   64,\n",
       "   80,\n",
       "   160,\n",
       "   2304,\n",
       "   1e-05,\n",
       "   0.8644508798762932,\n",
       "   48]),\n",
       " (-0.842741935483871,\n",
       "  [0.00010730204299876964,\n",
       "   1.138125981267018e-07,\n",
       "   7.122935672975829e-06,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   2.113246415492299e-08,\n",
       "   12,\n",
       "   11,\n",
       "   5,\n",
       "   4,\n",
       "   64,\n",
       "   48,\n",
       "   144,\n",
       "   192,\n",
       "   1024,\n",
       "   0.7160228780171382,\n",
       "   0.9430554444992874,\n",
       "   32]),\n",
       " (-0.8413978494623656,\n",
       "  [0.00011649377778053281,\n",
       "   1e-08,\n",
       "   2.397994737944248e-06,\n",
       "   1e-08,\n",
       "   4.3259561501766666e-05,\n",
       "   0.0009091024804463166,\n",
       "   12,\n",
       "   11,\n",
       "   5,\n",
       "   3,\n",
       "   64,\n",
       "   48,\n",
       "   144,\n",
       "   192,\n",
       "   1024,\n",
       "   0.22362814428009992,\n",
       "   0.888802912284881,\n",
       "   32]),\n",
       " (-0.8377016129032258,\n",
       "  [0.0001191619104978813,\n",
       "   1e-08,\n",
       "   8.563787882302707e-06,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   12,\n",
       "   11,\n",
       "   5,\n",
       "   2,\n",
       "   64,\n",
       "   48,\n",
       "   144,\n",
       "   192,\n",
       "   1024,\n",
       "   0.7094489594536884,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.8360215053763441,\n",
       "  [4.076556562208231e-05,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   0.009147852616642796,\n",
       "   8,\n",
       "   12,\n",
       "   8,\n",
       "   3,\n",
       "   64,\n",
       "   128,\n",
       "   48,\n",
       "   80,\n",
       "   1536,\n",
       "   0.21743492344214138,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.8356854838709677,\n",
       "  [8.824e-05,\n",
       "   1e-08,\n",
       "   1.327e-08,\n",
       "   1e-08,\n",
       "   5.0997e-05,\n",
       "   1e-08,\n",
       "   6,\n",
       "   10,\n",
       "   8,\n",
       "   3,\n",
       "   64,\n",
       "   32,\n",
       "   96,\n",
       "   160,\n",
       "   2560,\n",
       "   0.739,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.835013440860215,\n",
       "  [7.892675411902018e-05,\n",
       "   2.5345685378739517e-07,\n",
       "   3.895537375102736e-06,\n",
       "   1.068924686596689e-06,\n",
       "   7.617785012956353e-05,\n",
       "   1.2710348581139885e-08,\n",
       "   6,\n",
       "   11,\n",
       "   7,\n",
       "   5,\n",
       "   64,\n",
       "   32,\n",
       "   96,\n",
       "   160,\n",
       "   2560,\n",
       "   0.2809056548352467,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.8343413978494624,\n",
       "  [6.698496527817035e-05,\n",
       "   1.4158822631287776e-05,\n",
       "   7.479692657789584e-07,\n",
       "   0.028512401095979557,\n",
       "   6.026558367721699e-06,\n",
       "   2.2092091418028693e-05,\n",
       "   12,\n",
       "   11,\n",
       "   6,\n",
       "   3,\n",
       "   64,\n",
       "   48,\n",
       "   160,\n",
       "   192,\n",
       "   512,\n",
       "   0.23754557000003723,\n",
       "   0.8609409231836299,\n",
       "   32]),\n",
       " (-0.8336693548387093,\n",
       "  [8.458685784567106e-05,\n",
       "   4.107513404946546e-08,\n",
       "   0.00035118351403359664,\n",
       "   1.3109429797663652e-07,\n",
       "   1.0341170226362464e-07,\n",
       "   1.9406796085460828e-08,\n",
       "   11,\n",
       "   8,\n",
       "   6,\n",
       "   3,\n",
       "   64,\n",
       "   128,\n",
       "   144,\n",
       "   64,\n",
       "   2048,\n",
       "   0.5473303378085463,\n",
       "   0.8915013401652054,\n",
       "   48]),\n",
       " (-0.8333333333333334,\n",
       "  [0.00016281906783942085,\n",
       "   0.00021457330749527197,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   1.3970674452402375e-05,\n",
       "   1e-08,\n",
       "   10,\n",
       "   11,\n",
       "   5,\n",
       "   2,\n",
       "   64,\n",
       "   128,\n",
       "   160,\n",
       "   160,\n",
       "   1536,\n",
       "   0.3109857134834611,\n",
       "   0.8119523263507111,\n",
       "   32]),\n",
       " (-0.831989247311828,\n",
       "  [0.00018969276307215417,\n",
       "   1e-08,\n",
       "   2.644498110119269e-07,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   3.872877599843058e-06,\n",
       "   6,\n",
       "   12,\n",
       "   7,\n",
       "   2,\n",
       "   64,\n",
       "   48,\n",
       "   160,\n",
       "   96,\n",
       "   1024,\n",
       "   0.3819223425860745,\n",
       "   0.7759832155990695,\n",
       "   48]),\n",
       " (-0.8286290322580645,\n",
       "  [7.148147829711113e-05,\n",
       "   0.003535246192408111,\n",
       "   1.3356045565013733e-05,\n",
       "   0.001257303058314565,\n",
       "   1e-08,\n",
       "   0.00030120411281804664,\n",
       "   10,\n",
       "   11,\n",
       "   7,\n",
       "   3,\n",
       "   64,\n",
       "   48,\n",
       "   160,\n",
       "   16,\n",
       "   2304,\n",
       "   0.5952389414775963,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.8272849462365591,\n",
       "  [8.316977698922832e-05,\n",
       "   1.0901639032385663e-07,\n",
       "   4.4377854407355177e-07,\n",
       "   1e-08,\n",
       "   0.004195209232953196,\n",
       "   6.023023309379821e-06,\n",
       "   12,\n",
       "   11,\n",
       "   6,\n",
       "   5,\n",
       "   64,\n",
       "   48,\n",
       "   144,\n",
       "   144,\n",
       "   1024,\n",
       "   0.20071404164932966,\n",
       "   0.5071338684689515,\n",
       "   32]),\n",
       " (-0.8252688172043011,\n",
       "  [8.150027661405192e-05,\n",
       "   0.002335425717932047,\n",
       "   4.56758617645867e-08,\n",
       "   0.00040873635723883344,\n",
       "   4.269402723538348e-07,\n",
       "   0.0034939686585351505,\n",
       "   10,\n",
       "   11,\n",
       "   7,\n",
       "   3,\n",
       "   64,\n",
       "   48,\n",
       "   64,\n",
       "   192,\n",
       "   1024,\n",
       "   0.88631788955759,\n",
       "   0.727290416863022,\n",
       "   48]),\n",
       " (-0.8252688172043011,\n",
       "  [0.00013205239760262635,\n",
       "   0.006728863051879193,\n",
       "   0.06960730199379031,\n",
       "   2.3596106957958022e-08,\n",
       "   1.7853359176740216e-05,\n",
       "   0.00033386114350768547,\n",
       "   11,\n",
       "   10,\n",
       "   5,\n",
       "   5,\n",
       "   48,\n",
       "   48,\n",
       "   64,\n",
       "   160,\n",
       "   1536,\n",
       "   0.23480562725584408,\n",
       "   0.5430078577849745,\n",
       "   32]),\n",
       " (-0.8178763440860215,\n",
       "  [7.53266658411773e-05,\n",
       "   1e-08,\n",
       "   0.011853722103269532,\n",
       "   1e-08,\n",
       "   0.0008166497328894271,\n",
       "   1e-08,\n",
       "   4,\n",
       "   11,\n",
       "   8,\n",
       "   5,\n",
       "   48,\n",
       "   32,\n",
       "   96,\n",
       "   160,\n",
       "   2560,\n",
       "   0.4919871964063874,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.8172043010752689,\n",
       "  [8.728500373573816e-05,\n",
       "   2.7152332401319808e-05,\n",
       "   3.365752819169001e-07,\n",
       "   1e-08,\n",
       "   0.00037028197689590567,\n",
       "   1e-08,\n",
       "   12,\n",
       "   10,\n",
       "   7,\n",
       "   2,\n",
       "   48,\n",
       "   48,\n",
       "   96,\n",
       "   160,\n",
       "   2560,\n",
       "   1e-05,\n",
       "   0.8587420534867742,\n",
       "   32]),\n",
       " (-0.8172043010752689,\n",
       "  [9.861337013936578e-05,\n",
       "   0.1,\n",
       "   2.9449855515277235e-06,\n",
       "   0.0007042608020481106,\n",
       "   2.487047013411406e-06,\n",
       "   0.022909054634210748,\n",
       "   12,\n",
       "   10,\n",
       "   5,\n",
       "   5,\n",
       "   64,\n",
       "   112,\n",
       "   64,\n",
       "   160,\n",
       "   256,\n",
       "   0.06562966011742352,\n",
       "   0.7737813490121088,\n",
       "   48]),\n",
       " (-0.8145161290322581,\n",
       "  [7.39627930947612e-05,\n",
       "   1.659708777060693e-05,\n",
       "   1e-08,\n",
       "   1.3173411991746205e-05,\n",
       "   0.0010958722677391498,\n",
       "   0.00011812947527297868,\n",
       "   12,\n",
       "   10,\n",
       "   7,\n",
       "   5,\n",
       "   64,\n",
       "   48,\n",
       "   64,\n",
       "   192,\n",
       "   1280,\n",
       "   0.44780974789849437,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.8141801075268817,\n",
       "  [5.896584730809908e-05,\n",
       "   3.712900148416795e-08,\n",
       "   1.1231997711612885e-07,\n",
       "   0.0012594342395513937,\n",
       "   5.7777984078126935e-05,\n",
       "   8.539884805941086e-07,\n",
       "   11,\n",
       "   11,\n",
       "   5,\n",
       "   3,\n",
       "   96,\n",
       "   128,\n",
       "   128,\n",
       "   64,\n",
       "   512,\n",
       "   0.303414136278007,\n",
       "   0.24513293105444084,\n",
       "   32]),\n",
       " (-0.813508064516129,\n",
       "  [9.86937992092117e-05,\n",
       "   2.2798146090726507e-08,\n",
       "   0.08360610183214345,\n",
       "   0.00014197860681504113,\n",
       "   2.0303076010902142e-08,\n",
       "   5.142968952468593e-06,\n",
       "   12,\n",
       "   11,\n",
       "   5,\n",
       "   3,\n",
       "   64,\n",
       "   32,\n",
       "   112,\n",
       "   192,\n",
       "   1024,\n",
       "   0.9400413406598438,\n",
       "   0.9962580702628657,\n",
       "   32]),\n",
       " (-0.802755376344086,\n",
       "  [0.00016716230142632866,\n",
       "   1e-08,\n",
       "   0.00045373961168616785,\n",
       "   1.8284130523110672e-08,\n",
       "   8.69244732417167e-07,\n",
       "   2.2729069481935696e-07,\n",
       "   12,\n",
       "   10,\n",
       "   7,\n",
       "   4,\n",
       "   64,\n",
       "   128,\n",
       "   80,\n",
       "   16,\n",
       "   1024,\n",
       "   0.47011962146980363,\n",
       "   0.23263096341664388,\n",
       "   32]),\n",
       " (-0.801747311827957,\n",
       "  [2.4496144936729485e-05,\n",
       "   0.011883465790874534,\n",
       "   1e-08,\n",
       "   4.2619536609982544e-07,\n",
       "   1.0908304491820514e-05,\n",
       "   4.1268504224542413e-07,\n",
       "   7,\n",
       "   10,\n",
       "   7,\n",
       "   2,\n",
       "   64,\n",
       "   64,\n",
       "   96,\n",
       "   192,\n",
       "   1024,\n",
       "   0.2734131274725732,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.7983870967741935,\n",
       "  [8.235870094087091e-05,\n",
       "   0.002546165492187618,\n",
       "   1e-08,\n",
       "   1.7944320277191452e-05,\n",
       "   0.0003207808817987087,\n",
       "   0.001162975124580808,\n",
       "   4,\n",
       "   12,\n",
       "   6,\n",
       "   5,\n",
       "   64,\n",
       "   128,\n",
       "   112,\n",
       "   16,\n",
       "   1536,\n",
       "   0.9415184119301956,\n",
       "   0.7476001542134237,\n",
       "   32]),\n",
       " (-0.7960349462365591,\n",
       "  [8.968526398653102e-05,\n",
       "   0.07323570485480543,\n",
       "   4.758220914580009e-08,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   0.1,\n",
       "   7,\n",
       "   11,\n",
       "   5,\n",
       "   5,\n",
       "   64,\n",
       "   48,\n",
       "   64,\n",
       "   192,\n",
       "   512,\n",
       "   0.5679576435334471,\n",
       "   0.7543408688108257,\n",
       "   32]),\n",
       " (-0.7946908602150539,\n",
       "  [4.673207478434585e-05,\n",
       "   4.751288197191044e-08,\n",
       "   1e-08,\n",
       "   0.0013924431730968708,\n",
       "   5.87768647448268e-06,\n",
       "   0.04547923104666384,\n",
       "   4,\n",
       "   12,\n",
       "   7,\n",
       "   4,\n",
       "   48,\n",
       "   48,\n",
       "   80,\n",
       "   192,\n",
       "   2560,\n",
       "   0.38871282651177724,\n",
       "   0.49453420583067914,\n",
       "   48]),\n",
       " (-0.7889784946236557,\n",
       "  [4.738863221196986e-05,\n",
       "   4.019676643419349e-08,\n",
       "   0.0008372874051499829,\n",
       "   0.05455658673724388,\n",
       "   0.1,\n",
       "   1.0656531479382393e-08,\n",
       "   10,\n",
       "   12,\n",
       "   6,\n",
       "   3,\n",
       "   96,\n",
       "   80,\n",
       "   128,\n",
       "   144,\n",
       "   512,\n",
       "   0.06728775163671502,\n",
       "   0.34587546482041487,\n",
       "   48]),\n",
       " (-0.7856182795698924,\n",
       "  [4.801545129221976e-05,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   0.05949982873256843,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   11,\n",
       "   11,\n",
       "   5,\n",
       "   4,\n",
       "   64,\n",
       "   48,\n",
       "   144,\n",
       "   96,\n",
       "   256,\n",
       "   0.9801564147393446,\n",
       "   0.9633864139236519,\n",
       "   48]),\n",
       " (-0.7799059139784946,\n",
       "  [1.7132143866368164e-05,\n",
       "   0.0004640014210267877,\n",
       "   4.2953652577771936e-07,\n",
       "   3.806110436880117e-07,\n",
       "   0.004036237026658003,\n",
       "   0.01207181932765083,\n",
       "   11,\n",
       "   10,\n",
       "   8,\n",
       "   2,\n",
       "   80,\n",
       "   48,\n",
       "   160,\n",
       "   16,\n",
       "   512,\n",
       "   0.8471057838040581,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.7795698924731181,\n",
       "  [0.00013887931360919898,\n",
       "   0.003852544524119167,\n",
       "   0.002167806538090596,\n",
       "   2.8658002315831633e-07,\n",
       "   1.0994397398499635e-07,\n",
       "   3.759374796788877e-07,\n",
       "   7,\n",
       "   9,\n",
       "   7,\n",
       "   3,\n",
       "   16,\n",
       "   128,\n",
       "   32,\n",
       "   32,\n",
       "   256,\n",
       "   0.5830040760965082,\n",
       "   0.5192277143950457,\n",
       "   48]),\n",
       " (-0.7782258064516129,\n",
       "  [1.2617175279799917e-05,\n",
       "   1.040836063198015e-08,\n",
       "   0.07226544924632848,\n",
       "   1.4416956057253336e-07,\n",
       "   0.0001824452077456112,\n",
       "   1.2477678939000823e-07,\n",
       "   6,\n",
       "   12,\n",
       "   6,\n",
       "   3,\n",
       "   64,\n",
       "   64,\n",
       "   160,\n",
       "   144,\n",
       "   512,\n",
       "   0.04594504253685113,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.7711693548387096,\n",
       "  [5.285992973468371e-06,\n",
       "   7.961961457843264e-06,\n",
       "   0.09389697401269005,\n",
       "   9.332780628102119e-06,\n",
       "   0.005172913297735564,\n",
       "   2.1922837717193363e-07,\n",
       "   12,\n",
       "   11,\n",
       "   7,\n",
       "   4,\n",
       "   64,\n",
       "   128,\n",
       "   64,\n",
       "   192,\n",
       "   1280,\n",
       "   0.5605438534602424,\n",
       "   0.8085481432380034,\n",
       "   32]),\n",
       " (-0.7570564516129032,\n",
       "  [0.0011315367447961434,\n",
       "   8.510551202928674e-08,\n",
       "   0.0005421677615136829,\n",
       "   3.681129133543062e-07,\n",
       "   2.2329810013319796e-08,\n",
       "   0.0007460890224628089,\n",
       "   12,\n",
       "   11,\n",
       "   6,\n",
       "   5,\n",
       "   96,\n",
       "   16,\n",
       "   112,\n",
       "   176,\n",
       "   1536,\n",
       "   0.6502229586785792,\n",
       "   0.46903038195654745,\n",
       "   32]),\n",
       " (-0.7557123655913979,\n",
       "  [0.00010401909674620818,\n",
       "   0.000268545644984576,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   1.6087520674114133e-08,\n",
       "   2.220098073443413e-06,\n",
       "   12,\n",
       "   11,\n",
       "   8,\n",
       "   5,\n",
       "   80,\n",
       "   32,\n",
       "   96,\n",
       "   192,\n",
       "   1024,\n",
       "   0.354920263880008,\n",
       "   0.7423077593693593,\n",
       "   32]),\n",
       " (-0.7422715053763441,\n",
       "  [6.956895629084846e-05,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   0.1,\n",
       "   9.697191197049737e-08,\n",
       "   12,\n",
       "   8,\n",
       "   5,\n",
       "   5,\n",
       "   64,\n",
       "   32,\n",
       "   96,\n",
       "   160,\n",
       "   1024,\n",
       "   1e-05,\n",
       "   1e-05,\n",
       "   32]),\n",
       " (-0.7318548387096774,\n",
       "  [6.626366094316239e-06,\n",
       "   4.261502146970251e-05,\n",
       "   1.9663692583844843e-08,\n",
       "   0.001638831597815122,\n",
       "   0.057568075885272685,\n",
       "   2.0255499011080455e-05,\n",
       "   10,\n",
       "   11,\n",
       "   7,\n",
       "   3,\n",
       "   64,\n",
       "   128,\n",
       "   80,\n",
       "   32,\n",
       "   1280,\n",
       "   0.8927662135026401,\n",
       "   0.3840595391724651,\n",
       "   32]),\n",
       " (-0.7207661290322581,\n",
       "  [1.5075298055390107e-06,\n",
       "   1.7890651103825076e-08,\n",
       "   0.00029945768813065347,\n",
       "   0.005487239334389958,\n",
       "   1.2921030238648097e-08,\n",
       "   3.063987851199055e-06,\n",
       "   4,\n",
       "   11,\n",
       "   7,\n",
       "   3,\n",
       "   80,\n",
       "   80,\n",
       "   160,\n",
       "   160,\n",
       "   256,\n",
       "   0.7150581938906496,\n",
       "   0.43859486196575015,\n",
       "   32]),\n",
       " (-0.7160618279569892,\n",
       "  [1.0962897143464106e-05,\n",
       "   1.1345703354849577e-06,\n",
       "   1.3512187630344906e-07,\n",
       "   2.5078465642621213e-07,\n",
       "   1.9027133313524809e-06,\n",
       "   0.0009278473479075441,\n",
       "   11,\n",
       "   9,\n",
       "   5,\n",
       "   5,\n",
       "   96,\n",
       "   128,\n",
       "   128,\n",
       "   64,\n",
       "   256,\n",
       "   0.05909371946146818,\n",
       "   0.6990153552150562,\n",
       "   32]),\n",
       " (-0.7113575268817205,\n",
       "  [0.0005530139183102202,\n",
       "   0.0010669665887076613,\n",
       "   1.8944182210275842e-06,\n",
       "   5.2926852635399126e-08,\n",
       "   0.0077805163811973214,\n",
       "   2.0964519996983865e-06,\n",
       "   7,\n",
       "   8,\n",
       "   7,\n",
       "   3,\n",
       "   96,\n",
       "   64,\n",
       "   128,\n",
       "   64,\n",
       "   512,\n",
       "   0.41575020903699267,\n",
       "   0.2841084516880318,\n",
       "   48]),\n",
       " (-0.7110215053763441,\n",
       "  [5.686047384334293e-06,\n",
       "   1.1896389073157519e-06,\n",
       "   1.7576982637676603e-05,\n",
       "   0.0028884456120337847,\n",
       "   0.00010409200287500567,\n",
       "   3.732111945854698e-06,\n",
       "   9,\n",
       "   9,\n",
       "   5,\n",
       "   2,\n",
       "   96,\n",
       "   32,\n",
       "   32,\n",
       "   16,\n",
       "   256,\n",
       "   0.46324442869861776,\n",
       "   0.7947200197804732,\n",
       "   32]),\n",
       " (-0.7012768817204301,\n",
       "  [1.2754167349195913e-06,\n",
       "   1.366180428901538e-05,\n",
       "   0.012159267349393028,\n",
       "   3.3316985632150818e-06,\n",
       "   0.0001638575027504523,\n",
       "   2.550155947742958e-07,\n",
       "   9,\n",
       "   8,\n",
       "   5,\n",
       "   5,\n",
       "   96,\n",
       "   32,\n",
       "   128,\n",
       "   64,\n",
       "   512,\n",
       "   0.4321230175408357,\n",
       "   0.03772932275653298,\n",
       "   32]),\n",
       " (-0.6750672043010753,\n",
       "  [7.68944062872739e-07,\n",
       "   0.08429627041141229,\n",
       "   1.0192560056779807e-08,\n",
       "   1.159841995957678e-08,\n",
       "   0.03749066070180811,\n",
       "   0.0006500289218796252,\n",
       "   12,\n",
       "   10,\n",
       "   6,\n",
       "   5,\n",
       "   96,\n",
       "   112,\n",
       "   112,\n",
       "   16,\n",
       "   3072,\n",
       "   0.8155796407605778,\n",
       "   0.7199636542848179,\n",
       "   32]),\n",
       " (-0.6599462365591398,\n",
       "  [2.8846778292715103e-07,\n",
       "   5.444144931208947e-07,\n",
       "   4.2161223785694645e-07,\n",
       "   0.1,\n",
       "   0.0002474595905197343,\n",
       "   6.646902714994344e-06,\n",
       "   5,\n",
       "   12,\n",
       "   8,\n",
       "   3,\n",
       "   96,\n",
       "   128,\n",
       "   112,\n",
       "   192,\n",
       "   256,\n",
       "   0.20638782956517324,\n",
       "   0.7168360373332855,\n",
       "   32]),\n",
       " (-0.6542338709677419,\n",
       "  [3.9511344242172176e-07,\n",
       "   3.515499034362549e-06,\n",
       "   0.007181610458097709,\n",
       "   5.592792904608792e-06,\n",
       "   0.001933696569534301,\n",
       "   2.9181430260625876e-07,\n",
       "   8,\n",
       "   11,\n",
       "   7,\n",
       "   3,\n",
       "   16,\n",
       "   64,\n",
       "   32,\n",
       "   160,\n",
       "   256,\n",
       "   0.5220081794341872,\n",
       "   0.37465183542992875,\n",
       "   32]),\n",
       " (-0.6350806451612905,\n",
       "  [0.03630460355373526,\n",
       "   6.265736733648485e-07,\n",
       "   0.0002649831808012346,\n",
       "   0.0184189634330473,\n",
       "   5.184083950249856e-08,\n",
       "   5.793527938168135e-05,\n",
       "   6,\n",
       "   9,\n",
       "   7,\n",
       "   3,\n",
       "   16,\n",
       "   112,\n",
       "   32,\n",
       "   144,\n",
       "   1024,\n",
       "   0.007029167894632017,\n",
       "   0.6579279907824965,\n",
       "   48]),\n",
       " (-0.6058467741935486,\n",
       "  [0.0002695640482395255,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   0.08030874458206674,\n",
       "   1e-08,\n",
       "   12,\n",
       "   12,\n",
       "   7,\n",
       "   5,\n",
       "   64,\n",
       "   128,\n",
       "   96,\n",
       "   160,\n",
       "   1024,\n",
       "   0.6703463105977053,\n",
       "   1e-05,\n",
       "   48]),\n",
       " (-0.5997983870967742,\n",
       "  [0.0541333128723208,\n",
       "   2.9589071446327773e-08,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   0.0020819609740514514,\n",
       "   2.733059187342482e-08,\n",
       "   4,\n",
       "   9,\n",
       "   8,\n",
       "   2,\n",
       "   64,\n",
       "   32,\n",
       "   96,\n",
       "   160,\n",
       "   2560,\n",
       "   0.9999,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.5903897849462365,\n",
       "  [0.00011415630246118247,\n",
       "   1e-08,\n",
       "   2.8954346371104777e-06,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   12,\n",
       "   12,\n",
       "   5,\n",
       "   2,\n",
       "   64,\n",
       "   48,\n",
       "   128,\n",
       "   192,\n",
       "   1024,\n",
       "   0.9999,\n",
       "   0.5002811170893585,\n",
       "   32]),\n",
       " (-0.5782930107526881,\n",
       "  [9.807175337297393e-05,\n",
       "   0.023237863498314058,\n",
       "   1.5580511527210855e-06,\n",
       "   1e-08,\n",
       "   1.1350202002574844e-07,\n",
       "   0.0029685646046512305,\n",
       "   12,\n",
       "   12,\n",
       "   8,\n",
       "   5,\n",
       "   16,\n",
       "   128,\n",
       "   112,\n",
       "   192,\n",
       "   2560,\n",
       "   0.9999,\n",
       "   0.5669955297367031,\n",
       "   32]),\n",
       " (-0.5709005376344086,\n",
       "  [0.012224628899501036,\n",
       "   0.0003967737664812854,\n",
       "   0.00036745158994556476,\n",
       "   0.0654211837973057,\n",
       "   1.3821537259257128e-07,\n",
       "   0.025736682946352573,\n",
       "   11,\n",
       "   10,\n",
       "   7,\n",
       "   2,\n",
       "   96,\n",
       "   48,\n",
       "   128,\n",
       "   144,\n",
       "   256,\n",
       "   0.49382909769619054,\n",
       "   0.09521692935964228,\n",
       "   32]),\n",
       " (-0.5618279569892473,\n",
       "  [0.002044754971394743,\n",
       "   0.013355350013554578,\n",
       "   7.227828983335062e-07,\n",
       "   0.0001538511039561163,\n",
       "   3.064373236213228e-05,\n",
       "   0.013948855343842121,\n",
       "   7,\n",
       "   10,\n",
       "   7,\n",
       "   4,\n",
       "   96,\n",
       "   128,\n",
       "   48,\n",
       "   144,\n",
       "   2048,\n",
       "   0.573202121903716,\n",
       "   0.9086435758927431,\n",
       "   32]),\n",
       " (-0.5191532258064516,\n",
       "  [0.0001188351101654264,\n",
       "   1e-08,\n",
       "   1e-08,\n",
       "   0.0004828600079752768,\n",
       "   0.0035141976081386897,\n",
       "   0.0001147786520963806,\n",
       "   12,\n",
       "   12,\n",
       "   7,\n",
       "   4,\n",
       "   32,\n",
       "   128,\n",
       "   96,\n",
       "   192,\n",
       "   1280,\n",
       "   0.995697567332454,\n",
       "   0.8308005956860326,\n",
       "   32]),\n",
       " (-0.511760752688172,\n",
       "  [0.001042587142528311,\n",
       "   0.0009642173558542775,\n",
       "   1e-08,\n",
       "   1.0388960964821653e-05,\n",
       "   6.319556537665943e-08,\n",
       "   2.3608719960146257e-05,\n",
       "   9,\n",
       "   12,\n",
       "   7,\n",
       "   5,\n",
       "   48,\n",
       "   128,\n",
       "   64,\n",
       "   192,\n",
       "   256,\n",
       "   0.030110864086461443,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.5047043010752688,\n",
       "  [0.0007957196783963998,\n",
       "   3.111586549136912e-05,\n",
       "   0.001288538375301774,\n",
       "   0.000191156544636201,\n",
       "   2.3749702815188e-07,\n",
       "   3.907638178257284e-07,\n",
       "   12,\n",
       "   9,\n",
       "   6,\n",
       "   2,\n",
       "   96,\n",
       "   80,\n",
       "   128,\n",
       "   176,\n",
       "   2560,\n",
       "   0.539894993317384,\n",
       "   0.0568176654282331,\n",
       "   48]),\n",
       " (-0.49731182795698925,\n",
       "  [0.00706197449255158,\n",
       "   0.0032112669527156216,\n",
       "   5.097596365889932e-08,\n",
       "   1.4503589117178354e-08,\n",
       "   8.01246930713597e-06,\n",
       "   3.035921530123747e-08,\n",
       "   11,\n",
       "   9,\n",
       "   7,\n",
       "   5,\n",
       "   96,\n",
       "   128,\n",
       "   128,\n",
       "   64,\n",
       "   1536,\n",
       "   0.9758176241139793,\n",
       "   0.9169187594834153,\n",
       "   32]),\n",
       " (-0.4946236559139785,\n",
       "  [1.2230061308971027e-07,\n",
       "   1.1975375401673855e-08,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   4,\n",
       "   10,\n",
       "   8,\n",
       "   2,\n",
       "   48,\n",
       "   32,\n",
       "   96,\n",
       "   16,\n",
       "   1536,\n",
       "   0.9999,\n",
       "   0.734298760821593,\n",
       "   32]),\n",
       " (-0.4946236559139785,\n",
       "  [0.008326002889143936,\n",
       "   0.08553351273670627,\n",
       "   4.4061890107855624e-05,\n",
       "   0.010340933528199127,\n",
       "   0.038919475787067434,\n",
       "   7.117182220734183e-06,\n",
       "   4,\n",
       "   9,\n",
       "   6,\n",
       "   5,\n",
       "   96,\n",
       "   112,\n",
       "   80,\n",
       "   64,\n",
       "   512,\n",
       "   0.5448030149573859,\n",
       "   0.8902128559564233,\n",
       "   32])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.8561827956989247,\n",
       "  [8.8236737270166e-05,\n",
       "   1e-08,\n",
       "   1.326725515944222e-08,\n",
       "   1e-08,\n",
       "   5.0997369819268274e-05,\n",
       "   1e-08,\n",
       "   6,\n",
       "   10,\n",
       "   8,\n",
       "   3,\n",
       "   64,\n",
       "   32,\n",
       "   96,\n",
       "   160,\n",
       "   2560,\n",
       "   0.7391658564044657,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.854502688172043,\n",
       "  [6.16251948834497e-05,\n",
       "   1e-08,\n",
       "   1.0347315112413696e-08,\n",
       "   1e-08,\n",
       "   1.7225817253935858e-06,\n",
       "   2.5356671661874186e-06,\n",
       "   6,\n",
       "   10,\n",
       "   8,\n",
       "   4,\n",
       "   64,\n",
       "   144,\n",
       "   96,\n",
       "   160,\n",
       "   1280,\n",
       "   0.6460780723554685,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.8417338709677419,\n",
       "  [9.249516740964438e-05,\n",
       "   1.65645672799752e-06,\n",
       "   2.0212983450054314e-06,\n",
       "   5.9559985830515796e-05,\n",
       "   0.00474768850969449,\n",
       "   3.9520792545822195e-08,\n",
       "   7,\n",
       "   8,\n",
       "   8,\n",
       "   3,\n",
       "   16,\n",
       "   32,\n",
       "   96,\n",
       "   160,\n",
       "   2560,\n",
       "   0.28086501184972806,\n",
       "   0.9788445622478666,\n",
       "   32]),\n",
       " (-0.8373655913978495,\n",
       "  [5.859434388146192e-05,\n",
       "   0.008785894138915515,\n",
       "   2.03421259226207e-05,\n",
       "   0.1,\n",
       "   2.1067419920481055e-06,\n",
       "   0.001562306081019681,\n",
       "   9,\n",
       "   5,\n",
       "   3,\n",
       "   3,\n",
       "   64,\n",
       "   16,\n",
       "   128,\n",
       "   256,\n",
       "   2304,\n",
       "   0.37867987592535424,\n",
       "   0.9081180541372976,\n",
       "   32]),\n",
       " (-0.8343413978494628,\n",
       "  [4.4394189661246325e-05,\n",
       "   0.00029258441124090074,\n",
       "   0.0001317297675584068,\n",
       "   0.1,\n",
       "   8.238838034434065e-06,\n",
       "   5.720639847834733e-08,\n",
       "   7,\n",
       "   5,\n",
       "   3,\n",
       "   3,\n",
       "   64,\n",
       "   16,\n",
       "   128,\n",
       "   256,\n",
       "   2304,\n",
       "   1e-05,\n",
       "   0.9999,\n",
       "   48]),\n",
       " (-0.8336693548387096,\n",
       "  [9.533681551266233e-05,\n",
       "   2.26497801485173e-05,\n",
       "   0.03558726126318972,\n",
       "   0.013011893340899375,\n",
       "   0.016629945591718952,\n",
       "   2.520566743638647e-08,\n",
       "   6,\n",
       "   5,\n",
       "   4,\n",
       "   4,\n",
       "   32,\n",
       "   32,\n",
       "   32,\n",
       "   160,\n",
       "   2560,\n",
       "   0.21245749321598112,\n",
       "   0.7795780363189785,\n",
       "   48]),\n",
       " (-0.831989247311828,\n",
       "  [5.1919029633741406e-05,\n",
       "   0.1,\n",
       "   3.101528670704241e-08,\n",
       "   0.004434457019992078,\n",
       "   1e-08,\n",
       "   0.03626672035566686,\n",
       "   12,\n",
       "   9,\n",
       "   3,\n",
       "   5,\n",
       "   64,\n",
       "   16,\n",
       "   128,\n",
       "   256,\n",
       "   2304,\n",
       "   0.769069488869265,\n",
       "   0.6153539351210203,\n",
       "   32]),\n",
       " (-0.8282930107526881,\n",
       "  [5.542855054476552e-05,\n",
       "   0.012561511653911627,\n",
       "   0.1,\n",
       "   1.1542402058801961e-07,\n",
       "   0.005615105786220398,\n",
       "   6.344985233054229e-07,\n",
       "   10,\n",
       "   7,\n",
       "   4,\n",
       "   3,\n",
       "   80,\n",
       "   16,\n",
       "   48,\n",
       "   256,\n",
       "   2048,\n",
       "   1e-05,\n",
       "   0.7831838142544103,\n",
       "   32]),\n",
       " (-0.8192204301075269,\n",
       "  [6e-05,\n",
       "   0.0002,\n",
       "   0.0002,\n",
       "   0.0002,\n",
       "   0.0002,\n",
       "   0.001,\n",
       "   10,\n",
       "   7,\n",
       "   4,\n",
       "   4,\n",
       "   64,\n",
       "   128,\n",
       "   128,\n",
       "   256,\n",
       "   4096,\n",
       "   0.9,\n",
       "   0.999,\n",
       "   32]),\n",
       " (-0.8081317204301075,\n",
       "  [3.599765479432865e-05,\n",
       "   0.1,\n",
       "   7.398354183790574e-06,\n",
       "   0.1,\n",
       "   1.9236243436317369e-07,\n",
       "   0.1,\n",
       "   9,\n",
       "   7,\n",
       "   3,\n",
       "   5,\n",
       "   80,\n",
       "   32,\n",
       "   128,\n",
       "   256,\n",
       "   2304,\n",
       "   0.7276377970308386,\n",
       "   0.6707520463654754,\n",
       "   32]),\n",
       " (-0.7980510752688171,\n",
       "  [0.0001545151585483461,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   1e-08,\n",
       "   6,\n",
       "   5,\n",
       "   3,\n",
       "   3,\n",
       "   32,\n",
       "   32,\n",
       "   32,\n",
       "   160,\n",
       "   2048,\n",
       "   1e-05,\n",
       "   0.23071075175547506,\n",
       "   48]),\n",
       " (-0.7943548387096774,\n",
       "  [8.027544869036882e-05,\n",
       "   4.3155264632668056e-05,\n",
       "   1.4851504936859617e-06,\n",
       "   5.7396357888080574e-06,\n",
       "   1.675511068733851e-07,\n",
       "   0.07252610691574524,\n",
       "   12,\n",
       "   6,\n",
       "   3,\n",
       "   6,\n",
       "   80,\n",
       "   112,\n",
       "   32,\n",
       "   160,\n",
       "   2560,\n",
       "   0.3840934523372874,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.7926747311827957,\n",
       "  [9.30078225870394e-05,\n",
       "   0.1,\n",
       "   0.00014495037769236116,\n",
       "   0.1,\n",
       "   1.7812640518635282e-06,\n",
       "   7.92492270687262e-05,\n",
       "   7,\n",
       "   5,\n",
       "   5,\n",
       "   3,\n",
       "   64,\n",
       "   128,\n",
       "   112,\n",
       "   256,\n",
       "   512,\n",
       "   0.6448581462223936,\n",
       "   0.9999,\n",
       "   32]),\n",
       " (-0.7909946236559137,\n",
       "  [0.0004271660523891373,\n",
       "   1.816271345179006e-07,\n",
       "   0.013680541417110838,\n",
       "   7.100672872936599e-08,\n",
       "   1.1624970096069801e-06,\n",
       "   5.0306754221676625e-08,\n",
       "   11,\n",
       "   6,\n",
       "   7,\n",
       "   5,\n",
       "   96,\n",
       "   32,\n",
       "   32,\n",
       "   160,\n",
       "   1280,\n",
       "   0.18472823202499572,\n",
       "   0.04146710391960219,\n",
       "   48]),\n",
       " (-0.7852822580645161,\n",
       "  [1.2672682145785753e-05,\n",
       "   0.028497645757735043,\n",
       "   0.00020192254270016776,\n",
       "   1.0640098453042812e-07,\n",
       "   2.068239513066196e-07,\n",
       "   4.397451075043287e-06,\n",
       "   11,\n",
       "   5,\n",
       "   7,\n",
       "   4,\n",
       "   32,\n",
       "   32,\n",
       "   32,\n",
       "   160,\n",
       "   2048,\n",
       "   0.9594856693198146,\n",
       "   0.536213513349149,\n",
       "   32]),\n",
       " (-0.7849462365591398,\n",
       "  [5.4031301597777776e-05,\n",
       "   5.192068136114765e-06,\n",
       "   2.7275425588363527e-05,\n",
       "   0.04956027495723233,\n",
       "   1.120190819887008e-07,\n",
       "   0.036356911960904355,\n",
       "   7,\n",
       "   5,\n",
       "   4,\n",
       "   3,\n",
       "   64,\n",
       "   16,\n",
       "   32,\n",
       "   256,\n",
       "   3840,\n",
       "   0.21153367530332268,\n",
       "   0.48062057889565296,\n",
       "   32]),\n",
       " (-0.7708333333333334,\n",
       "  [7.292231225936188e-06,\n",
       "   0.002892637004589595,\n",
       "   1.2959765367197094e-08,\n",
       "   5.846017871175605e-05,\n",
       "   0.00012055926685599095,\n",
       "   0.005137043664263209,\n",
       "   7,\n",
       "   5,\n",
       "   6,\n",
       "   4,\n",
       "   16,\n",
       "   112,\n",
       "   128,\n",
       "   256,\n",
       "   1280,\n",
       "   0.390333715820761,\n",
       "   0.35506199347664613,\n",
       "   32]),\n",
       " (-0.7708333333333334,\n",
       "  [1.5781796953781333e-05,\n",
       "   0.00018869281915911068,\n",
       "   3.137985556249964e-07,\n",
       "   6.489709870493954e-08,\n",
       "   0.0027165324972935013,\n",
       "   0.00017114921723266127,\n",
       "   8,\n",
       "   9,\n",
       "   4,\n",
       "   5,\n",
       "   16,\n",
       "   48,\n",
       "   80,\n",
       "   192,\n",
       "   1024,\n",
       "   0.40364839927810503,\n",
       "   0.17235221292550384,\n",
       "   32]),\n",
       " (-0.7459677419354839,\n",
       "  [3.367795916545147e-06,\n",
       "   0.016313424690630872,\n",
       "   1.1540844495313483e-05,\n",
       "   4.950906976148164e-08,\n",
       "   1.8443546145975796e-06,\n",
       "   0.007466577138250061,\n",
       "   12,\n",
       "   8,\n",
       "   8,\n",
       "   6,\n",
       "   80,\n",
       "   32,\n",
       "   32,\n",
       "   256,\n",
       "   1792,\n",
       "   0.7874868754806217,\n",
       "   0.20392451996042602,\n",
       "   32]),\n",
       " (-0.7268145161290323,\n",
       "  [1.911650467945094e-06,\n",
       "   7.079536391473009e-07,\n",
       "   9.389553313928497e-06,\n",
       "   0.1,\n",
       "   0.011092300858180254,\n",
       "   3.99968120350601e-06,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   5,\n",
       "   32,\n",
       "   16,\n",
       "   128,\n",
       "   256,\n",
       "   1280,\n",
       "   0.5703835733849612,\n",
       "   0.9050391247315777,\n",
       "   32]),\n",
       " (-0.7174059139784946,\n",
       "  [1.4802056760161276e-06,\n",
       "   3.32700701264256e-05,\n",
       "   0.00037826784437053806,\n",
       "   0.0033451422602195024,\n",
       "   4.9548553840755416e-08,\n",
       "   1.0353361125722466e-07,\n",
       "   10,\n",
       "   7,\n",
       "   7,\n",
       "   4,\n",
       "   16,\n",
       "   80,\n",
       "   144,\n",
       "   160,\n",
       "   2048,\n",
       "   0.6664431257557869,\n",
       "   0.7874034749409503,\n",
       "   32]),\n",
       " (-0.7039650537634409,\n",
       "  [0.0006328559388271128,\n",
       "   0.009172946393489602,\n",
       "   0.00014868221412578793,\n",
       "   2.182091082233226e-08,\n",
       "   5.842798041282555e-07,\n",
       "   1.033296154740724e-07,\n",
       "   8,\n",
       "   8,\n",
       "   3,\n",
       "   5,\n",
       "   80,\n",
       "   16,\n",
       "   160,\n",
       "   32,\n",
       "   1280,\n",
       "   0.2593206480183037,\n",
       "   0.276746594449071,\n",
       "   32]),\n",
       " (-0.6663306451612903,\n",
       "  [1.2121858263273482e-07,\n",
       "   1.9227984710640378e-07,\n",
       "   0.003974135601903887,\n",
       "   0.02578334963599709,\n",
       "   0.06450502135816752,\n",
       "   1.7941762761600812e-07,\n",
       "   10,\n",
       "   7,\n",
       "   6,\n",
       "   4,\n",
       "   64,\n",
       "   80,\n",
       "   112,\n",
       "   192,\n",
       "   3072,\n",
       "   0.5485545710811742,\n",
       "   0.8982493965644359,\n",
       "   48]),\n",
       " (-0.6266801075268817,\n",
       "  [0.0018922314190892252,\n",
       "   0.008248598302617698,\n",
       "   2.5141148483432145e-07,\n",
       "   2.890283092515089e-06,\n",
       "   0.0018115012619543472,\n",
       "   0.018207296033372813,\n",
       "   9,\n",
       "   10,\n",
       "   5,\n",
       "   5,\n",
       "   16,\n",
       "   80,\n",
       "   96,\n",
       "   224,\n",
       "   2304,\n",
       "   0.43621111113685324,\n",
       "   0.34225016922539125,\n",
       "   48]),\n",
       " (-0.5967741935483871,\n",
       "  [0.0739400064170401,\n",
       "   0.002007671767390811,\n",
       "   8.175501308975809e-06,\n",
       "   1.78358696947472e-08,\n",
       "   9.284001008404622e-06,\n",
       "   0.08262125930067209,\n",
       "   7,\n",
       "   7,\n",
       "   4,\n",
       "   5,\n",
       "   48,\n",
       "   128,\n",
       "   16,\n",
       "   160,\n",
       "   2304,\n",
       "   0.4495459270396916,\n",
       "   0.6154473574635756,\n",
       "   32]),\n",
       " (-0.5803091397849464,\n",
       "  [0.0034179001459333,\n",
       "   2.6580692911664925e-06,\n",
       "   0.09513013903747987,\n",
       "   1.4827355825497654e-06,\n",
       "   3.8998941736467277e-08,\n",
       "   0.011603221380791005,\n",
       "   8,\n",
       "   8,\n",
       "   7,\n",
       "   4,\n",
       "   32,\n",
       "   96,\n",
       "   128,\n",
       "   32,\n",
       "   1280,\n",
       "   0.07387432781416357,\n",
       "   0.6063600401091502,\n",
       "   48]),\n",
       " (-0.561491935483871,\n",
       "  [4.832547786779934e-05,\n",
       "   0.06825905089663369,\n",
       "   0.01248663317595344,\n",
       "   3.527139092222278e-07,\n",
       "   0.0015945467776882906,\n",
       "   0.0003197084598000713,\n",
       "   11,\n",
       "   8,\n",
       "   5,\n",
       "   4,\n",
       "   64,\n",
       "   128,\n",
       "   96,\n",
       "   32,\n",
       "   4096,\n",
       "   0.9841251826151078,\n",
       "   0.3712411039502596,\n",
       "   48]),\n",
       " (-0.5504032258064516,\n",
       "  [0.002887785884074586,\n",
       "   1e-08,\n",
       "   1.9698268167972554e-06,\n",
       "   0.000549950494239654,\n",
       "   0.000642577373792084,\n",
       "   0.002334255685374792,\n",
       "   6,\n",
       "   5,\n",
       "   5,\n",
       "   3,\n",
       "   16,\n",
       "   112,\n",
       "   128,\n",
       "   256,\n",
       "   1280,\n",
       "   1e-05,\n",
       "   0.2531071412111334,\n",
       "   32]),\n",
       " (-0.539986559139785,\n",
       "  [0.00410503547592976,\n",
       "   0.062597321310174,\n",
       "   2.259956016301591e-06,\n",
       "   3.5867830022995886e-07,\n",
       "   0.00010955092186774995,\n",
       "   0.00019438014916410237,\n",
       "   11,\n",
       "   9,\n",
       "   3,\n",
       "   5,\n",
       "   96,\n",
       "   128,\n",
       "   144,\n",
       "   16,\n",
       "   2304,\n",
       "   0.816509121904795,\n",
       "   0.04528276855373469,\n",
       "   32]),\n",
       " (-0.47614247311827956,\n",
       "  [0.002445068211972435,\n",
       "   0.1,\n",
       "   0.01474202260927845,\n",
       "   0.0012758628400542566,\n",
       "   0.000272246233986787,\n",
       "   0.006623065890463127,\n",
       "   11,\n",
       "   8,\n",
       "   3,\n",
       "   5,\n",
       "   32,\n",
       "   32,\n",
       "   32,\n",
       "   16,\n",
       "   2048,\n",
       "   0.7213897906460444,\n",
       "   0.2563049337606032,\n",
       "   48])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.8236737270166e-05,\n",
       " 1e-08,\n",
       " 1.326725515944222e-08,\n",
       " 1e-08,\n",
       " 5.0997369819268274e-05,\n",
       " 1e-08,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 3,\n",
       " 64,\n",
       " 32,\n",
       " 96,\n",
       " 160,\n",
       " 2560,\n",
       " 0.7391658564044657,\n",
       " 0.9999,\n",
       " 32]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
